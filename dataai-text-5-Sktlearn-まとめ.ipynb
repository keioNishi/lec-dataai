{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.6"},"colab":{"name":"dataai-text-5-Sktlearn-まとめ.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"5yW7JkEY3phS"},"source":["#@title Data-AI（必ず自分の名前・学籍番号を入力すること） { run: \"auto\", display-mode: \"form\" }\n","\n","import urllib.request as ur\n","import urllib.parse as up\n","Name = '\\u6C5F\\u6D32\\u51FA\\u4E95 \\u592A\\u90CE' #@param {type:\"string\"}\n","EName = 'Esudei Taro' #@param {type:\"string\"}\n","StudentID = '87654321' #@param {type:\"string\"}\n","Addrp = !cat /sys/class/net/eth0/address\n","Addr = Addrp[0]\n","url = 'https://class.west.sd.keio.ac.jp/classroll.php'\n","params = {'class':'dataai','name':Name,'ename':EName,'id':StudentID,'addr':Addr,\n","           'page':'dataai-text-5','token':'88375811'}\n","data = up.urlencode(params).encode('utf-8')\n","#headers = {'itmes','application/x-www-form-urlencoded'}\n","req = ur.Request(url, data=data)\n","res = ur.urlopen(req)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lsz8-1vf359K"},"source":["---\n","着ない羽織があるなら持ってこい\\\n","吹かない風があるなら持ってこい\\\n","飛行しない雲があるなら持ってこい\\\n","(夏井いつき)\n","\n","**解析できないデータがあるなら持ってこい**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"KCNmVGdcEtWV"},"source":["# Scikit-learnの俯瞰と一連の流れのおさらい\n","簡単な機械学習アプリケーションを通してScikit-learnの主要機能とモデルを確認する\n","\n","ここでは、最も一般的なデータであるiris(アヤメ分類)を用い、最も基本的な手法(k-近傍法)で学習を行うことで、Scikit-learnについて俯瞰する"]},{"cell_type":"markdown","metadata":{"id":"8kTWxQKlPBbd"},"source":["# アイリスのクラス分類\n","アヤメの種類を予測する\\\n","そのために、種類が分かっているアイリスの測定値を用いて機械学習モデルを構築する\n","\n","setosa, versicolor, virginicaの３種類に分類\n","（教師あり学習、クラス分類）\n","\n","<img src = 'http://class.west.sd.keio.ac.jp/dataai/scilearn/iris_petal_sepal.png' width = \"30%\">"]},{"cell_type":"markdown","metadata":{"id":"l67xocjmEtWV"},"source":["## irisデータの読み込み"]},{"cell_type":"markdown","metadata":{"id":"kUVKRYMpEtWW"},"source":["必要なライブラリの読み込み"]},{"cell_type":"code","metadata":{"id":"gT4lBmNWEtWY"},"source":["%matplotlib inline\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import sklearn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l8aTC5EXEtWc"},"source":["irisデータセットの読み込み\n","（3種類のアイリスの花について、花弁の長さと幅、がくの長さと幅を測定した結果）"]},{"cell_type":"code","metadata":{"id":"587XC3kbEtWd"},"source":["from sklearn.datasets import load_iris\n","iris_dataset = load_iris()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cHlwLNVGEtWh"},"source":["load_irisが返すirisオブジェクトは辞書型に似たオブジェクトで、キーと値を持つ"]},{"cell_type":"code","metadata":{"id":"C9XUaOgEEtWh"},"source":["iris_dataset.keys()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lx0u8g0KEtWm"},"source":["キー DESCR の値はデータセットの簡単な説明（description）"]},{"cell_type":"code","metadata":{"id":"9_CAOudKEtWn"},"source":["print(iris_dataset['DESCR'][:200] + '\\n...')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"stbOX4pqEtWs"},"source":["キー target_names に対応する値は予測する花の種類"]},{"cell_type":"code","metadata":{"id":"-lBmJAT5EtWs"},"source":["print('Target names: {}'.format(iris_dataset['target_names']))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v8km1QgrEtWv"},"source":["キー feature_names に対応する値は、それぞれの特徴量の説明"]},{"cell_type":"code","metadata":{"id":"PDQwqRidEtWw"},"source":["print('Feature names: {}'.format(iris_dataset['feature_names']))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ndRTQH4oEtW0"},"source":["データ本体は、targetとdataフィールドに格納されている\n","\n","dataには、がくの長さ、がくの幅、花弁の長さ、花弁の幅がNumpy配列として格納されている"]},{"cell_type":"code","metadata":{"id":"RIEJachMEtW1"},"source":["type(iris_dataset['data'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6NXej7O5EtW4"},"source":["配列dataの各行は個々の花、列は個々の花に対して行われた４つの測定に対応"]},{"cell_type":"code","metadata":{"id":"FlWkr06WEtW4"},"source":["iris_dataset['data'].shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RnHsYDE3EtW8"},"source":["配列には150の花の測定結果が格納されている\\\n","最初の５つのサンプルを見る"]},{"cell_type":"code","metadata":{"id":"ZHKDSSpwEtW8"},"source":["iris_dataset['data'][:5]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TpJs4U91EtXA"},"source":["配列targetには、測定された個々の花の種類がNumpy配列として格納されている"]},{"cell_type":"code","metadata":{"id":"PDwhvQ-lEtXD"},"source":["type(iris_dataset['target'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LbPuGuVuEtXF"},"source":["targetは1次元配列で、個々の花に1つのエントリが対応する"]},{"cell_type":"code","metadata":{"id":"LBVAuNZzEtXG"},"source":["iris_dataset['target'].shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"14LttcVyEtXK"},"source":["花の種類は0から2までの整数としてエンコードされている\\\n","0：setosa, 1：versicolor, 2：virginica"]},{"cell_type":"code","metadata":{"id":"L4g1XEvvEtXK"},"source":["iris_dataset['target']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AZ31aE_EEtXN"},"source":["訓練データとテストデータに分割する"]},{"cell_type":"code","metadata":{"id":"jECCs_0EEtXO"},"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'], iris_dataset['target'], random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0dLSR8LnEtXR"},"source":["print('X_train shape: {}'.format(X_train.shape))\n","print('y_train shape: {}'.format(y_train.shape))\n","print('X_test shape: {}'.format(X_test.shape))\n","print('y_test shape: {}'.format(y_test.shape))    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GnAuSfplEtXU"},"source":["scikit-learnでは、データを大文字のX、ラベルを小文字のyで表すのが一般的\\\n","train_test_splitによって112の訓練データと38のテストデータに分割された"]},{"cell_type":"markdown","metadata":{"id":"VJeAmQxDEtXU"},"source":["## データの観察"]},{"cell_type":"markdown","metadata":{"id":"4rFpD0vcEtXV"},"source":["機械学習モデルを構築する前に、データを観察することが大切\n","\n","**散布図**による可視化  \n","ここでは、全ての特徴量の組み合わせをプロットする**ペアプロット**を用いる"]},{"cell_type":"code","metadata":{"id":"9sB72OByEtXV"},"source":["iris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)\n","grr = pd.plotting.scatter_matrix(iris_dataframe, c=y_train, figsize=(15,15), marker='o', hist_kwds={'bins':20}, s=60, alpha=.8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JsIc-fLJEtXY"},"source":["３つのクラスは花弁とがくの測定結果で分離していることが分かる\n","つまり、うまく分類できるように機械学習モデルを訓練することができる可能性が高いことを意味する"]},{"cell_type":"markdown","metadata":{"id":"C69D68XAEtXY"},"source":["# k-近傍法"]},{"cell_type":"markdown","metadata":{"id":"ftau5RmaEtXY"},"source":["実際に機械学習モデルを構築してみる  \n","ここではk-近傍法(k-NearestNeighbors)によるクラス分類を行う\n","\n","k-NNは、新しい点に最も近いk個の点を訓練セットから探し、これらの多数決で新しいデータのラベルを決定する方法である\n","\n","その動作の仕組みを様々ビジュアライズしたサイトがあるので確認すると良い"]},{"cell_type":"markdown","metadata":{"id":"VxwU3sMiEtXZ"},"source":["<img src = 'http://class.west.sd.keio.ac.jp/dataai/scilearn/knn.png' width = \"50%\">"]},{"cell_type":"markdown","metadata":{"id":"K-wZziKCEtXZ"},"source":["k-近傍法アルゴリズムはneighborsモジュールのkNeighborsClassifierクラスに実装されている\\\n","モデルを使う前に、クラスのインスタンスを作成してオブジェクトを作る\\\n","今回は k = 1とする  "]},{"cell_type":"code","metadata":{"id":"p24Xk3NTEtXb"},"source":["from sklearn.neighbors import KNeighborsClassifier\n","knn = KNeighborsClassifier(n_neighbors=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lVJkzlkpEtXd"},"source":["knnオブジェクトは、訓練データからモデルを構築する際に用いられるアルゴリズムと、新たなデータポイントに対する予測のためのアルゴリズムをカプセル化している\\\n","訓練セットからモデルを構築するには、knnオブジェクトのfitメソッドを呼び出す"]},{"cell_type":"code","metadata":{"id":"XeqXsL5SEtXe"},"source":["knn.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w3JR90YeEtXh"},"source":["## 予測"]},{"cell_type":"markdown","metadata":{"id":"hSO-S5cYEtXi"},"source":["構築したモデルを用いてラベルが分かっていない新たなデータの予測をしてみる\\\n","がくの長さと幅、花弁の長さと幅をNumpy配列に格納し、knnオブジェクトのpredictメソッドを呼び出す"]},{"cell_type":"code","metadata":{"id":"4fmhFwiYEtXi"},"source":["#新たなデータを格納\n","X_new = np.array([[5, 2.9, 1, 0.2]])  #sciklit-learnは常に2次元Numpy配列の入力を前提としているため\n","#予測を行う\n","prediction = knn.predict(X_new)\n","#結果を表示\n","print('Prediction: {}'.format(prediction))\n","print('Predicted target name: {}'.format(iris_dataset['target_names'][prediction]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E27I7Ew4EtXl"},"source":["## モデル評価"]},{"cell_type":"markdown","metadata":{"id":"UEzhCgFtEtXm"},"source":["先ほど作ったテストデータのそれぞれのアイリスに対して予測を行う"]},{"cell_type":"code","metadata":{"id":"fUTo4z6qEtXn"},"source":["y_pred = knn.predict(X_test)\n","y_pred"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vFzPuk1CEtXp"},"source":["予測結果を正解のラベルと比較して精度を計算  \n","knnオブジェクトのscoreメソッドを用いる"]},{"cell_type":"code","metadata":{"id":"BiDQ3f41EtXp"},"source":["knn.score(X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1GEBto2NEtXt"},"source":["# サンプルデータセット\n","以下様々な機械学習アルゴリズムを紹介していくが、その際にいくつかのデータセットを用いるので、それらについて簡単に説明する\n","\n","まず、2クラス分類データセットとして、forgeデータセットを、回帰分析としてwaveデータセットを合成する\\\n","これらは、アルゴリズム的に機械合成されたデータである\n","\n","またより現実的なデータとして、先ほど示したアイリスの他、ウィスコンシン乳癌データ、ボストン住宅データを利用する"]},{"cell_type":"code","metadata":{"id":"dUffS-BFEtXu"},"source":["!pip install mglearn\n","import mglearn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fg86QqxSuQ4v"},"source":["make_forgeによりデータを合成する"]},{"cell_type":"code","metadata":{"id":"_IRWim2sEtXv"},"source":["# データセットの生成\n","X, y = mglearn.datasets.make_forge()\n","# プロット\n","mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n","plt.legend(['Class 0', 'Class 1'], loc=4)\n","plt.xlabel('First feature')\n","plt.ylabel('Second feature')\n","print('X.shape:{}'.format(X.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iIbjUB2PEtXy"},"source":["回帰アルゴリズムでは、waveデータセットを合成して用いる"]},{"cell_type":"code","metadata":{"id":"HrlcnKZ6EtXy"},"source":["X, y = mglearn.datasets.make_wave(n_samples=40)\n","plt.plot(X, y, 'o')\n","plt.ylim(-3, 3)\n","plt.xlabel('Feature')\n","plt.ylabel('Target')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"flk_M36MEtX0"},"source":["これらの小さい合成データセットの他に、scikit-learnに含まれている2つの実問題から取ったデータセットを用いる\n","\n","１つ目は、ウィスコンシン乳癌データセットで、癌の腫瘍に良性(benign)か悪性(malignant)かのラベルがつけられている"]},{"cell_type":"code","metadata":{"id":"eQi_OyPGEtX0"},"source":["from sklearn.datasets import load_breast_cancer\n","cancer = load_breast_cancer()\n","cancer.keys()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"whNMjgerEtX2"},"source":["print(\"Shape of cancer data:\", cancer.data.shape)\n","print(\"Sample counts per class:\", {n: v for n, v in zip(cancer.target_names, np.bincount(cancer.target))})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VKItKq5eEtX5"},"source":["30の特徴量を持つ569のデータポイントで構成される\n","- そのうち212が悪性、357が良性"]},{"cell_type":"markdown","metadata":{"id":"ULueKvpNEtX6"},"source":["２つ目は、お馴染みboston_housingデータセットである\n","- これは、1970年代のボストン郊外の住宅価格の中央値を、犯罪率、チャールズ川からの距離、高速道路への利便性などから予測する"]},{"cell_type":"code","metadata":{"id":"xVUnr153EtX7"},"source":["from sklearn.datasets import load_boston\n","boston = load_boston()\n","boston.data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xDPxlCPpEtX9"},"source":["13の特徴量を持つ、506のデータポイントが含まれる\\\n","ここでは、13の特徴量だけではなく、これらの積も特徴量として考える（特徴量エンジニアリング）"]},{"cell_type":"code","metadata":{"id":"elI1Gxn7EtX-"},"source":["X, y = mglearn.datasets.load_extended_boston()\n","X.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2tsS52rLEtXs"},"source":["# 教師あり学習"]},{"cell_type":"markdown","metadata":{"id":"5rDOKOjUEtXt"},"source":["## 教師あり学習について\n"," \n","- クラス分類　…クラスラベルを予測\n"," - ２クラス分類\n"," - 多クラス分類\n","- 回帰　…連続値を予測"]},{"cell_type":"markdown","metadata":{"id":"OhlszcJ7EtX_"},"source":["## k-近傍法\n","k-近傍法は最も単純な学習アルゴリズムと言われる\\\n","forgeデータセットに対するクラス分類の例を示す\n","\n","k = 1 の場合の例"]},{"cell_type":"code","metadata":{"id":"vRZY_aTIEtX_"},"source":["mglearn.plots.plot_knn_classification(n_neighbors=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rvKlZat5EtYB"},"source":["k = 3 の場合の例"]},{"cell_type":"code","metadata":{"id":"mC_aAS0pEtYC"},"source":["mglearn.plots.plot_knn_classification(n_neighbors=3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dRtUJvODEtYE"},"source":["**例題**：forgeデータを訓練セットとテストセットに分割し、k-近傍法アルゴリズムを適用してモデルの汎化性能（精度）を評価せよ\\\n","random_state = 0, k = 3"]},{"cell_type":"code","metadata":{"id":"9u1UAhG3EtYE"},"source":["# 解\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","X, y = mglearn.datasets.make_forge()\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n","clf = KNeighborsClassifier(n_neighbors=3)\n","clf.fit(X_train, y_train)\n","print(\"Test set predictions:\", clf.predict(X_test))\n","print(\"Test set accuracy: {:.2f}\".format(clf.score(X_test, y_test)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-l5ucK12EtYG"},"source":["kの値を変化させる\\\n","多くの近傍点を考慮するほど複雑度が低いモデルに対応する"]},{"cell_type":"code","metadata":{"id":"5r_iOGO0EtYG"},"source":["fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n","for n_neighbors, ax in zip([1, 3, 9], axes):\n","  # the fit method returns the object self, so we can instantiate\n","  # and fit in one line\n","  clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X, y)\n","  mglearn.plots.plot_2d_separator(clf, X, fill=True, eps=0.5, ax=ax, alpha=.4)\n","  mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n","  ax.set_title(\"{} neighbor(s)\".format(n_neighbors))\n","  ax.set_xlabel(\"feature 0\")\n","  ax.set_ylabel(\"feature 1\")\n","axes[0].legend(loc=3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l7NJ7d5lEtYJ"},"source":["## 線形回帰"]},{"cell_type":"markdown","metadata":{"id":"2mBW-6EgEtYK"},"source":["既に学習済みなので、シンプルに済ませる"]},{"cell_type":"code","metadata":{"id":"IDEfYx8tEtYK"},"source":["mglearn.plots.plot_linear_regression_wave()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dHaBCOy1EtYM"},"source":["from sklearn.linear_model import LinearRegression\n","X, y = mglearn.datasets.make_wave(n_samples = 60)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n","\n","# モデルを適用\n","lr = LinearRegression().fit(X_train, y_train)\n","\n","# 傾きを表すパラメータである重み（係数）はcoef_属性、切片はintercept_属性に格納\n","print(\"lr.coef_:\", lr.coef_)\n","print(\"lr.intercept_:\", lr.intercept_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-RQZefDPEtYO"},"source":["print('Training set score: {:.2f}'.format(lr.score(X_train, y_train)))\n","print('Test set score: {:.2f}'.format(lr.score(X_test, y_test)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PhHgBoKlEtYP"},"source":["精度はよくないが、訓練データとテストデータに対する値が非常に近いことから、過剰適合ではなく適合不足であると考えられる\n","- つまり、この問題のモデルとしては線形回帰は適していない、ということになる\n","このような1次元データセットでは過剰適合の危険は少ないが、高次元になる（データセットが多くの特徴量を持つ）と過剰適合の可能性が高まる\n","\n","次に、より複雑なデータセットに対する挙動を見てみる"]},{"cell_type":"markdown","metadata":{"id":"bwEXwnDyEtYQ"},"source":["**例題**: boston_housingのデータセットを読み込み、訓練セットとテストセットに分割し、線形回帰モデルを作り、汎化性能を評価せよ"]},{"cell_type":"code","metadata":{"id":"5q4pgjQWEtYR"},"source":["X, y = mglearn.datasets.load_extended_boston()\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n","lr = LinearRegression().fit(X_train, y_train)\n","print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\n","print(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AJ5CSLG9EtYU"},"source":["このように、訓練セットとテストセットで性能が大きく異なるのは、過剰適合が起きている証拠である\\\n","モデルの複雑度を制御できる線形回帰として、リッジ回帰やLassoがあるが、ここでは特に触れない"]},{"cell_type":"markdown","metadata":{"id":"MN1aDla_EtYU"},"source":["## 決定木"]},{"cell_type":"markdown","metadata":{"id":"Gn8rsHIpEtYV"},"source":["決定木はクラス分類や回帰タスクに広く用いられているモデルである"]},{"cell_type":"code","metadata":{"id":"KY5zKA45EtYW"},"source":["mglearn.plots.plot_animal_tree()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yVPpz1uTEtYX"},"source":["two_moonsデータセットを用いて決定木の構築過程を見る\n","\n","決定木における学習とは、最も早く正解にたどり着けるような一連のYes/No型の質問の学習である  \n","- まず、目的変数に対して最も情報量が多い（＝よく分割する）テストを選択する\n","- ここでは、x[1] <= 0.0596 が選ばれた  \n","\n","このプロセスを個々の領域に対して再帰的に繰り返すことで、２分木による決定木が得られる\n","\n","データの再帰分割は，一つの領域に一つのクラス（または回帰値）しか含まれなくなるまで繰り返される(これは変更できる)"]},{"cell_type":"code","metadata":{"id":"v6v1GsPYEtYX"},"source":["mglearn.plots.plot_tree_progressive()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PfhwVRLoEtYZ"},"source":["cancerデータセットを用いて完全な木を構築してみる"]},{"cell_type":"code","metadata":{"id":"vGXfkjbHEtYa"},"source":["from sklearn.tree import DecisionTreeClassifier\n","cancer = load_breast_cancer()\n","X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)\n","tree = DecisionTreeClassifier(random_state = 0)\n","tree.fit(X_train, y_train)\n","print('Accuracy on training set: {:.3f}'.format(tree.score(X_train, y_train)))\n","print('Accuracy on test set: {:.3f}'.format(tree.score(X_test, y_test)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AMZc1XWWEtYc"},"source":["訓練セットに対する精度は100％、テストセットに対する精度は94％程度となった\n","- 決定木の深さに制約を与えないと、決定木はいくらでも深く複雑になり、新しいデータに対する汎化性能が低くなる\n","- そこで、木が完全に訓練データに適合する前に木の成長を止めてみる\n","- ここでは max_depth = 4（質問の列は４つまでということ）とする"]},{"cell_type":"code","metadata":{"id":"U2dP5a01EtYd"},"source":["tree = DecisionTreeClassifier(max_depth=4, random_state=0)\n","tree.fit(X_train, y_train)\n","\n","print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n","print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OnzCZgukEtYf"},"source":["## ランダムフォレスト"]},{"cell_type":"markdown","metadata":{"id":"NLSMuIN8EtYg"},"source":["ランダムフォレストをtwo_moonsデータセットに適用してみる"]},{"cell_type":"code","metadata":{"id":"p2hDOPiWEtYg"},"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import make_moons\n","X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n","\n","forest = RandomForestClassifier(n_estimators=5, random_state=2)\n","forest.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"biq9D5ctEtYj"},"source":["fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n","for i, (ax, tree) in enumerate(zip(axes.ravel(), forest.estimators_)):\n","  ax.set_title('Tree {}'.format(i))\n","  mglearn.plots.plot_tree_partition(X_train, y_train, tree, ax=ax)\n","mglearn.plots.plot_2d_separator(forest, X_train, fill=True, ax=axes[-1, -1], alpha=.4)\n","axes[-1, -1].set_title('Random Forest')\n","mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yM0oRVerEtYk"},"source":["５つのランダム化された決定木による決定境界と、それらを平均して得られた決定境界を示す\n","- 個々のどの決定木よりも過剰適合が少なく，直観に合致した決定境界を描いている"]},{"cell_type":"markdown","metadata":{"id":"e9zqgNf2EtYt"},"source":["**例題**: cancerデータセットに対して100個の決定木を用いたランダムフォレストを適用し、精度を確かめよ"]},{"cell_type":"code","metadata":{"id":"G-xqmpueEtYt"},"source":["X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=0)\n","forest = RandomForestClassifier(n_estimators=100, random_state = 0)\n","forest.fit(X_train, y_train)\n","forest.score(X_train, y_train), forest.score(X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tNUrUUEbEtYv"},"source":["## サポートベクタマシン(SVM)"]},{"cell_type":"markdown","metadata":{"id":"XtbKzP9DEtYv"},"source":["線形SVMモデルをforgeデータセットに適用してみる"]},{"cell_type":"code","metadata":{"id":"3UKuwBJ1EtYv"},"source":["from sklearn.svm import LinearSVC\n","linear_svc = LinearSVC()\n","X, y = mglearn.datasets.make_forge()\n","clf = linear_svc.fit(X, y)\n","mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5,alpha=.7)\n","mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n","plt.title(clf.__class__.__name__)\n","plt.xlabel(\"Feature 0\")\n","plt.ylabel(\"Feature 1\")\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gSIQ_WJ-EtYx"},"source":["SVCにおけるハイパーパラメータCを変えてみる"]},{"cell_type":"code","metadata":{"id":"KIs0PHRtEtYx"},"source":["mglearn.plots.plot_linear_svc_regularization()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gy_GvVGPEtYz"},"source":["右側ほどすべての点を正しく分類することを重視するあまり、全体を捉えられていない\\\n","おそらく過剰適合している"]},{"cell_type":"markdown","metadata":{"id":"66zjGtAAEtY0"},"source":["以下に示すように、線形分離が不可能な例も存在する"]},{"cell_type":"code","metadata":{"id":"_rpHSkTLEtY0"},"source":["from sklearn.datasets import make_blobs\n","X, y = make_blobs(centers=4, random_state=8)\n","y = y % 2\n","mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n","plt.xlabel(\"Feature 0\")\n","plt.ylabel(\"Feature 1\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"znXN_c_-EtY1"},"source":["from sklearn.svm import LinearSVC\n","linear_svm = LinearSVC().fit(X, y)\n","mglearn.plots.plot_2d_separator(linear_svm, X)\n","mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n","plt.xlabel(\"Feature 0\")\n","plt.ylabel(\"Feature 1\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"owj7hvj9EtY3"},"source":["そこで、入力特徴量を拡張する\\\n","例えば，２番目の特徴量の２乗を新たな特徴量とする"]},{"cell_type":"code","metadata":{"id":"TWqjGb28EtY3"},"source":["# add the squared first feature\n","X_new = np.hstack([X, X[:, 1:] ** 2])\n","from mpl_toolkits.mplot3d import Axes3D, axes3d\n","figure = plt.figure()\n","# visualize in 3D\n","ax = Axes3D(figure, elev=-152, azim=-26)\n","# plot first all the points with y==0, then all with y == 1\n","mask = y == 0\n","ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n","           cmap=mglearn.cm2, s=60, edgecolor='k')\n","ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n","           cmap=mglearn.cm2, s=60, edgecolor='k')\n","ax.set_xlabel(\"feature0\")\n","ax.set_ylabel(\"feature1\")\n","ax.set_zlabel(\"feature1 ** 2\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s7NnUIKIEtY4"},"source":["この拡張されたデータセットに対して線形モデルを適用する"]},{"cell_type":"code","metadata":{"id":"uwRVJ0LqEtY5"},"source":["linear_svm_3d = LinearSVC().fit(X_new, y)\n","coef, intercept = linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_\n","# show linear decision boundary\n","figure = plt.figure()\n","ax = Axes3D(figure, elev=-152, azim=-26)\n","xx = np.linspace(X_new[:, 0].min() - 2, X_new[:, 0].max() + 2, 50)\n","yy = np.linspace(X_new[:, 1].min() - 2, X_new[:, 1].max() + 2, 50)\n","XX, YY = np.meshgrid(xx, yy)\n","ZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]\n","ax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)\n","ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n","           cmap=mglearn.cm2, s=60, edgecolor='k')\n","ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n","           cmap=mglearn.cm2, s=60, edgecolor='k')\n","ax.set_xlabel(\"feature0\")\n","ax.set_ylabel(\"feature1\")\n","ax.set_zlabel(\"feature1 ** 2\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZQ2Bb06rEtY6"},"source":["これを元の空間で見ると、決定境界が直線から曲線になっていることが分かる"]},{"cell_type":"code","metadata":{"id":"g401RfF3EtY7"},"source":["ZZ = YY ** 2\n","dec = linear_svm_3d.decision_function(np.c_[XX.ravel(), YY.ravel(), ZZ.ravel()])\n","plt.contourf(XX, YY, dec.reshape(XX.shape), levels=[dec.min(), 0, dec.max()],\n","             cmap=mglearn.cm2, alpha=0.5)\n","mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n","plt.xlabel(\"Feature 0\")\n","plt.ylabel(\"Feature 1\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sy-eQfpZEtY8"},"source":["以上、カーネルトリックの効果を確認した\n","\n","ここで、多項式カーネルやガウシアンカーネル(RBFカーネルとも呼ばれる)があることは述べたが、forgeデータセットに対してRBFカーネルを用いたSVMによる決定の様子を見てみる"]},{"cell_type":"code","metadata":{"id":"0pby1gCGEtY8"},"source":["from sklearn.svm import SVC\n","X, y = mglearn.tools.make_handcrafted_dataset()                                                                  \n","svm = SVC(kernel='rbf', C=10, gamma=0.1).fit(X, y)\n","mglearn.plots.plot_2d_separator(svm, X, eps=.5)\n","mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n","# plot support vectors\n","sv = svm.support_vectors_\n","# class labels of support vectors are given by the sign of the dual coefficients\n","sv_labels = svm.dual_coef_.ravel() > 0\n","mglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3)\n","plt.xlabel(\"Feature 0\")\n","plt.ylabel(\"Feature 1\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q8Olo4TuEtY9"},"source":["この場合、SVMによる境界は非常になめらかで非線形である\\\n","ここでは、Cとgammaの2つのパラメータを調整している\n","\n","gammaパラメータはガウシアンカーネルの幅を調整する（点が近いということを意味するスケールを決定する）\\\n","Cパラメータは正規化パラメータで、個々のデータポイントの重要度を制限する\\\n","これらを変化させる"]},{"cell_type":"code","metadata":{"id":"wN9yefLWEtY-"},"source":["fig, axes = plt.subplots(3, 3, figsize=(15, 10))\n","for ax, C in zip(axes, [-1, 0, 3]):\n","  for a, gamma in zip(ax, range(-1, 2)):\n","    mglearn.plots.plot_svm(log_C=C, log_gamma=gamma, ax=a)   \n","axes[0, 0].legend([\"class 0\", \"class 1\", \"sv class 0\", \"sv class 1\"],\n","                  ncol=4, loc=(.9, 1.2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VD9gN2fhEtY_"},"source":["gammaが小さいとカーネルの直径が大きくなり、多くの点を近いと判断する\\\n","Cを大きくすると、個々のデータポイントが決定境界に強い影響を与えるようになる"]},{"cell_type":"markdown","metadata":{"id":"8tzr0gaLEtY_"},"source":["## ニューラルネットワーク\n","\n","ニューラルネットワークが近年「ディープラーニング」という名前で注目を集めている\\\n","ここでは、比較的簡単な多層パーセプトロン（Multilayer Perceptron：MLP）によるクラス分類についてのみ取り扱う"]},{"cell_type":"markdown","metadata":{"id":"YVjCfzEjEtZA"},"source":["線形回帰では、入力特徴量の重み付き和によって予測を行っていた\\\n","MLPでは、重み付けが繰り返し行われる\\\n","中間処理ステップである隠れユニットの計算で重み付き和が行われ、この隠れユニットの値に対して重み付き和が行われて、最後の結果が算出される"]},{"cell_type":"markdown","metadata":{"id":"oM8xHY5NEtZB"},"source":["two_moonsデータセットに対してMLPが動く様子を見てみる"]},{"cell_type":"code","metadata":{"id":"z3BFVpLWEtZB"},"source":["from sklearn.neural_network import MLPClassifier\n","from sklearn.datasets import make_moons\n","X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n","mlp = MLPClassifier(solver='lbfgs', random_state=0).fit(X_train, y_train)\n","mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\n","mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n","plt.xlabel(\"Feature 0\")\n","plt.ylabel(\"Feature 1\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YylackeiEtZD"},"source":["これは、隠れユニットの数を100（デフォルト）としているが、この小さいデータセットに対しては大きすぎるため、隠れユニット数を10としてモデルの複雑さを減らす"]},{"cell_type":"code","metadata":{"id":"z-LFK9sZEtZD"},"source":["mlp = MLPClassifier(solver='lbfgs', random_state=0, hidden_layer_sizes=[10])\n","mlp.fit(X_train, y_train)\n","mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\n","mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n","plt.xlabel(\"Feature 0\")\n","plt.ylabel(\"Feature 1\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"heEae8zVEtZH"},"source":["ニューラルネットワークの複雑さは、隠れ層の数、隠れ層のユニットの数、重みを0に近づける(正規化)などによって制御できる\\\n","正規化の強さはパラメータalphaで決定される\\\n","10ユニットもしくは100ユニットの2層の隠れ層を持つニューラルネットをtwo_moonsに適用した場合のパラメータalphaの効果を以下に示す"]},{"cell_type":"code","metadata":{"id":"t0pICOQ6EtZH"},"source":["fig, axes = plt.subplots(2, 4, figsize=(20, 8))\n","for axx, n_hidden_nodes in zip(axes, [10, 100]):\n","  for ax, alpha in zip(axx, [0.0001, 0.01, 0.1, 1]):\n","    mlp = MLPClassifier(solver='lbfgs', random_state=0, hidden_layer_sizes=[n_hidden_nodes, n_hidden_nodes], alpha=alpha)\n","    mlp.fit(X_train, y_train)\n","    mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3, ax=ax)\n","    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax)\n","    ax.set_title(\"n_hidden=[{}, {}]\\nalpha={:.4f}\".format(n_hidden_nodes, n_hidden_nodes, alpha))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"slVeYqznEtZI"},"source":["# 教師なし学習"]},{"cell_type":"markdown","metadata":{"id":"B-10RQTHEtZJ"},"source":["## 教師なし学習の種類\n","\n","- 教師なし変換\n"," - 次元削減\n","- クラスタリングアルゴリズム"]},{"cell_type":"markdown","metadata":{"id":"jI-xtKFoEtZJ"},"source":["## k-meansクラスタリング\n","最も単純なクラスタリングアルゴリズム  \n","- 個々のデータポイントを最寄りのクラスタ重心に割り当てる\n","- 個々のクラスタ重心をその点に割り当てられたデータポイントの平均に設定する\n","\n","というステップを繰り返し、割り当てが変化しなくなったらアルゴリズムは終了する\n","\n","合成データセットにこれを適用してみる"]},{"cell_type":"code","metadata":{"id":"LMvgX1XVEtZK"},"source":["mglearn.plots.plot_kmeans_algorithm()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a2slFa9cEtZK"},"source":["mglearn.plots.plot_kmeans_boundaries()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uzWjyENMEtZM"},"source":["from sklearn.datasets import make_blobs\n","from sklearn.cluster import KMeans\n","# generate synthetic two-dimensional data\n","X, y = make_blobs(random_state=1)\n","# build the clustering model\n","kmeans = KMeans(n_clusters=3)\n","kmeans.fit(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W8bMG8dPEtZN"},"source":["mglearn.discrete_scatter(X[:, 0], X[:, 1], kmeans.labels_, markers='o')\n","mglearn.discrete_scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], [0, 1, 2], markers='^', markeredgewidth=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XRg4xbVJEtZP"},"source":["fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n","# using two cluster centers:\n","kmeans = KMeans(n_clusters=2)\n","kmeans.fit(X)\n","assignments = kmeans.labels_\n","mglearn.discrete_scatter(X[:, 0], X[:, 1], assignments, ax=axes[0])\n","# using five cluster centers:\n","kmeans = KMeans(n_clusters=5)\n","kmeans.fit(X)\n","assignments = kmeans.labels_\n","mglearn.discrete_scatter(X[:, 0], X[:, 1], assignments, ax=axes[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1muB1FkCEtZR"},"source":["以下は上手くいかない例である"]},{"cell_type":"code","metadata":{"id":"xsgZS1zZEtZR"},"source":["# generate some random cluster data\n","X, y = make_blobs(random_state=170, n_samples=600)\n","rng = np.random.RandomState(74)\n","# transform the data to be stretched\n","transformation = rng.normal(size=(2, 2))\n","X = np.dot(X, transformation)\n","# cluster the data into three clusters\n","kmeans = KMeans(n_clusters=3)\n","kmeans.fit(X)\n","y_pred = kmeans.predict(X)\n","# plot the cluster assignments and cluster centers\n","mglearn.discrete_scatter(X[:, 0], X[:, 1], kmeans.labels_, markers='o')\n","mglearn.discrete_scatter(\n","  kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], [0, 1, 2],\n","  markers='^', markeredgewidth=2)\n","plt.xlabel(\"Feature 0\")\n","plt.ylabel(\"Feature 1\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GcPh05PcEtZS"},"source":["# generate synthetic two_moons data (with less noise this time)\n","from sklearn.datasets import make_moons\n","X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n","# cluster the data into two clusters\n","kmeans = KMeans(n_clusters=2)\n","kmeans.fit(X)\n","y_pred = kmeans.predict(X)\n","# plot the cluster assignments and cluster centers\n","plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm2, s=60, edgecolor='k')\n","plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n","            marker='^', c=[mglearn.cm2(0), mglearn.cm2(1)], s=100, linewidth=2,\n","            edgecolor='k')\n","plt.xlabel(\"Feature 0\")\n","plt.ylabel(\"Feature 1\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WPVkjtwkEtZT"},"source":["このように、k-meansはクラスタが丸くない場合や複雑な形状の場合にはうまく機能しない"]},{"cell_type":"markdown","metadata":{"id":"oR-rMNcuEtZU"},"source":["## 教師なし学習の難しさ\n","教師なし学習の難しさは、アルゴリズムが学習したことの有用性の評価にある\\\n","教師なし学習で与えるデータはラベルが分かっていないデータであるため、出力が正解かどうかを判断することができず、結果が人間が求めているものとなっているかを人間が確かめるしかない \n","\n","参考として、次のある学生のスライドを紹介しておく\n","\n","[スライドはこちら](https://www.slideshare.net/ngkry/ss-51045351)\n","\n","なお、論文になっている\n","\n","[論文はこちら](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwj8u4PM78PsAhUGM94KHT1IDfIQFjAAegQIBxAC&url=https%3A%2F%2Fipsj.ixsq.nii.ac.jp%2Fej%2Findex.php%3Faction%3Dpages_view_main%26active_action%3Drepository_action_common_download%26item_id%3D176518%26item_no%3D1%26attribute_id%3D1%26file_no%3D1%26page_id%3D13%26block_id%3D8&usg=AOvVaw3iiWXIfl2c_uSq3zhFjb99)"]},{"cell_type":"markdown","metadata":{"id":"7ms3Uc42SU_1"},"source":["# 課題5(データの分類)\n","いつもの通りの提出方法でお願いします\n","\n","**[課題]**\n","- 上記のcancerおよびtwo_moonデータセットについて、ランダムフォレストおよび、既に扱ったLightGBMそれぞれについて分類し、評価しなさい\n","- 比較は、cancer、two_moonデータセット100個、two_moonデータセット300個の3種類、それぞれランダムフォレスト、Light_GBMを用いるため、全部で6個の結果を用いて評価する\n","- その上で、どちらが優れているかを比較しなさい\n","\n","なお、解答にあたっては次の点に注意しなさい\n","- 単純な比較が行われ、結果が妥当ではない場合でも特に問題はない\n","- 本来は、例えば交差検証を行う、何度か試みて平均をとるなどしなければ、論文やきちんとした評価などでは通用しないが、ここでは厳密さは問わない\n","- コードと結果を示し、最後に、それらを纏めて、単純にランダムフォレストとLightGBMのどちらが優れていると考えられるかだけ記述しなさい\n","\n","上記で解答が困難な場合は、以下にcancerのランダムフォレストとLightGBMの参考コードを記載する\n","- このコードはそのまま課題の解答の一部として用いてよい\n","- このコードを書き替えて、two_moonの100および300について分類し、その精度を求めれば課題は成立する\n","\n","- わからない！という場合の多くは各関数の意味が理解できていないのが原因と思われるので、まずはググること\n","- mglernのコードは課題では求めておらず不要である\n","  - 下記余裕がある場合を参照\n","- テンプレがあれば1から素で書けるようになること\n","  - これからの課題や試験が太刀打ちできなくなります\n","  - コードを見てわかる通り、実質データを準備するのに2行、モデルの準備に1行、モデルの構築(学習)に1行、推定に1行、アキュラシーを求めるのに1行ですので、ここで根をあげないように\n","    - 高々6行でできるところに注目し、6行に振り回されないように\n","    - PyTorchはそうはいきませんので\n","\n","**(余裕がある場合)**\n","\n","相変わらずのコピペ課題だなぁ、と感じる学生もいるであろう\n","  - 余裕がある人は、答えを使った混同行列やF値、MCCといった評価が行えるので試してみると良い\n","- 敢えてmglernのコードを追加しているのは、なぜ〇〇が〇〇でよくないのか？を知るためである\n","  - 是非その理由を考察してみると良い\n","  - mglernを用いればすべてがわかるというわけではない。他のツールを屈指して、本質に迫ると良いであろう\n","  - Kagglerでも大人気、最強といわれるLightGBMだけに頼ることができないことがよくわかるであろう\n","- LightGBMは、先に示した例が回帰であったように、回帰にも2値・多値分類にも使える万能選手である\n","  - LightGBMのマニュアルにある、objectiveを調べると良い\n","- さらに余裕のある人は、LightGBMのランダムフォレストモードを利用してみると良いだろう\n","  - ランダムフォレストと結果は同じになるだろうか？"]},{"cell_type":"code","metadata":{"id":"9W_Yl_9z2ABx"},"source":["!pip install mglearn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"57sgahHy3XIt"},"source":["## ランダムフォレスト cancer"]},{"cell_type":"code","metadata":{"id":"j2dtKekx3FfV"},"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","cancer = load_breast_cancer()\n","X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)\n","forestc = RandomForestClassifier(n_estimators=6, random_state=2)\n","forestc.fit(X_train, y_train)\n","print('Accuracy on training set: {:.3f}'.format(forestc.score(X_train, y_train)))\n","print('Accuracy on test set: {:.3f}'.format(forestc.score(X_test, y_test)))\n","x_pred = forestc.predict(X_test) # forestc.score(X_test, y_test)としても一緒、書き下すとこういう意味、答えも一緒\n","acc_rf_c = (y_test==x_pred).sum()/len(y_test)\n","acc_rf_c"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DpeEb8DF3GNr"},"source":["import mglearn\n","mglearn.discrete_scatter(X_test[:, 0], X_test[:, 1], y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VfqI8RWm3HxD"},"source":["mglearn.discrete_scatter(X_test[:, 0], X_test[:, 1], x_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U__vRR0Z3Yl_"},"source":["## LightGBM cancer"]},{"cell_type":"markdown","metadata":{"id":"phRvYnnyjZbS"},"source":["以下を実行すると、おそらく、「No further splits with positive gain, best gain: -inf」とwarningが表示されるであろう\n"," - LightGBMが内部で決定木を成長させてる際、pre-pruning（特徴空間をそれ以上分割しても情報利得が得られず分割を停止する機能）が働いたことを示す\n"," - 決定木の成長アルゴリズムとして次の2つがあり、一般的にdepth-firstが利用されるのに対してLightGBMはbest-firstを利用する\n","  - depth-first (level-wise) ：順番固定で通常左から右の順で伸ばす\n","  - best-first (leaf-wise) ：分岐することでより不純度・予測誤差を下げることがえきる枝から伸ばす\n"," - なお、完全に成長させれば差はなく同じ木構造になる\n"," \n"," - 決定木の剪定アルゴリズム、つまり木を無駄に成長させないようにする手法には次の2種類がある\n","  - pre-pruning：追加で分岐して予測誤差を下げられる時だけ分岐、最適な木を発見できない可能性があるが、post-pruningよりも計算コストが小さい(LightGBMで利用)\n","  - post-pruning：まず決定木を完全に成長させた後に最も予測誤差を下げる木のサイズを選択\n","\n","- つまり、最良の分岐を探索する際に、分岐による情報利得が得られない（それ以上分割しても得をしない）ところまできた、ということを意味する\n"]},{"cell_type":"code","metadata":{"id":"vl3mLjyf3LKH"},"source":["import lightgbm as lgb\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","cancer = load_breast_cancer()\n","X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)\n","dtrain = lgb.Dataset(X_train, label=y_train)\n","dtest = lgb.Dataset(X_test, label=y_test)\n","params={\n","  'objective': 'binary',\n","  'random_state': 42,\n","  'metric': 'binary_logloss'\n","}\n","bstc = lgb.train(params, dtrain, num_boost_round=1000,valid_sets=[dtrain, dtest],verbose_eval=100)\n","print(\"Best Score:\", bstc.best_score[\"valid_1\"]['binary_logloss'])\n","x_pred = (bstc.predict(X_test)>0.5)\n","acc_lg_c = (y_test==x_pred).sum()/len(y_test) # こちらは、scoreメソッドが存在しない\n","acc_lg_c"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FfuZjIA_qnPR"},"source":["### Accuracyの演算について\n","\n","ランダムフォレストは、scoreを用いることができた\n","```\n","forestc.score(X_test, y_test)\n","```\n","LightGBMには、scoreがない、そこで同じ意味で次のように記述した\n","\n","```\n","(y_test==x_pred).sum()/len(y_test)\n","```\n","もう一つ、次の記述の方法も取得済みである\n","- 実行してみよう\n","- すべて同じ値を出力するので、ランダムフォレストの例で試してみると良いであろう"]},{"cell_type":"code","metadata":{"id":"kQ5_qxfnp-aE"},"source":["from sklearn.metrics import accuracy_score\n","accuracy_score(y_test, x_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I9RW2iJc3M2w"},"source":["import mglearn\n","mglearn.discrete_scatter(X_test[:, 0], X_test[:, 1], y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cDFaYfjF3OMv"},"source":["mglearn.discrete_scatter(X_test[:, 0], X_test[:, 1], x_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T6pejA323RuD"},"source":["## 比較のまとめ"]},{"cell_type":"code","metadata":{"id":"2CAI_vx73Q2j"},"source":["print(\"ランダムフォレスト cancer:\", acc_rf_c)\n","print(\"LightGBM cancer:\", acc_lg_c)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iqUa2hyN2myB"},"source":["他の比較も同様に加えること"]}]}