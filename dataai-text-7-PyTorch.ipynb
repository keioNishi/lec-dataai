{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dataai-text-7-PyTorch.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"lP3nM7Vif72v"},"source":["---\n",">闇の夜道に松明\\\n",">蓋し、灯されしもののあないみじや\n","---"]},{"cell_type":"markdown","metadata":{"id":"ai_AeQdN1sff"},"source":["# PyTorch\n","\n","PyTorch は科学技術計算向けパッケージであり、\n","- GPUを使った高速計算が可能\n","  - 普通やらないがNumPyの代わりに使える\n","  - NumPyをGPU付きで使いたいならCuPyがある\\\n","    PyTorchによりも先に登場したPFNのChainerはCuPyを使う\n","- 柔軟かつ高速にディープラーニングプラットフォームを構築可能\n","である。\n","\n","https://pytorch.org/ にほぼすべての情報がある。\n","\n","英語ではあるが、関連するドキュメントが十分かつ分かりやすく準備されている\n","\n","PyTorchの以前のロゴは次の通り\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/pytorchlogo.jpeg\" width=\"50%\">\n","\n","リングフィットアドベンチャーに似ているので変えたとか変えないとか\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/rfa.png\" width=\"20%\">"]},{"cell_type":"markdown","metadata":{"id":"bwFkktPsj5nw"},"source":["## PyTorchで線形回帰\n","\n","PyTorchを用いても線形回帰（Linear Regression）を実現できる\n","\n","ここでは、Deep Learning実装で用いるのが主たる目的であるPyTorchを使って、線形回帰を実装する\n","\n","- データとして、手入力の配列データで実装する\n","\n","- ハイパーパラメータについても、最初に宣言しておくので、値を変えて様々トライしてみるとよいであろう"]},{"cell_type":"markdown","metadata":{"id":"rqKpfbl7i8yd"},"source":["# PyTorchの実装例\n","\n","PyTorchについて学んでいくが、まずはPyTorchを用いた機械学習コードのその実行イメージを掴む\n","- コードの各行の意味にこだわらず、全体の雰囲気を感じ取ること\n","  - 英語も文法から入るとつまらないし、自由に使いこなすという観点からは良い方法といえない\n","  - 使うことを念頭に学ぶのであれば、英語を聞いて話すのと同様、細かいことにこだわらず、コードを見て書けばよい\n","- 次回のノートブックで基本的なテンソルの扱い方などを学ぶが、そういう内容を深く知らなくてもなんとなくできてしまうという感覚を身につける\n","  - 自分が何を学んでいるのか、何を学ばないといけないのかを知るには、適度にシンプルな最終形を示すのが一番\n"]},{"cell_type":"code","metadata":{"id":"3vXRrGd-kIFW"},"source":["import torch            \n","import torch.nn as nn\n","import numpy as np\n","import matplotlib.pyplot as plt\n","input_size = 1\n","output_size = 1\n","num_epochs = 1000\n","learning_rate = 0.002"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zLTkunP3kKys"},"source":["手入力で適当なデータセットを準備する\n","\n","- 既に用いたことのあるデータを利用してもよい\n","\n"," `nn.Linear` に対する入力は `(N,∗,in_features)` であるため `reshape` が必要となる\n","\n","- `N`は次元、`*`には任意の次元を追加できるが今回は1次元データであるため、特に指定しない\n","\n","表示して内容を確認する\n","\n","- 再びのPyTorchでmodelに食わせるためのreshapeであるが、`reshape(15, 1)`と同じこと"]},{"cell_type":"code","metadata":{"id":"yBW8mtaCkPEs"},"source":["x_train = np.array([3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167,\n","                    7.042, 10.791, 5.313, 7.997, 3.1], dtype=np.float32)\n","\n","y_train = np.array([1.7, 2.76, 2.09, 3.19, 1.694, 1.573, 3.366, 2.596, 2.53, 1.221,\n","                    2.827, 3.465, 1.65, 2.904, 1.3], dtype=np.float32)\n","print(\"ORG:\", x_train)\n","x_train = x_train.reshape(-1, 1)\n","y_train = y_train.reshape(-1, 1)\n","print(\"RESHAPE:\",x_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M_jMCVG0kUOE"},"source":["ここでは、おおよその記述スタイルを学ぶという観点から、詳細には触れず要点のみ触れる\n","\n","実装は、まずデータセットを準備し、次にモデルを構成する\n","\n","- データセットの準備(既に終了している)\n","- モデルの定義\n","- ロス関数の定義\n","- 最適化手法の定義\n","- 学習\n","  - 順伝搬で出力を計算(内部でforwardを呼び出す)\n","  - 出力値と正解値から誤差を計算\n","    - ロス関数の利用、内部でbackwordを呼び出すが計算式はmodelのforwardを求める際に自動で獲得されており、outputに含まれている\n","  - 重みによる誤差の偏微分値を計算\n","  - 誤差を逆伝搬\n","    - 最適化手法でstepを呼び出す\n","\n","ネットワークの構成は次の通り\n","\n","-  `nn.Module` を継承したクラスを作成  \n","クラスを継承して作成するのがPyTorchの設計の基本スタイルの一つ\n","- `__init__()` に層オブジェクトを定義  \n","コンストラクタを用いて、層のオブジェクトを定義\n","- `forward()` に順方向の処理"]},{"cell_type":"code","metadata":{"id":"3f-koMWLk4Kn"},"source":["class LinearRegression(nn.Module):\n","  def __init__(self, input_size, output_size):\n","    super(LinearRegression, self).__init__()\n","    self.linear = nn.Linear(input_size, output_size)\n","  def forward(self, x):\n","    out = self.linear(x)\n","    return out\n","model = LinearRegression(input_size, output_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rUBB7a1Cksim"},"source":["Loss計算とOptimizerについて\n","\n","- Loss計算について、線形回帰であるため、誤差は平均二乗誤差（mean squared error）を用いる\n","- Optimizerについて、ここではシンプルなSGD（Stochastic Gradient Descent）を指定する\n"]},{"cell_type":"code","metadata":{"id":"YUe-EnIqsJcl"},"source":["criterion = nn.MSELoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4_hR24T3sSg6"},"source":["学習について\n","\n","- PyTorchでは、学習部分の繰り返し、すなわちエポックの記述をforで実装するのが一般的\n","- 各エポックで、勾配のクリアを忘れずに行う\n","  - 実際には、zero_grad()を用いてクリアする\n","- パラメータは、optimizer.step()メソッドを用いて更新する\n","- ここでは、100エポック毎のlossを表示する\n","  - 一般的なif文の記述スタイルを用いている\n","- 最後にモデルを保存する\n","  - モデルの保存は、torch.saveを用いる\n","  - 学習済みモデルをmodel.state_dict()メソッドで取り出す\n","  - この学習済みモデルを保存し再利用することで、学習させずに学習結果を利用したアプリケーションが設計できる\n","\n"]},{"cell_type":"code","metadata":{"id":"UyCczJtasNVx"},"source":["for epoch in range(num_epochs):\n","    inputs = torch.tensor(x_train) # 新しい書き方に修正しています\n","    targets = torch.tensor(y_train)\n","    optimizer.zero_grad()\n","    outputs = model(inputs)\n","    loss = criterion(outputs, targets)\n","    loss.backward()\n","    optimizer.step()\n","    if (epoch + 1) % 100 == 0:\n","        print('Epoch [%d/%d], Loss: %.4f' % (epoch + 1, num_epochs, loss.item()))\n","# save the model\n","torch.save(model.state_dict(), 'model.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CVnbNo-Yuqve"},"source":["評価する\n","\n","- 予測結果と元のデータとを比較する\n","- 勾配(grad)の情報を保有するTensorはそのままnumpy arrayに変換できないため、detach()する"]},{"cell_type":"code","metadata":{"id":"2FPurWkgut5r"},"source":["predicted = model(torch.tensor(x_train)).detach().numpy()\n","plt.plot(x_train, y_train, 'ro', label='Original data')\n","plt.plot(x_train, predicted, label='Fitted line')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_1SpBP8xdYxO"},"source":["dictでモデルが取り出せるが、モデルのパラメタを別途直接指定したい、個別に取り出したい等の場合は、`torch.nn.Parameter`オブジェクトを扱う\n","\n","- `torch.nn.Parameter`クラスの`__init__`関数にはtorchテンソルを指定することで、全体を設定できる\n","\n","- 個別には次のようにする\n","  - `＜モデル名＞.＜レイヤー名＞.weight`プロパティに重みが指定できる\n","  - `＜モデル名＞.＜レイヤー名＞.baias`プロパティにバイアスが指定できる\n","\n","- 既に述べた通り重みやバイアスといったパラメーターなどの`torch.nn.Module`全体の状態は、`＜モデル名＞.state_dict()`メソッドで取得できる\n","  - パラメーターを最適化で使う際は専用の取得方法があり、`＜モデル名＞.parameters()`メソッドで取得する"]},{"cell_type":"code","metadata":{"id":"8rlBZY0NZTGi"},"source":["list(model.parameters())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s_-4n8K0daj3"},"source":["## PyTorchでNNによる分類\n","\n","### 学習データ\n","scikit-learnに含まれるワイン分類データセットを用いて、PyTorchでワインの等級分けを行う\n","\n","- このデータセットには、13個の特徴量が含まれている\n","- 出力は本来class_0, 1, 2の3種類であるが、今回はclass_0,1のみを判別するため2つ用いる\n","- その他は次のようなモデルを想定する\n","  - 入力層(x)は13個:（特徴量に等しい）\n","  - 隠れ層(fc1): 全結合\n","  - 出力層(fc2): 全結合\n","  - softmax層: 判定するラベル数と同数の2ノード\n","    - 確率が足して1になるように調整する\n"," - 出力 ラベル0と1の確率\n","\n","このネットワーク図は次の通り\n","- 記述とこの図の対応がとれるように\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/model1.png\" width=500>\n","\n","復習として、Softmax関数は、$d$次元のベクトル${\\bf y} \\in \\mathbb{R}^d$が与えられたとき、各次元の値の合計が1になるように正規化する\n","\n","- すなわち、確率分布のような出力を任意の実数ベクトルから作ることができる\n","\n","- ${\\bf y}$の$i$番目の次元を$y_i$と書くと，Softmax関数は\n","\n","$$\n","p_i = \\frac{e^{y_i}}{\\sum_{j=1}^d e^{y_j}}\n","$$\n","\n","と表せる\n","\n","### 学習\n","\n","学習の流れは次の通り\n","\n","- 順伝搬で出力を計算(図中①)\n","- 出力値と正解ラベルから誤差を計算(図中②)\n","- 重みによる誤差の偏微分値を計算(図中③)\n","- 誤差を逆伝搬(図中④)\n","- 更新した重みで500エポック計算\n","\n","実装は、まずデータセットを準備し、次にネットワークを構成する"]},{"cell_type":"markdown","metadata":{"id":"bgLS5hkei9MY"},"source":["### 前準備\n","`import torch`としてtorchを利用\n","\n","基本これだけでよいが、プログラムの記述上省略したほうが簡潔でよい、わかりやすいといった観点から、下記は必須ではないが、定義して省略形で使えるように準備する場合もある\n","- `import torch.nn as nn`: nnでニューラルネットワーク関数を参照\n","- `import tourch.nn.functional as F`： Fで活性化関数定義\n","- `import torch.optim as optim` ：optimで最適化関数定義\n","- `from torch.utils.data import DataLoader, TensorDataset` ：データローダ、データセット利用を簡略化\n","\n","簡単にpython文法を復習する\n","- `import A.B`とすると、Aの中のBという機能を利用できる\n","  - この場合、使うたびにA.Bと名前を指定する\n","  - A全体を読み込むより、A.Bしか読み込まないためメモリ利用効率は向上するが、利用の便宜上は何も変わらない\n","  - そこで、`from A import B`とすると、読み込みもA.BのみでBとして利用できる\n","- as はエイリアス(別名定義)である\n","  - `import A.B as C`とすれば、Cという名前で当該機能を利用できる\n","- 両方まとめて`from A.B import C.D as E`とできる"]},{"cell_type":"code","metadata":{"id":"KV_Wdt-fdWrJ"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UpKGtR1EkzTT"},"source":["scikit-learnをデータセット入手だけに利用する\n","- ワインのデータセット、およびデータセットから教師データとテストデータを分離する関数もscikit-learn提供の機能を利用する"]},{"cell_type":"code","metadata":{"id":"SNr72zzVkmQ4"},"source":["from sklearn.datasets import load_wine\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZJv3zaiBnS5Z"},"source":["pandasも利用する"]},{"cell_type":"code","metadata":{"id":"nbcNRj7dnUqZ"},"source":["import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1twTUboknYum"},"source":["### 学習データの準備\n","####  ワインデータセットの読み込み\n","load_wineを用いてデータセットを読み込む\n","\n","データの中身を煩雑だが確認する\n","- 13個の特徴点リストを含むデータセットである\n","- feature_namesは、13個の特徴点の名称を含むデータセットである\n","- targetは、dataそれぞれがどのワイン種別に属するかを表している正解ラベルである\n","- target_namesはワインのラベルで、class_0、 class_1といった等級分けである\n"]},{"cell_type":"code","metadata":{"id":"wUzgLmP3ndyN"},"source":["wine = load_wine()\n","wine"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nizIfSwJovP1"},"source":["pandasのDataFrameを用いて、feature_namesをラベルとしてデータを表示する\n","- 中身をよりわかりやすく確認できる"]},{"cell_type":"code","metadata":{"id":"-qikjlMPnh31"},"source":["pd.DataFrame(wine.data, columns=wine.feature_names)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m8rNZSIXpFkn"},"source":["次に、2のラベルがついているデータを省いて、0と1だけにする\n","\n","今回利用するデータが、たまたまクラスが0, 1, 2の順に並んでいるため、下記のようにすれば十分であるが、これでは汎用性がない"]},{"cell_type":"code","metadata":{"id":"z0P-cgd3op4H"},"source":["wine_data2 = wine.data[0:130]\n","wine_target2 = wine.target[0:130]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xsL9FQctpwSY"},"source":["今後のことも考え、pandasのdropを用い、より汎用性の高い方法で分離する\n","\n","`pandas.DataFrame.drop(labels=None,axis=0,index=None,columns=None,level=None,inplace=False,errors=’raise’)`\n","\n","とする\n","\n","各パラメータは次の通り\n","\n","- labels: ラベル名またはラベル名のリスト\n","  -  (省略可能)初期値None  \n","  -  消去したい列データor行データのラベルを指定\n","- axis: 0または`index`,1または`columns`\n","  -  (省略可能)初期値0  \n","  -  行データ(0または`index`)を削除するか列データ(1または`columns`)を削除するかを指定\n","- index,columns: ラベル名またはラベル名のリスト\n","  - (省略可能)初期値None  \n","  - 省略したいラベル名を行データ(index)、列データ(columns)で個別に指定\n","- level: intもしくは階層名\n","  -  (省略可能)初期値None  \n","  -  ラベルとして利用する階層を指定\n","- inplace: bool値\n","  - (省略可能)初期値False  \n","  - Trueでvoidとなり元データに変更を反映\n","- errors:\t`ignore`または`raise`\n","  - (省略可能)初期値’raise’  \n","  - ‘ignore’でエラーを無視し処理続行\n","\n","教師データ、正解ラベルの両方からclass2のデータを消す必要があるため、一度データをマージして削除する\n","\n","ここでは、横方向に連結して、第0列が正解ラベル、第1列以降に特徴量データが並ぶようにデータセットを構成する\n","\n","- データの連結には`pd.concat([df1, df4], axis=1)`とする\n","  - 横方向に連結するため axis=1 を指定\n","  - このとき紐付けは 連結方向でないラベル=index について行われる\n","  - 連結方向のラベルにあたる columns はそのまま維持される\n","\n","pd.concat([wine.data,wine.target], axis=1)としたいところだが、型が合わないといわれるので変換する\n","- 暗黙でやれといいたい\n","- ラベルが勝手につくと厄介なので、ここでラベルを付けておく"]},{"cell_type":"code","metadata":{"id":"DzQ4_z4hq5UQ"},"source":["wine_cat = pd.concat([pd.DataFrame(wine.data, columns=wine.feature_names),\n","                      pd.DataFrame(wine.target, columns=['class'])], axis=1)\n","wine_cat.head(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JlqQD70ct1dB"},"source":["結合できたので、ここから、クラス2のデータを削除する\n","\n","消す前に、練習として、classが2であるデータだけ抽出するには、次のようにする"]},{"cell_type":"code","metadata":{"id":"W9VwuOG9-a16"},"source":["wine_cat[wine_cat['class'] == 2].head(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8xOXWpmfDigW"},"source":["あとで削除したときにどれだけ削除したかわかるように、データのサイズも確認しておく"]},{"cell_type":"code","metadata":{"id":"DcEwtA5W7gZ-"},"source":["wine_cat.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dCY3WFXjDHnW"},"source":["実際にデータを削除する\n","- これには、dropメソッドを使う\n","- 先に示したように、データはインデックスを使って選択が可能で、dropは該当したデータだけ削除する\n","- inplace=Trueとして、結果を易直接wine_catに代入する"]},{"cell_type":"code","metadata":{"id":"JldstDqYtiqm"},"source":["wine_cat[wine_cat['class']==2].index"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Cv0PZdQwWz5"},"source":["wine_cat.drop(wine_cat[wine_cat['class'] == 2].index, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WFkarKz8_0wj"},"source":["行を抽出するのではなく列を抽出するため、`[:,:13]`となる\n","- 13であるが、これは0から12を意味する\n","  - 0から13個の情報、未満と解釈してもよい"]},{"cell_type":"code","metadata":{"id":"CCWYPaTv1-MJ"},"source":["wine_data = wine_cat.values[:,:13]\n","wine_target = wine_cat.values[:,13]\n","print(wine_data, len(wine_data))\n","print(wine_target, len(wine_target))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BjWlI6mMAT9h"},"source":["### データセットの分割\n","\n","データセットから取り出した特徴量XおよびラベルYについて、さらに「トレーニング用」と「テスト用」のデータに分割する\n"]},{"cell_type":"code","metadata":{"id":"f05QFiMD_fwm"},"source":["Train_X, Test_X, Train_Y, Test_Y = train_test_split(wine_data, wine_target, test_size=0.25)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3jB6Q-_ZCZSk"},"source":["内容を確認する130$\\times$0.25$\\approx$33である。"]},{"cell_type":"code","metadata":{"id":"1NQSZHQR_hHY"},"source":["Train_X.shape, Test_X.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UQQgLeeyEBhQ"},"source":["### PyTorchテンソルへの変換と訓練データセット作成"]},{"cell_type":"markdown","metadata":{"id":"JjyTXBwaFeF_"},"source":["from_numpyを使って変換(こちらはよくありますが古い書き方です)"]},{"cell_type":"code","metadata":{"id":"SoAFDQL0CcK4"},"source":["train_X = torch.from_numpy(Train_X).float()\n","train_Y = torch.from_numpy(Train_Y).long()\n","test_X = torch.from_numpy(Test_X).float()\n","test_Y = torch.from_numpy(Test_Y).long()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wbe6eDmPzk3e"},"source":["torch.tensorを使って変換"]},{"cell_type":"code","metadata":{"id":"oDadQI8VzpG5"},"source":["train_X = torch.tensor(Train_X, dtype=torch.float)\n","train_Y = torch.tensor(Train_Y, dtype=torch.long)\n","test_X = torch.tensor(Test_X, dtype=torch.float)\n","test_Y = torch.tensor(Test_Y, dtype=torch.long)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nThnCFkB3n7T"},"source":["今風かつ推奨の書き方"]},{"cell_type":"code","metadata":{"id":"HjFhHpzw3mCv"},"source":["train_X = torch.FloatTensor(Train_X)\n","train_Y = torch.LongTensor(Train_Y)\n","test_X = torch.FloatTensor(Test_X)\n","test_Y = torch.LongTensor(Test_Y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VYvFqt_1FkDx"},"source":["中身を確認"]},{"cell_type":"code","metadata":{"id":"AFlCatPFFWV_"},"source":["train_X.shape, test_X.shape, train_Y.shape, test_Y.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S3vSt6u8GBkL"},"source":["学習には、TensorDatasetを用いて一つのテンソルの中に入れる必要がある"]},{"cell_type":"code","metadata":{"id":"QcmYSRwBFAOc"},"source":["train = TensorDataset(train_X, train_Y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DUl6Thc-GSFQ"},"source":["中身を確認する"]},{"cell_type":"code","metadata":{"id":"4FFqpDROFCpl"},"source":["train[0], len(train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O7LQcP5WlDP6"},"source":["ちなみに、`train.view()`は失敗する\n","- このtrainは、ミニバッチ専用の型を持ち、テンソル型ではない\n","  - 以前のバージョンでは見えた気がするので、今後見えるかもしれない"]},{"cell_type":"code","metadata":{"id":"AgFnMjXpo9Sv"},"source":["type(train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zZkHOQqyGc44"},"source":["訓練データセットからミニバッチで順にデータを学習する\n","- ミニバッチではデータをミニバッチサイズ分まとめて並列的に学習させるため、入力させるデータの次元が一つ増えてミニバッチサイズ分束ねたテンソルとなる\n","  - PyTorchでは、入力される方(モデル)も対応して束ねたテンソルを受け付けることができるようになる\n","  - 自動化されているのでサイズなどは気にせずともよいが、込み入ると気にしないといけない場合もある\n","- ここでは、バッチサイズを15とする\n","  - かなり小さく、モデルにもよるがGPUなら100を超えてもよい\n","- shuffle=Trueとしてでたらめに並び替える\n","  - shuffleは、取り出すたびにランダムに取り出すのではない\n","  - まずデータをシャッフルして、その後順に取り出す、全てが取り出し終わったらもう一度シャッフルしてデータを作り直す、という処理が行われる\n","  - 訓練データは普通Trueを指定"]},{"cell_type":"code","metadata":{"id":"esiR19NtGIqi"},"source":["train_loader = DataLoader(train, batch_size=15, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LDDWazI_HhCb"},"source":["### モデル定義\n","\n","一番最初に述べたネットワークモデルを構築する。既に存在するクラスを継承して用いると簡単であるため、PyTorchでは通常継承して利用\n","\n","- `__init__`は、インスタンス生成時に呼ばれるコンストラクタ(1)\n","- superは継承元の親クラスを意味し、その親クラスのコンストラクタを呼び出す(2)\n","- nn.Linierでは、全結合層を構成\n","  - 入力の数を13、中間層のノード数を128と指定する(3)\n","- さらに、128入力2出力の全結合層を構成(4)\n","- 次に、ネットワークにデータを通して出力値を求める関数を作成\n","  - 名前はforwardとする必要がある(5)\n","- fc1の活性化関数に例としてReLUを利用(6)\n","- その結果を次のfc2に投入(7)\n","- 値を返す(8)\n","  - 本来は最後にsoftmax関数を配置するが、次のノートブックで説明するようにPyTorchのCrossEntropyLoss処理はsoftmax処理も内包しているため、この場合softmax関数は不要\n","  - もしここでSoftmaxを指定する場合は、どの軸に対してsoftmaxを施すかを指定するためdim=0とする\n","    - 各自試すと良いが、誤ってsoftmaxを指定した場合でも、学習に失敗するといったことはないであろう\n","  - 結果を出力するため、戻り値をreturnする"]},{"cell_type":"code","metadata":{"id":"0a3LZ0FDHfHn"},"source":["class Net(nn.Module):\n","  def __init__(self): #(1)\n","    super(Net, self).__init__() #(2)\n","    self.fc1 = nn.Linear(13, 128) #(3)\n","    self.fc2 = nn.Linear(128, 2) #(4)\n","  def forward(self, x): #(5)\n","    x = F.relu(self.fc1(x)) #(6)\n","    x = self.fc2(x) #(7)\n","    return x #(8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BAG_7ZFsNLtP"},"source":["インスタンス化してmodelを作成する\n","- このmodelにデータを投入する"]},{"cell_type":"code","metadata":{"id":"M1xQHZ49NAxp"},"source":["model = Net()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JfrfcF__OuIK"},"source":["### 学習の実行\n","\n","損失をどのように定義するかを決定する\n","- 今回は交差エントロピーを利用する\n","- また、確率的勾配効果法SGDを利用して最適値へと漸近させる"]},{"cell_type":"code","metadata":{"id":"hvYFdlXHNEJ-"},"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sB2ybYRnP939"},"source":["今回は500回エポックを廻す\n","- 損失を格納する変数を0で初期化しておく(1)\n","- ミニバッチからデータを順次取り出して、train_xとtrain_yに格納する(2)  \n","- 初期勾配を0にする(3)\n","- 図の①に相当するforward計算を行う。これには、modelにtrain_xを渡せばよい(4)\n","- 図の②に相当する損失計算を行う。すでに宣言したcriterionに、計算結果と教師データを渡す(5)\n","- 図の③に相当する処理として、ロスを後方に伝搬させる(6)\n","- 図の④に相当する処理として、その結果を用いてパラメータを更新する(7)\n","- 毎回のlossの値を積算する(8)\n","- 10回に一度結果を表示して確認する。epochは0スタートであることに注意する(9)"]},{"cell_type":"code","metadata":{"id":"WUABHBETPIBN"},"source":["for epoch in range(500):\n","  total_loss = 0 #(1)\n","  for train_x, train_y in train_loader: #(2)\n","    optimizer.zero_grad() #(3)\n","    output = model(train_x) #(4)\n","    loss = criterion(output, train_y) #(5)\n","    loss.backward() #(6)\n","    optimizer.step() #(7)\n","    total_loss += loss.data #(8)\n","  if(epoch+1)%10 == 0: #(9)\n","    print(epoch+1, total_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CUaYxRohtIVr"},"source":["### 精度の計算\n","まずは値を取り出して、testに対する答えを出す\n","- `.detach`は、含まれている勾配を計算する演算情報を削除することができる\n"]},{"cell_type":"code","metadata":{"id":"3Wo7fR5g2-z0"},"source":["test_model = model(test_X).detach()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1yB8BH_tt04V"},"source":["その結果と1の大きい方を結果とする\n","- test_yのデータと比較して、等しければ1、違っていれば0とし、全部足し合わせる\n","- その値を、データの総数で割り、平均を求める"]},{"cell_type":"code","metadata":{"id":"cOcjFRr93TJP"},"source":["test_model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Of0TMBj9_kIm"},"source":["この行列は、それぞれのtestに対する評価値を保存している\n","- クラス0と考えられる値および、クラス1と考えられる値のセットである\n","\n","値の大きい方が「推定した答え」であるので、こちらを選別する\n","- このように行列で値の大きい方をまとめて選別する特別な関数 torch.maxが準備されている\n","\n","まずは、torch.maxについて簡単に説明する"]},{"cell_type":"markdown","metadata":{"id":"0g44aYvoFaw_"},"source":["#### torch.maxについて\n","\n","モデルの出力はデータ数×クラス数の行列である。正解データはデータ数次元のベクトルである。これを比較するのが、torch.maxである。データ数xクラス数の行列の結果を変換する。\n","\n","torch.maxは、行列（ベクトル）の最大値とそのインデックスを返す。以下の例で確認する。\n","\n","|  |  |  |\n","|--|--|--|\n","|0.2|\t1.4|\t1.7|\n","|0.3|\t1.5|\t0.7|\n","|1.1|\t1.3|\t0.4|\n","|0.9|\t2.3|\t0.5|\n","\n","このような例について、考えてみると、\n","\n","縦で見れば、02, 0.3, 1.1, 0.9で最もスコアが高いのは1.1である。したがって、第2行が選択される。同様に、2, 3, 0が選択されるとわかる。下記のセルで答えが一致することを確認する。また、戻り値は配列の配列であり、値で取得するのか、行もしくは列番号で取得するのかによって、取得する場所を変える。\n","\n","まずは、無理やり上記の行列を評価可能なテンソル型にする。この変換自体も、型がどのように扱われているかの理解を進めるであろう。\n","\n","その後、torch.maxで最大を抽出する\n","- 行・列どちらで判断するかを0か1で指定する\n","- 0を指定すると列、1を指定すると行でみて最大値を取り出す\n","\n","なお、類似の関数にtorch.argmaxも準備されている\n","- 違いはアルゴリズムで、argmaxの方が計算速度を速くすることができるが、同じデータが並んでいた場合、torch.maxは必ず最初の要素を返すのに対して、argmaxは最初の値を返す保証がなくなるという問題がある\n","- torch.argmaxは最大値を与えることはできず、最大であるindexのみ返す"]},{"cell_type":"code","metadata":{"id":"toeYxx92F96j"},"source":["import numpy as np\n","testX = np.array([[0.2, 1.4, 1.7],[0.3, 1.5, 0.7],[1.1, 1.3, 0.4],[0.9, 2.3, 0.5]])\n","x = torch.tensor(testX, requires_grad=False, dtype=torch.float) # 新しい書き方に修正しています\n","print(x)\n","value, index = torch.max(x.data, 0) \n","print(\"value0:\", value, \"index0:\", index)\n","value, index = torch.max(x.data, 1)\n","print(\"value1:\", value, \"index1:\", index)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ycVjnNetCIYG"},"source":["さて、本題にもどって最大を抽出する"]},{"cell_type":"code","metadata":{"id":"taBL9XbF7WkL"},"source":["torch.max(test_model, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3cZySbTtCPo0"},"source":["値が確認できたら、精度を求める"]},{"cell_type":"code","metadata":{"id":"ZIKECv-wt1j4"},"source":["result = torch.max(test_model, 1)[1]\n","accuracy = sum(test_Y.data.numpy() == result.numpy()) / len(test_Y.data.numpy())\n","accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j1Mq32JixHY_"},"source":["現在の設計ではこの程度\n","- あえてうまくいかないようにネットワークを小さくしている\n","- シンプルには学習パラメータを変更して精度を上げることができる"]},{"cell_type":"markdown","metadata":{"id":"_JVross6x-PA"},"source":["# 課題1(PyTorch NN改良)\n","\n","同様に次の点を改良してみよう\n","\n","**[改良1]** Learning Rate (lr)を$1 \\over 10$の値に変更すると、accuracyの値は向上するかどうか、実際に確認しなさい\n","- `optim.SGD(model.parameters(), lr=0.01)`のlrの値を指す\n","- あくまでも結果は今回の例に限定されることに注意する\n","  \n","**[改良2]** 次のネットワークの改善の方で同様にパラメータを調整し、90%超えを狙いなさい\n","\n","例えば、ネットワークを以下のように変更し、隠れ層をさらに深く、全結合層を5層に増やす\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/model2.png\" width=\"70%\">\n","\n","以下、モデルの再定義例を示す\n","- 各自でaccuracyの値を確認しなさい"]},{"cell_type":"code","metadata":{"id":"vpt7tQfg7Xuu"},"source":["class Net(nn.Module):\n","  def __init__(self):\n","    super(Net, self).__init__()\n","    self.fc1 = nn.Linear(13, 128)\n","    self.fc2 = nn.Linear(128, 128)\n","    self.fc3 = nn.Linear(128, 128)\n","    self.fc4 = nn.Linear(128, 128)\n","    self.fc5 = nn.Linear(128, 128)\n","    self.fc6 = nn.Linear(128, 2)\n","  def forward(self, x):\n","    x = F.relu(self.fc1(x))\n","    x = F.relu(self.fc2(x))\n","    x = F.relu(self.fc3(x))\n","    x = F.relu(self.fc4(x))\n","    x = F.relu(self.fc5(x))\n","    x = self.fc6(x)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Niaz8_W6OX34"},"source":["## PyTorchで手書き文字認識\n","\n","シンプルなディープラーニングの例を示す\n","\n","- 手書き文字認識といえば、MNISTが定番であるが、ここではさらに簡便なscikit-learnに付属する手書き文字の認識を行う\n","- 高々3層なのでディープといえるかどうかは微妙だが…"]},{"cell_type":"markdown","metadata":{"id":"x9Gzbn25XSlF"},"source":["scikit-learnから、手書き数字の画像データを読み込んで表示する"]},{"cell_type":"code","metadata":{"id":"81Il03RNAmbS"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import datasets\n","digits_data = datasets.load_digits()\n","n_img = 10  # 表示する画像の数\n","plt.figure(figsize=(10, 4))\n","for i in range(n_img):\n","  ax = plt.subplot(2, 5, i+1)\n","  plt.imshow(digits_data.data[i].reshape(8, 8), cmap=\"Greys_r\")\n","  ax.get_xaxis().set_visible(False)  # 軸を非表示に\n","  ax.get_yaxis().set_visible(False)\n","plt.show()\n","print(\"データの形状:\", digits_data.data.shape)\n","print(\"ラベル:\", digits_data.target[:n_img])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sFWHMOf2D4b1"},"source":["画像サイズが8×8と小さい、つまり解像度が低いため、つぶれた画像になっている\n","- より解像度の高い画像の認識については改めて学びます\n","  - scikit-learnが持っている画像データは利用しやすいように小さく構成されています\n","- 例として、0から9までの手書き数字の画像を表示したが、この程度の解像度しかない\n","\n","このような手書き数字の画像が、scikit-learnの手書き文字データセットには1797枚含まれている\n","- 各画像は正解となる描かれた数字を表すラベルとセットになっている\n"]},{"cell_type":"markdown","metadata":{"id":"w40h1X0FXmrh"},"source":["scikit-learnのtrain_test_splitを使って、データを訓練用とテストに分割する\n","- 本来はこの方法ではないが、最初なので無理やり使ってみる\n","- 真似しないように"]},{"cell_type":"code","metadata":{"id":"GLGwXks542pJ"},"source":["import torch\n","from sklearn.model_selection import train_test_split\n","digit_images = digits_data.data\n","labels = digits_data.target\n","x_train, x_test, t_train, t_test = train_test_split(digit_images, labels)  # 25%がテスト用\n","# PyTorchで扱うためTensorに変換\n","x_train = torch.tensor(x_train, dtype=torch.float32)\n","t_train = torch.tensor(t_train, dtype=torch.int64) \n","x_test = torch.tensor(x_test, dtype=torch.float32)\n","t_test = torch.tensor(t_test, dtype=torch.int64) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FalXNYaJPkoE"},"source":["`nn`モジュールの`Sequential`クラスによりモデルを構築する\n","- 今回は、あまり精度の良いモデルではないが、このサイズのデータセットであれば十分といえる\n","- 低解像度の画像は、ある意味畳み込み層やプーリング層を介した結果の画像ともいえなくもない"]},{"cell_type":"code","metadata":{"id":"SuqqZmsh_jNK"},"source":["from torch import nn\n","net = nn.Sequential(\n","  nn.Linear(64, 32),  # 全結合層\n","  nn.ReLU(),          # ReLU\n","  nn.Linear(32, 16),\n","  nn.ReLU(),\n","  nn.Linear(16, 10)   #0から9の10種の数字に対応\n",")\n","print(net)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qsW5zCKhQE9p"},"source":["学習として、モデルを訓練する\n","\n","- 損失関数に交差エントロピー誤差を用い、最適化アルゴリズムにSGD(確率的勾配降下法)を用いる\n","- 引数に構築したネットワーク(net)のパラメータを渡す\n","- 順伝播は訓練データ、テストデータ両者で行い誤差を計算する\n","- 訓練データについてのみ、逆伝播により$w$や$b$を`step`メソッドで更新する\n","- 今回はバッチ学習で全部のデータを入力して誤差を求める\n","\n","なお、`lossfn = nn.CrossEntropyLoss()`として\\\n","`loss_train = lossfn(y_train, t_train)`などとする例の方が一般的である\n"]},{"cell_type":"code","metadata":{"id":"u6zwN3nArbGC"},"source":["from torch import optim\n","# SGDを利用\n","optimizer = optim.SGD(net.parameters(), lr=0.01)  # 学習率は0.01\n","# 損失のログ\n","record_loss_train = []\n","record_loss_test = []\n","# 1000エポック学習\n","for i in range(1000):\n","  # 最初に勾配を初期化つまり0にする\n","  optimizer.zero_grad()   \n","  # 順伝播\n","  y_train = net(x_train)\n","  y_test = net(x_test)   \n","  # 交差エントロピー誤差で誤差を求める\n","  loss_train = nn.CrossEntropyLoss()(y_train, t_train)\n","  loss_test = nn.CrossEntropyLoss()(y_test, t_test)\n","  record_loss_train.append(loss_train.item()) # ゼロ次元テンソルからpythonの値を得るには.itemが便利\n","  record_loss_test.append(loss_test.item())\n","  # 逆伝播(勾配を求める)\n","  loss_train.backward()\n","  # パラメータの更新\n","  optimizer.step()\n","  if i%100 == 0:\n","    print(\"Epoch:\", i, \"Loss_Train:\", loss_train.item(), \"Loss_Test:\", loss_test.item())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rJwwrWTw43rx"},"source":["訓練データ、テストデータ両方の誤差の推移をグラフで表示する\n","どんどん誤差が低減しているのがわかるであろう\n","\n","なお、追加で実行するとさらに削減できるが、削減幅はあまり期待できない\n","\n","グラフを確認し、訓練データとテストデータの最終的な値がほぼ同じであることを確認する\n","- 異なる場合は過学習\n","- 同程度に誤差が大きい場合は学習不足"]},{"cell_type":"code","metadata":{"id":"OaJx4swE45XI"},"source":["plt.plot(range(len(record_loss_train)), record_loss_train, label=\"Train\")\n","plt.plot(range(len(record_loss_test)), record_loss_test, label=\"Test\")\n","plt.legend()\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Error\")\n","plt.show()  # ラベルがあるときは、きちんとplt.show()を呼び出すこと"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iMrpac0m4Nct"},"source":["正答率を求めて、モデルの性能を確認する\n","\n","ここでは、テストデータを用いる"]},{"cell_type":"code","metadata":{"id":"IRkGCYMM_N35"},"source":["y_test = net(x_test)\n","count = (y_test.argmax(1) == t_test).sum().item()\n","print(\"正解率:\", str(count/len(y_test)*100) + \"%\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LrRAJzwD4zpN"},"source":["訓練済みのモデルを使って、任意のデータを入力し、予測させる\n","\n","実際に、`img_id`を変更して任意の画像を入力し、モデルが機能していることを確かめよう\n","\n","- 間違いを探そうとすると確率5%程度なので、20回に1回しか成功しないであろう"]},{"cell_type":"code","metadata":{"id":"Pdy9nPckTDik"},"source":["img_id = 17   # ここを好きな数字IDに変更するとよい\n","x_pred = digit_images[img_id]\n","image = x_pred.reshape(8, 8)\n","plt.imshow(image, cmap=\"Greys_r\")\n","plt.show()\n","x_pred = torch.tensor(x_pred, dtype=torch.float32)\n","y_pred = net(x_pred)\n","print(\"正解:\", labels[img_id], \"予測結果:\", y_pred.argmax().item())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7wmirtm6nI9M"},"source":["y_pred"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"re4k5OS4ngdr"},"source":["# 課題2(PyTorch MNIST)\n","\n","scikit-learnの手書き文字認識データを使ったPyTorchによる認識について改良を施す\n","\n","レポート条件\n","- 上記の例の結果を添付する\n","- 改良し、上記よりも少ないエポック数で、上記よりも良い結果を得るモデルを作成して添付する\n","- 改良は、下記の点に限定すること\n","  - 指定箇所以外の変更で性能を向上するとチート行為とみなされる\n","- 自動採点を行うため#Qから始まる部分は残しておくこと\n","\n","改良点\n","- ネットワーク[改良点1]\n","  - 「ここから」「ここまで」の範囲を修正\n","  - 層の数や、層のノード数を変更するとよい\n","- 最適化アルゴリズム[改良点2]\n","  - 修正は1行とすること、複数行になってはいけない\n","- エポック数[改良点3]\n","  - 修正は数字だけにすること\n","\n","下記コード中に、改良点を施す場所を記載しており、また、上記のオリジナルコードも記載している\n","\n","[改良点1]、[改良点2]、[改良点3]の部分を変更して条件を満たすこと- 今回は、その他の部分の変更は認めない\n","\n","なお、運で上記の例の結果が悪く、運で改良した結果が良くなった場合も、一応解答として認めるが、追試で成功することを加点要素とする\n","\n","提出方法はこれまでと同様とする\n","\n","これまでの学習内容を踏まえれば、十分解答できるが、内容が多岐にわたるため、課題提出期間を拡大します\n","- 簡単にすぐにできる課題ですが、じっくりと取り組むこともできます\n","  - 既学習者は様々な工夫を試みてください\n","  - 初めて学ぶ人は、一つ一つ、概念的な意味をくみ取りながら進めてください\n","    - また、気になることが少しでもあれば、その周辺を調べてみてください"]},{"cell_type":"code","metadata":{"id":"MFgAOOZgngdt"},"source":["import torch\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","digits_data = datasets.load_digits()\n","digit_images = digits_data.data\n","labels = digits_data.target\n","x_train, x_test, t_train, t_test = train_test_split(digit_images, labels)\n","x_train = torch.tensor(x_train, dtype=torch.float32)\n","t_train = torch.tensor(t_train, dtype=torch.int64) \n","x_test = torch.tensor(x_test, dtype=torch.float32)\n","t_test = torch.tensor(t_test, dtype=torch.int64) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eEPjrTJzngd0"},"source":["## モデルの工夫[改良点1]\n","\n","最初の64と最後の10さえ守れば大丈夫(なはず)"]},{"cell_type":"code","metadata":{"id":"ns8ABV4Rngd1"},"source":["from torch import nn\n","\n","net = nn.Sequential(\n","  # ------- [改良点1] #Q1S --------\n","  # ここから「ここまで」のコードを変更すること\n","  nn.Linear(64, 32),  # 8x8の画像なので64からスタートするのは固定\n","  nn.ReLU(),\n","  nn.Linear(32, 16),\n","  nn.ReLU(),\n","  nn.Linear(16, 10)   #0から9の10種の数字に対応するので最後が10も固定\n","  # ここまで\n","  # ------- #Q1E -------\n",")\n","print(net)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CQFmYGijngd6"},"source":["## 学習における最適化アルゴリズムの工夫[改良点2]\n","\n","様々な最適化アルゴリズムが準備されている\n","- 学習した、しないに関わらず好きなモデルを使うと良い\n","- 次のwebページにマニュアルがあり各種最適化アルゴリズムが指定されている\n","  - https://pytorch.org/docs/stable/optim.html\n","  - 英語が嫌だとう人は、Googleの変訳はこちら\\\n","  https://translate.google.co.jp/translate?hl=ja&sl=en&tl=ja&u=https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Foptim.html\n","\n","何してよいかわからないという人は、ひとまず学んだadamを使ってみてはどうだろう\n","\n","- マニュアルにはadamだけでも様々存在するようだが、シンプルなadamが準備されている\n","  - もちろんハイパーパラメータもいろいろあるが、全部デフォルトでやってみるとよい\n","  - adamの威力を知れるであろう\n","  - ついでに、AMSGradの指定についても調べておくと良い\\\n","  現時点で一般的に高性能とされるのが、AMSGradであろう\n","  \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FfOVuC0Z0MTH"},"source":["## エポック数の削減[改良点3]\n","\n","[改良点1][改良点2]によりエポック数を削減しても、良い結果を得ることができるであろう\n","\n","- 特に、[改良点2]の工夫はエポック数削減に大きく寄与し、現状のままでは過学習気味になるであろう\n","\n","そこで、エポック数を削減し調整すること"]},{"cell_type":"code","metadata":{"id":"8gywkbOengd7"},"source":["from torch import optim\n","# 次のなんだか追加されている部分は、この部分のセルを何度実行しても\n","#「追加で学習」ではなく「最初から学習しなおし」になるように\n","# パラメータをXavierで初期化している\n","#   その他、Heの初期化などがあり、それぞれ定義されている\n","#   詳細は省略するが収束しやすいように工夫して初期化しておく、ということである\n","#   一般に、シグモイド関数であればXavierの初期化がよいとされ、\n","#   さらにReLUも考慮したのが、Heの初期化である\n","#   nn.init.kaiming_uniform_(m.weight)とする\n","#   いずれもuniformは、normalの分布形状を指定できる\n","def init_weights(m):\n","  if type(m) == nn.Linear:\n","    torch.nn.init.xavier_uniform(m.weight)\n","    m.bias.data.fill_(0.01)\n","net.apply(init_weights)\n","# ここまでが初期化のためのコードで無視してよい\n","# 皆さんはこのセルだけ実行すればよい\n","# つまり、先ほどのコードは初期化されないため繰り返し押すと学習が追加で進むことになる\n","lossfn = nn.CrossEntropyLoss()\n","optimizer =  optim.SGD(net.parameters(), lr=0.01) #[変更点2] #Q2\n","record_loss_train = []\n","record_loss_test = []\n","for i in range(1000): #[変更点3] #Q3\n","  optimizer.zero_grad()\n","  y_train = net(x_train)\n","  y_test = net(x_test)\n","  loss_train = lossfn(y_train, t_train)\n","  loss_test = lossfn(y_test, t_test)\n","  record_loss_train.append(loss_train.item())\n","  record_loss_test.append(loss_test.item())\n","  loss_train.backward()\n","  optimizer.step()\n","  if i%100 == 0:\n","    print(\"Epoch:\", i, \"Loss_Train:\", loss_train.item(), \"Loss_Test:\", loss_test.item())\n","import matplotlib.pyplot as plt\n","plt.plot(range(len(record_loss_train)), record_loss_train, label=\"Train\")\n","plt.plot(range(len(record_loss_test)), record_loss_test, label=\"Test\")\n","plt.legend()\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Error\")\n","plt.show()\n","y_test = net(x_test)\n","count = (y_test.argmax(1) == t_test).sum().item()\n","print(\"正解率:\", str(count/len(y_test)*100) + \"%\")"],"execution_count":null,"outputs":[]}]}