{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dataai-text-8-PyTorch-Basics.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"4NSy6Xq5Zs_I"},"source":["#@title Data-AI（必ず自分の名前・学籍番号を入力すること） { run: \"auto\", display-mode: \"form\" }\n","\n","import urllib.request as ur\n","import urllib.parse as up\n","Name = '\\u6C5F\\u6D32\\u51FA\\u4E95 \\u5929\\u9B42' #@param {type:\"string\"}\n","EName = 'Esudei Tensouru' #@param {type:\"string\"}\n","StudentID = '87654321' #@param {type:\"string\"}\n","Addrp = !cat /sys/class/net/eth0/address\n","Addr = Addrp[0]\n","url = 'https://class.west.sd.keio.ac.jp/classroll.php'\n","params = {'class':'dataai','name':Name,'ename':EName,'id':StudentID,'addr':Addr,\n","           'page':'dataai-text-8','token':'98555190'}\n","data = up.urlencode(params).encode('utf-8')\n","#headers = {'itmes','application/x-www-form-urlencoded'}\n","req = ur.Request(url, data=data)\n","res = ur.urlopen(req)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ilbOCfxXhbF8"},"source":["---\n",">テンソル(tensor)はテンション(張力:tension)に由来する\\\n",">密度が一様の弾性球体に一方向から張力を作用させたとき体積不変のまま楕円球体へと変形するが、その時の変形との関係を表すために導入されたのがテンソルであった\\\n",">その意味が拡大され、今のテンソルとなった\\\n",">読みはテンソル派とテンサー派があるが、日本語なら「テンソル」が大勢で、英語なら「テンソー」が近いのではないか\n","---"]},{"cell_type":"markdown","metadata":{"id":"bjyKrhvx9wDy"},"source":["# Tensorの扱い\n","\n","PyTorchのTutorialsに従う\n","\n","PyTorchについてわからないことがあれば、はやり本家の解説をまず参照するとよい\n","\n","PyTorchホームページ(https://pytorch.org/)のメニューにあるDocsをクリックすると公式のドキュメントがある\n","- 用語や関数名など関連する内容を調べることができる\n","- 様々な記述例も存在するため参考にするとよい"]},{"cell_type":"markdown","metadata":{"id":"r3ubhrlr8Y2w"},"source":["## 基本演算\n","\n","PyTorchではTensor型を用いて行列を表現する\n","- TensorはNumpyのndarrayと類似しているが、GPU利用による高速化が可能であるのと、後で述べるが自動微分用のパラメタが追加されている\n","- Tensorを作成するには、次のようにするが中身は初期化されていないことに注意する"]},{"cell_type":"code","metadata":{"id":"SunhcA1qnGnL"},"source":["import torch\n","x = torch.empty(5, 3)\n","x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yGBlIxw98v3H"},"source":["乱数で初期化するには次のようにする"]},{"cell_type":"code","metadata":{"id":"maivEJvftvrf"},"source":["x = torch.rand(5, 3)\n","x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fuPF2iQq89tY"},"source":["ゼロで初期化するには次のようにする\n","- torch.longは64bitの整数で与えることを意味している。"]},{"cell_type":"code","metadata":{"id":"1JP14iBe8xkl"},"source":["x = torch.zeros(5, 3, dtype=torch.long)\n","x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JU25waDF9Cef"},"source":["値で初期化するには次のようにする"]},{"cell_type":"code","metadata":{"id":"cyJXLQjA85eV"},"source":["x = torch.tensor([[0.2417, 0.2270, 0.5315],\n","        [0.4860, 0.9984, 0.3871],\n","        [0.4769, 0.8593, 0.2609],\n","        [0.0173, 0.1879, 0.9608],\n","        [0.1309, 0.9766, 0.6346]])\n","x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_6wQ3Sob9MBW"},"source":["特に1で埋めるメソッドがあり、次のようにする"]},{"cell_type":"code","metadata":{"id":"2Wm8GmmG9GKJ"},"source":["x = x.new_ones(5, 3, dtype=torch.double)\n","x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ryjIAdjj9UfH"},"source":["xと同じサイズで異なる値を持つテンソルを生成する\n","- _likeがつく関数はすでに定義したxと同じサイズでテンソルを作成する\n","- randは平均0、分散1の正規分布で埋める"]},{"cell_type":"code","metadata":{"id":"lUtEAWWzB3Fj"},"source":["x = torch.randn_like(x, dtype=torch.float)\n","x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p6VcSY8s9cp3"},"source":["大きさを知るには、sizeメソッドを呼び出す\n","\n","`type(x)`ではテンソル型であることがわかるだけである"]},{"cell_type":"markdown","metadata":{"id":"3honXkZlp0VG"},"source":["整数を要素にもつTensorと浮動小数点を要素にもつTensorをそれぞれ作成して違いを確認する\n","- 小文字tensorであることに注意する。"]},{"cell_type":"code","metadata":{"id":"1THD1CWwp7Gn"},"source":["x = torch.tensor([1,2,3,4,5,6])\n","x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p3c1bJcWrAYm"},"source":["x.type()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rBJgJ9KUq7ss"},"source":["x = torch.FloatTensor([1,2,3,4,5,6])\n","x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YXLNf77crDA4"},"source":["x.type()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C7dQIQLiri0_"},"source":["次の大文字Tensorは注意が必要であろう"]},{"cell_type":"code","metadata":{"id":"8V9NmmY7rikP"},"source":["x = torch.Tensor([1,2,3,4,5,6])\n","x.type()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vo0Da_CA9Pti"},"source":["x = torch.randn_like(x, dtype=torch.float)\n","x   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jIJOK6_39Z29"},"source":["x.size()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r9vOEbLh9oUG"},"source":["足し算を行う"]},{"cell_type":"code","metadata":{"id":"gSqUqgif9ikI"},"source":["x = torch.rand(5, 3)\n","y = torch.rand(5, 3)\n","x+y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VtvRTCK29ua-"},"source":["torchが準備するaddメソッドを用いることもできるが、やっていることは同じであるため、演算子オーバーロードされている方がわかりやすいであろう"]},{"cell_type":"code","metadata":{"id":"_RBcV3ng9pk1"},"source":["torch.add(x,y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1ADMUxMM97d3"},"source":["結果を格納するTensorを準備して、出力先を指定することができるが、直観的ではない"]},{"cell_type":"code","metadata":{"id":"WeDlZRiU92yR"},"source":["result = torch.empty(5, 3)\n","torch.add(x, y, out=result)\n","result"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EHxCvpzG-MTw"},"source":["次のように演算子オーバーロードを利用するのがよい\n","- 今何の型を使って演算しているのかがわかりにくいという意見は平成の人だから無視してよい\n","  - 昭和の人は型に縛られる言語を使っていない\n","  - 令和世代は柔軟に対応しよう\n","- この辺りはTensorFlowを使っている人からすると夢のような記述スタイルであろう"]},{"cell_type":"code","metadata":{"id":"PG0M2Nxl9_Ef"},"source":["y1 = y\n","y2 = y\n","y1.add_(x)\n","y2 += x\n","print(y1)\n","print(y2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J3We2QrH72pJ"},"source":["行の入手はy1[1]とすればよい"]},{"cell_type":"code","metadata":{"id":"HmphhQZA76oR"},"source":["y1[1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mO_RbJOT7s1R"},"source":["NumPyと同様の方法で列を入手することもできる\n","\n","- :でテンソル全体を指定し、その1番目の列を指定することで、１番目の列が入手できる(0オリジン)"]},{"cell_type":"code","metadata":{"id":"HWh_noMM6_aG"},"source":["y1[:,1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W-a2BMQ7qkTf"},"source":["部分要素を取り出す"]},{"cell_type":"code","metadata":{"id":"QgRF5T1mqmav"},"source":["y1[2:4,1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l2yK9tgA-ZA-"},"source":["view()メソッドを使って、Tensorをリサイズできる\n","\n","PyTorchでテンソルを操作する関数としてtranspose, view, reshapeがある\n","\n","- 基本はtransposeでTensorを転置するだけ\n","- view(y, x)で、y行x列のデータに変換\n","  - -1をどちらかに指定すると自動的に大きさから計算してくれるが、約数でないと変換できずエラーになる\n","  - viewはメモリ操作、つまりデータの入れ替えは行わず見え方のみ操作する関数であるため、メモリ上の並びを変えるような変換はできない\n","  - 例えば、transferつまり転置したデータをviewで並び替えるとメモリ上での位置の変更が伴うためエラーになる\n","  - これを防ぐためにviewの前にcontiguous関数を呼び出してメモリ上で要素順に並び替えてからviewすると回避できる\n","- reshapeは、viewと同様であるが、contiguous関数を必要としない\n","  - それは、データのコピーを生成するから\n","  - メモリは食べるけどエラーにはならない\n","\n","結局自分で適切な手法を選べということ\n","\n","得たい結果とその結果を得るために必要なリソースの観点があるため、様々なニーズが存在する\n","\n","- 命令セットが複雑になるのは、ニーズがあるためである\n","\n","- 何でも扱えるように簡単にするか、細分化して計算速度やリソースの使い方まで言及するかは言語設計の基本に影響を与えるため、言語多様性における必要悪ともいえるであろう"]},{"cell_type":"code","metadata":{"id":"prmXqtTf-LTB"},"source":["x = torch.randn(4, 4) # 4x4の乱数行列を作成\n","y = x.view(16) # 16次元の行列を準備\n","z = x.view(-1, 8)  # -1を使うと，サイズを自動調整\n","print(x.size(), y.size(), z.size())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gHOL3w0G-4nm"},"source":["中身を確認する"]},{"cell_type":"code","metadata":{"id":"u0GgfVN3-jjF"},"source":["print(x, \"\\n\", y, \"\\n\", z)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ln-gMrbx-_yu"},"source":["要素数1のTensorにitem()メソッドを使うことで、計算履歴を含まないpythonの値として取得できる\n","\n","ここで、detachとitemメソッドの違いについて説明しておく\n","- detachはautogradに必要な計算履歴管理を外す\n","  - 対象となるtorch.tensorがどのような型や要素数であっても問題ない\n","  - 型や総素数はなんでもよく、変化しない\n","- itemは要素数が1個のtorch.Tensorに限定されそれ以外はエラーとなり、スカラーとして取り出すことができる\n","  - 変換できるかどうかは次元数ではなく要素数で決まるため、多次元テンソルでも要素数が1個であれば変換可能\n","  - torch.tensorではなくなるため、結果的にdetachされている\n","\n","なお、GPU変数をnumpyに取り出す際には、`.cpu().detach().numpy()`とする(後述)"]},{"cell_type":"code","metadata":{"id":"bA_RIHJ2_Jub"},"source":["for i in range(3):\n","  print(y[[i]].item())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F323wSDNCNgG"},"source":["普通に表示するとtensor型になる\n","\n","なお、```y[[i for i in range(3)]].item()```とすると、要素が1つではないためエラーとなる"]},{"cell_type":"code","metadata":{"id":"V-KSV4ix_Oi2"},"source":["print(y[[i for i in range(3)]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ona9kCHjCcvM"},"source":["これは不便だと思うかもしれないが、基本的にはNumpy Arrayに変更できるため困らないはずである\n","\n","なお、100種類以上のテンソル操作関数が準備されている\n","\n","詳細は、\n","http://pytorch.org/docs/stable/torch.html\n","を参照のこと\n","\n","- フーリエ変換、逆行列、何でもあり\n","- 意味を知っていて使えればよく、実装原理はここではどうでもよい"]},{"cell_type":"markdown","metadata":{"id":"IbOrbPb8e3DW"},"source":["PyTorchテンソルの要素の型\n","\n","テンソルの中に含める数値には、PyTorch独自のデータ型（`torch.dtypes`）が与えられているが、あるテンソルに含まれる全要素は全て同じデータ型である必要がある\n","\n","以下の型が準備されているが、基本的に`torch.float`か`torch.int`しか使わないので、忘れてよい\n","\n","データ型 | dtype属性への記述 | 対応するPython／NumPy（np）のデータ型\n","---------|-----------------|--------------------------------\n","Boolean（真偽値）|torch.bool|bool／np.bool\n","8-bitの符号なし整数|torch.uint8|int／np.uint8\n","8-bitの符号付き整数|torch.int8|int／np.int8\n","16-bitの符号付き整数|torch.int16 ／ torch.short|int／np.uint16\n","32-bitの符号付き整数|torch.int32 ／ torch.int|int／np.uint32\n","64-bitの符号付き整数|torch.int64 ／ torch.long|int／np.uint64\n","16-bitの浮動小数点|torch.float16 ／ torch.half|float／np.float16\n","32-bitの浮動小数点|torch.float32 ／ torch.float|float／np.float32\n","64-bitの浮動小数点|torch.float64 ／ torch.double|float／np.float64\n"]},{"cell_type":"markdown","metadata":{"id":"MYuya4Oj9syb"},"source":["## NumPyとの連携"]},{"cell_type":"markdown","metadata":{"id":"edT_7XQ6-ISS"},"source":["### Torch TensorからNumPyへの変換"]},{"cell_type":"code","metadata":{"id":"jHiFvIyLB9Vg"},"source":["a = torch.ones(5)\n","b = a.numpy()\n","b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q_BiHf6gCpjN"},"source":["ここから、pythonらしいマジックが入るが(これがpython嫌いを生む要素でもあるが)、numpy()はメモリを共有するため、aを変更すると、bも同時に変更される\n","\n","ただし、bを変更すると、**bを変更した瞬間にこの関係は崩れ**、aを変更してもbは変更されなくなる\n","\n","- この現象は、C++などにおけるCall by referenceとCall by valueの違いと同じ原理で発生する\n","\n","- b = a.numpy()は、aのnumpy表示をbとする、ということで、bに実体はなく、bを呼び出すといつもa.numpy()が呼び出される\n","\n"," - つまり、要するにエイリアスの関係にあり、毎回変換処理関数が呼び出される\n","\n","  - そのあとb=「～」とbを変更すると、bはa.numpy()のある場所を指すのをやめて、新しく「～」というメモリ上の値を指すようになる\n","\n","  - その後bを変更してもオリジナルが変更されるわけではない\n","\n","python嫌いの気持ちがわかるであろう\\\n","昨日できたことが今日できないのだから\n","\n"," - 実際には=のオーバーロードである程度対応することも可能であるといえるが、言語仕様としては=しか代入がないのが問題ともいえる"]},{"cell_type":"markdown","metadata":{"id":"gcbquML1Uo6v"},"source":["なお、より実践的には次のようにする\n","- `to('cpu')`で、GPUではなくCPUにデータを保持する\n","- `detach()`で勾配計算のための付属情報を削除する\n","- `copy()`で不用意な書換を避ける\n","  - torch tensor と numpy ndarray はメモリを共有しており、どちらかを変更すると相手もそれにつられて変化する\n","  - これを避ける場合はtorch.clone()やnumpy.copy()もしくはdeepcopy()とする\n","  - 例でa.copy()はエラーとなる \\\n","  a.to('cpu').detach().clone().numpy().copy()も無意味\n"]},{"cell_type":"code","metadata":{"id":"tS02GqkcUarx"},"source":["b = a.to('cpu').detach().numpy().copy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OK_sx_nu9H7H"},"source":["b=a.to('cpu').detach().clone().numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NdUO4g4SCjJc"},"source":["a.add_(1)\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OruKEkp0wiCl"},"source":["b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ldZsLbamEdWz"},"source":["ここでbを変更すると、aとbの一致性が崩れる"]},{"cell_type":"code","metadata":{"id":"c24wzyuJC1CT"},"source":["b = [b[i]*2 for i in range(5)]\n","print(a,b)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dy1FgxB_Eh96"},"source":["一度崩れるともう戻らない"]},{"cell_type":"code","metadata":{"id":"_Wbu5quxD89k"},"source":["a.add_(1)\n","print(a,b)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tpk5uByP-M_w"},"source":["### NumPyからTorch Tensorへの変換"]},{"cell_type":"markdown","metadata":{"id":"eVbEHuDQ-os3"},"source":["aに要素がすべて1で5個有するArrayとする\n","\n","torch Tensorに変換するには、from_numpy()メソッドを利用する"]},{"cell_type":"code","metadata":{"id":"6x8YLUUM-c5S"},"source":["import numpy as np\n","a = np.ones(5)\n","b = torch.from_numpy(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KuxfROvjw5Q6"},"source":["a, b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6cN3OQuh_Aro"},"source":["numpyで全要素に1を加えるには、ちょっと面倒な書き方になる"]},{"cell_type":"code","metadata":{"id":"7ZprnJQZ__eE"},"source":["print(a)\n","np.add(a, 1, out=a)\n","print(a)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4FWHIYcRAEpL"},"source":["bを表示すると、bも変化していることがわかる"]},{"cell_type":"code","metadata":{"id":"DlXcU4wMAK4W"},"source":["b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2oi67EmM1f0q"},"source":["次のように記述すればよいではないか？という意見もあるであろう"]},{"cell_type":"code","metadata":{"id":"dLnfQ7tD1Cc-"},"source":["a += 1\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yucPZAft1m4_"},"source":["では、bをみてみよう\n","\n","同様にbも更新されていることがわかる"]},{"cell_type":"code","metadata":{"id":"kRhipsx21oTw"},"source":["b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D5AFLc6p2ipi"},"source":["さらに、次はどうだろうか"]},{"cell_type":"code","metadata":{"id":"WrmRnHQD2kDa"},"source":["a = a + 1\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IFevTICR2mGx"},"source":["bを見てみよう\n","\n","今度は更新されない"]},{"cell_type":"code","metadata":{"id":"N8YIcYYE2tYS"},"source":["b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S-htXjTR2AZw"},"source":["=はメモリのコピーを生成するが、+=はコピーを生成しない、こんなややこしいことになっているが、奇天烈な日本語よりはいい\n","\n","aとbのリンクが切れてしまったので時を戻そう"]},{"cell_type":"code","metadata":{"id":"sF2rtCp86c25"},"source":["b = torch.from_numpy(a)\n","b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cmzlrb5nAV0X"},"source":["時は戻らなかったが、悪くないだろう\n","\n","次に、逆にbを変更する、もう答えはわかるであろう"]},{"cell_type":"code","metadata":{"id":"4s-wSPPh_KyS"},"source":["b+=5\n","b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jKOegHQH_clG"},"source":["次にaを表示する。"]},{"cell_type":"code","metadata":{"id":"7MCSurgIBKfW"},"source":["a"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V-Fktg7ABYXD"},"source":["きちんとaも変化している\n","\n","では、代入するとどうであろうか？"]},{"cell_type":"code","metadata":{"id":"RkcI9rNcBexZ"},"source":["a = np.zeros(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HsjXqbdBBo-T"},"source":["print(a)\n","print(b)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JRhczbljBvOB"},"source":["ということで、今度はaを変化させても、bは変化しないが、理由はこちらも同様である\n","\n","つまり、メソッドを使ううちはリンクは切れないが、違うインスタンスに付け替えられるとリンクが切れて、新しい値へのリンクになる"]},{"cell_type":"markdown","metadata":{"id":"ilc1d_e92Fcz"},"source":["さて、TensorとNymPy相互変換について、from_numpy()を紹介したが、これとは別にcopy()が存在する\n","\n","- 生成されるデータは変わらないが、from_numpy()がメモリ共有するのに対して、copy()はメモリコピーを行う\n","\n","- 同様に、にnumpy()によりtensorからnumpyに変換する場合もclone()が用意されている\n","\n","- なお、PyTorchは一般にtorch.floatを利用するので、numpyでfloat32にキャストしてから利用することになる\n","\n","いろいろと面倒であろうから、実践的な変換事例を示す\n","- ついでなので、`%%timeit`による処理速度計測の例も一緒に示すので結果の確認と処理遅延計測を行ってみよう\n","- なお、普通にfrom_numpyとして通常はなんら問題はない"]},{"cell_type":"markdown","metadata":{"id":"5Q_bactBW5Q4"},"source":["numpyからtensorへ"]},{"cell_type":"code","metadata":{"id":"rCG4FAi2XLXC"},"source":["%%timeit\n","b = torch.from_numpy(a.astype(np.float32))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FLyfOMbqU2A4"},"source":["%%timeit\n","b = torch.from_numpy(a.astype(np.float32)).clone()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rHR74RS-W7hT"},"source":["tensorからnumpyへ\n","\n","GPUメモリで操作する場合はgpuとするが、numpyはcpuでしか操作できない\n","- cpuにもってきて操作した場合でさらにgpuで引き継いで計算する場合は忘れずにb.to('gpu')などとしてgpuに移動させること"]},{"cell_type":"code","metadata":{"id":"X8rJ3ThsXNhg"},"source":["%%timeit\n","c = b.to('cpu').detach().numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r_rqbeERVE1L"},"source":["%%timeit\n","c = b.to('cpu').detach().numpy().copy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bO1Jw3537xqJ"},"source":["ついでに、pythonのネガティブキャンペーンを続けよう\n","\n","次は、`a = np.array([0,1,2])`と同じである"]},{"cell_type":"code","metadata":{"id":"f2Dy9dcS8B-2"},"source":["a = np.arange(3)\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Y-tXxCh8n37"},"source":["これを転置する"]},{"cell_type":"code","metadata":{"id":"5KxfO8V38pVG"},"source":["a.T"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dsvvxIXb8rxl"},"source":["え？転置できない？？？\n","\n","その通り、これではできない\\\n","次のようにする"]},{"cell_type":"code","metadata":{"id":"ogBmsfQ8800j"},"source":["a = np.array([[0, 1, 2]])\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c1_-5_eI89rO"},"source":["転置する"]},{"cell_type":"code","metadata":{"id":"w3PCAX1C8_O5"},"source":["a.T"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WIHr4Eac8uKk"},"source":["なぜ2重配列なのだ？と思うかもしれないが、そういう仕様である\n","- [0, 1, 2]はメモリに順番に0, 1, 2と記載されていることを意味するので、転置されてもメモリに格納されている順序や格納形態は変わらない\n","- [0], [1], [2]も同様ではないか、と思うかもしれないが、これは違う\n","  - [0]は配列であり、メモリ上は、array([0]), array([1]), array([2])とarrayであるという識別子も記載されており、占有メモリサイズは[0, 1, 2]よりも大きい、つまり、より表現力が拡大している\n","\n","- array([0, 1, 2])とは、メモリ上の表現自体が変化しており、転置に成功する"]},{"cell_type":"markdown","metadata":{"id":"RE_jsRkaJDz1"},"source":["もうひとつ、`TypeError: '～' object is not callable`というエラーに遭遇したら要注意\n","\n","これは、`～`というpythonのコマンドと同じ名前の変数を作ってしまった場合など発生し、初心者は結構これでハマることが多い\n","\n","- 周辺の記述を間違えると、`～`という記述がコードに出てきてそれが変数と解釈される結果、このエラーが発生する\n","\n","対処法は、例えば`%whos`コマンドで、使用している変数一覧をみて、問題となるpythonコマンドが変数として宣言されていないかを確認する\n","\n","- 宣言されていれば、`del ～`として削除すればよい\n","\n","- ぶっちゃけ、正しいコードである自信があるなら、仮想マシンをリフレッシュしてもよい\n","\n","  - メニューのランタイムから、ランタイムを出荷状態にリセットを選択すればよい"]},{"cell_type":"markdown","metadata":{"id":"Xpum-rW-iKQX"},"source":["## GPUとCPUの使い分け\n","\n","まず、GPUが搭載されているか、どのGPUが使われているかを確認する"]},{"cell_type":"code","metadata":{"id":"owWJob6cgzYV"},"source":["if torch.cuda.is_available():\n","  print(f'GPUデバイス数： {torch.cuda.device_count()}')\n","  print(f'現在のGPUデバイス番号： {torch.cuda.current_device()}')\n","  print(f'1番目のGPUデバイス名： {torch.cuda.get_device_name(0)}')\n","else:\n","  print('GPUは使えません')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gyLmWe4wiRL0"},"source":["現在利用しているCPUのデバイスを取得する"]},{"cell_type":"code","metadata":{"id":"njBe4kMYhKJm"},"source":["device = torch.device(\"cuda\")          # デフォルトのCUDAデバイスオブジェクトを取得\n","device0 = torch.device(\"cuda:0\")       # 1番目（※0スタート）のCUDAデバイスを取得\n","print(device)\n","print(device0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4yHmbXQWieYM"},"source":["テンソル計算にGPUを使う場合について以下の通りにする\n","\n","- まずテンソルをGPUで生成するときはdeviceで指定する\n","- テンソルを移動するときはtoを使う\n","- 別の書き方として、\n","  - GPUで演算するには、演算前にcudaメソッドを使う\n","  - CPUで演算するには、演算前にcpuメソッドを使う\n","\n","もちろんであるが、直接cpuやcudaをしてしてもよいし、省略してもよい"]},{"cell_type":"code","metadata":{"id":"_F4vQEmcie-k"},"source":["g = torch.ones(2, 3, device=device)\n","g = x.to(device)\n","g = x.cuda(device)\n","f = x.cpu()\n","g = x.to(\"cuda\")\n","f = x.to(\"cpu\")\n","g = x.cuda() # GPUがセットされていて、かつGPUが獲得できていなければエラーになる"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_PFNqPcUiHfG"},"source":["なお、テンソルではなく、学習したモデルのcpuとcudaの移動は次のようにする\n","- model.cuda()\n","  - モデルの全パラメーターとバッファーをCUDAに移行\n","- model.cpu()\n","  - モデルの全パラメーターとバッファーをCPUに移行"]},{"cell_type":"markdown","metadata":{"id":"YJ9rNcmBDIMG"},"source":["# 自動微分について\n","\n","自動微分(Autograd)とは\n","- Tensorの各要素による微分を自動で行う機能\n","- 演算内容と計算グラフを保持し順伝播の経路を遡って勾配を計算できる\n","\n","PyTorchのテンソルとAutogradにより、逆伝播の演算を自動化できる\n","\n","### grad（勾配）の扱い\n","\n","- Pytorch内では、torch.Tensorクラスでテンソル（多次元行列）を扱う\n","- .requires_gradメンバ変数で勾配をトラックするかを選択できる\n","  - Trueにすると重くなる\n","  - .backward()で勾配（微分値）を取得する\n","  - また、得られた勾配から誤差逆伝搬法で重みを更新する\n","- .bradメンバ変数に勾配データを保持する\n","- .detach()メンバ関数でトラック対象から外すことができる\n","- .requires_grad_ メンバ関数でフラグを反転させることができる。\n"," \n","各変数は、.grad_fnというプロパティ（属性）を保有し、これが実際に勾配を計算するメソッドである"]},{"cell_type":"code","metadata":{"id":"WMxylzBqC8-8"},"source":["import numpy as np\n","import torch\n","import torch.nn as nn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EGHP9TAe1XEO"},"source":["## 実際の操作\n","\n","テンソルを作成する\n","\n","requires_grad=Falseとすると微分の対象にならず勾配としてNoneを返す\n","\n","Trueとして宣言する"]},{"cell_type":"code","metadata":{"id":"zw3eBowD1poT"},"source":["x = torch.ones(2, 2, requires_grad=True)\n","x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i4Mo2noq19ML"},"source":["y = x+2を計算し表示させる"]},{"cell_type":"code","metadata":{"id":"Zt34k_-g2DrU"},"source":["y = x+2\n","y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xdd3m1QN2G4E"},"source":[".grad_fnを確認する\n","\n","AddBackwordと表示されるように、足し算であることを記録している"]},{"cell_type":"code","metadata":{"id":"zjWWk1Lv2JT9"},"source":["y.grad_fn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q4zP9GY82QRc"},"source":["関数へのポインタが示されているので実態はわからないが、足し算であることはわかる\n","\n","$Y\\cdot Y \\cdot 3$を計算し表示する"]},{"cell_type":"code","metadata":{"id":"dx0xVNDN2mPb"},"source":["z=y*y*3\n","z"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Y7KOfch3D8j"},"source":["zの平均を計算する"]},{"cell_type":"code","metadata":{"id":"egtLYP5T2rh7"},"source":["out = z.mean()\n","out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Knn_p3z3JRq"},"source":["平均値である27が得られる\n","\n","このZつまり平均を求める計算も勾配計算でき、MeanBackwordとして示されている"]},{"cell_type":"markdown","metadata":{"id":"W3wAmgrW3mXD"},"source":["### autograd属性の反転\n","\n","2x2の平均が0である乱数Tensorを作成する。"]},{"cell_type":"code","metadata":{"id":"KmDRPILT387R"},"source":["a = torch.randn(2,2)\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bg5aJbHN4ItT"},"source":["requires_gradを確認する\n","\n","randnはデフォルトがFalseであるため、明示的にTrueとする"]},{"cell_type":"code","metadata":{"id":"nc-I7dun4MHh"},"source":["a.requires_grad"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s08MGRgW0eHC"},"source":["a.requires_grad_(True)\n","a.requires_grad"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IyjwDApE1j27"},"source":["$a\\cdot a$を求め、その要素の総和を計算する。"]},{"cell_type":"code","metadata":{"id":"Za6FbP9g12cL"},"source":["b=(a*a).sum()\n","b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DfAVuEBL2yUQ"},"source":["### backward()を用いて微分値を求める\n","\n","まず、ここまでを振り返ると、out という値を得るために、\n","- 要素がすべて1の2x2行列$\\boldsymbol{a}$を定義\n","- $\\boldsymbol{y} = \\boldsymbol{x}+2$を計算\n","- $\\boldsymbol{z} = \\boldsymbol{y}\\cdot \\boldsymbol{y} \\cdot 3$を計算\n","- outをzの要素の平均値とする\n","\n","以上の計算を行ってきた\n","\n","ここで、xの変化量が、outにどれだけ影響するか？の勾配を算出する\n","\n","out.backward()として、微分演算ができるようにし、実際にxによる偏微分を求める\n","\n","つまり、 \n","${d\\boldsymbol{out} \\over d\\boldsymbol{x}}$\n","を計算する。"]},{"cell_type":"code","metadata":{"id":"YoA63JM-2sdk"},"source":["out.backward()\n","x.grad"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D1ac2K1D6I0y"},"source":["答えを確認する\n","\n","$$\\boldsymbol{z}_i = 3(\\boldsymbol{x}_i+2)^2$$\n","$$\\boldsymbol{out} = {1 \\over 4}\\sum_i \\boldsymbol{z}_i$$\n","$$\\left.\\boldsymbol{z}_i \\right|_{x_i=1} = 27$$\n","である\n","\n","$$\n","\\begin{aligned}\n"," {\\partial \\boldsymbol{out} \\over \\partial \\boldsymbol{x}_i}&={1 \\over 4}\\cdot{\\partial (3(\\boldsymbol{x}_i+2)^2)\\over \\partial \\boldsymbol{x}_i} \\\\\n","&={3 \\over 2}(\\boldsymbol{x}_i+2)\n","\\end{aligned}\n","$$\n","となる\n","\n","$\\boldsymbol{x}_1=1$のとき$4.5$となることがわかる\n","\n","同様に、$y=w\\cdot x + b$について、実際に微分値を求める\n","- $(x, w, b)=(1, 2, 3)$と初期化し、requires_gradをTrueとする\n","- $y$を求め、`y.backward()`で勾配を計算し、これを表示する"]},{"cell_type":"code","metadata":{"id":"1UykFaMLDEr-"},"source":["x = torch.tensor(1.0, requires_grad=True)\n","w = torch.tensor(2.0, requires_grad=True)\n","b = torch.tensor(3.0, requires_grad=True)\n","y = w * x + b\n","y.backward()\n","print(x.grad)  # dy/dx = w = 2\n","print(w.grad)  # dy/dw = x = 1\n","print(b.grad)  # dy/db = 1\n","x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UsNorTrlDRRS"},"source":["$y=x^2$について計算してみると${dy \\over dx} = 2x$であり、実際$x=2$のとき4になる"]},{"cell_type":"code","metadata":{"id":"yI1y03brDdix"},"source":["x = torch.tensor(2.0, requires_grad=True)\n","y = x ** 2\n","y.backward()\n","x.grad"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QwCYNgF2GhdC"},"source":["なお、若干型が異なるが、次のようにしても計算できる"]},{"cell_type":"code","metadata":{"id":"hnMi18ifGhx9"},"source":["x = torch.tensor(2.0, requires_grad=True)\n","y = x ** 2\n","torch.autograd.grad(y, x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ifpuoq1kDklB"},"source":["$y=e^x$とすると、${dy \\over dx}=e^x$であるから、$e^2$の値が得られる\n","\n","計算グラフを構築するときは numpy の関数 numpy.exp() を使ってはいけない\n","- 当然であるが、テンソル計算を行う専用の関数を使う torch.exp()\n"]},{"cell_type":"code","metadata":{"id":"r2NhEOZbDixH"},"source":["x = torch.tensor(2.0, requires_grad=True)\n","y = torch.exp(x)\n","y.backward()\n","x.grad"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RNopAsyRD0rW"},"source":["(利用するかどうかはともかく)三角関数も問題ない\n","\n","$y=sin(x)$のとき${dy \\over dx} = cos(x)$であるので、$x=\\pi$のとき-1になる"]},{"cell_type":"code","metadata":{"id":"qqKBkPXmEBC6"},"source":["x = torch.tensor(np.pi, requires_grad=True)\n","y = torch.sin(x)\n","y.backward()\n","print(x.grad)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sQ_hh9MqEGKi"},"source":["もっと複雑な例で試す\n","\n","$y=(\\sqrt{x}+1)^3$とすると${dy \\over dx} = {3(\\sqrt{x}+1)^2 \\over 2\\sqrt{x}}$となる\n","\n","答えがあっているかどうかは各自で確認してほしい"]},{"cell_type":"code","metadata":{"id":"yPj6LuShEy-H"},"source":["x = torch.tensor(2.0, requires_grad=True)\n","y = (torch.sqrt(x) + 1) ** 3\n","y.backward()\n","x.grad"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9F2qQ9-UE3Y_"},"source":["偏微分も問題ない\n","\n","$z=(x+2y)^2$とすると、\n","- ${\\partial z \\over \\partial x} = 2(x+2y)$\n","- ${\\partial z \\over \\partial y} = 4(x+2y)$\n","\n","となることから、$(x, y) = (1, 2)$のとき、それぞれ10および20となる"]},{"cell_type":"code","metadata":{"id":"SwFPsK5aFA7r"},"source":["x = torch.tensor(1.0, requires_grad=True)\n","y = torch.tensor(2.0, requires_grad=True)\n","z = (x + 2 * y) ** 2\n","z.backward()\n","print(x.grad)  # dz/dx\n","print(y.grad)  # dz/dy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oECJ6crpV2Lb"},"source":["計算を連結させても問題ない"]},{"cell_type":"code","metadata":{"id":"NSETroXUV-Rw"},"source":["def calc(x):\n","  x = x*2+1\n","  x = x**2\n","  x = x/(x+2)\n","  x = x.mean()\n","  return x\n","x = [1.0, 2.0, 3.0]\n","x = torch.tensor(x, requires_grad=True)\n","y= calc(x)\n","y.backward()\n","x.grad.tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tfW2ycOcWuKw"},"source":["勾配が正しく計算できているかどうかをxを微小変化させたときのyの差を見て確認する\n","\n","確認作業では特に自動微分の機能を使わないのでrequires_gradをFalseとして計算速度を上げる\n","  - といってもほとんど変わらないので意味はない\n","  - それでもやる！地球にやさしい授業にしたい\n"]},{"cell_type":"code","metadata":{"id":"l2sajYdSXFdN"},"source":["delta = 0.001  #変化量\n","x = [1.0, 2.0, 3.0]\n","x = torch.tensor(x, requires_grad=False)\n","y = calc(x).item()\n","\n","x_1 = [1.0+delta, 2.0, 3.0]\n","x_1 = torch.tensor(x_1, requires_grad=False)\n","y_1 = calc(x_1).item()\n","\n","x_2 = [1.0, 2.0+delta, 3.0]\n","x_2 = torch.tensor(x_2, requires_grad=False)\n","y_2 = calc(x_2).item()\n","\n","x_3 = [1.0, 2.0, 3.0+delta]\n","x_3 = torch.tensor(x_3, requires_grad=False)\n","y_3 = calc(x_3).item()\n","\n","# 勾配の計算\n","grad_1 = (y_1 - y) / delta\n","grad_2 = (y_2 - y) / delta\n","grad_3 = (y_3 - y) / delta\n","\n","print(grad_1, grad_2, grad_3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mx0MI5ioXum9"},"source":["殆ど値が一致していることがわかるであろう"]},{"cell_type":"markdown","metadata":{"id":"_voQwjIdS_2S"},"source":["PyTorchの機能や中身の一部であるが核心が理解できたであろう"]},{"cell_type":"markdown","metadata":{"id":"5X2eGmjkjvH2"},"source":["# 実際のデータの取り込み\n","\n","実際にはコード中でデータを直接記述することはなく、外部からデータを読み込むことが殆どであろう\n","\n","また、データはcsvなどの形式から取り込むであろうが、学習においては、準備されているデータセットを用いることも多いであろう\n","\n","- PyTorchも様々なデータセットを準備しており、詳細はPyTorchのDocsを参照するとよい\n","\n","ここでは、PyTorchが準備するデータセット、さらにPyTorchが備えるDataLoader、またPyTorchにおけるミニバッチ法の実装例を示す\n","\n","今回も手書き文字認識であるが、MNISTと呼ばれる、より本格的なデータを用いる"]},{"cell_type":"markdown","metadata":{"id":"Niaz8_W6OX34"},"source":["## DataLoaderを用いたミニバッチ法\n","\n","既に簡単に触れたが、PyTorchが準備するDataLoaderを利用するとデータの読み込みおよびミニバッチ法の実装が容易になる\n","\n","次の例のように、`torchvision.datasets`を使って最初に手書き文字のデータを読み込む\n","\n","- `torchvision.datasets`には様々なデータセットが準備されている\n","\n","`transform`をimportすることで、前処理としてのデータ変換を行う`transforms.ToTensor()`が利用できるようにする\n","\n","- PyTorchはテンソル型のデータしか扱えないため、変換するために必要\n","- なぜテンソル型にする必要があるかは既に説明済み\n","\n","そして**DataLoader**を用いてデータ分割とバッチ生成を行う\n","\n","- scikit-learnのtrain_test_split関数と目的は同じ\n","  - PyTorchは、モデル側もバッチに対応してまとめて入力できるようになるため、モデル側の記述の修正が不要\n","\n","- DataLoaderではバッチサイズを指定してデータ分割を行うため応用しやすい\n","\n","- ここではバッチサイズを大きめの256と設定しているが、GPU搭載マシンである場合はこの程度の数でも問題ない\n","\n","  - 大きめの数を指定することで、GPUの能力を発揮できるようになる\n","  - ただし、まとめて逆伝播の勾配計算も行われるため、学習の進み方、精度などに影響することに注意する\n","\n","- 訓練用データの取り出しには、毎回異なるデータをランダムに取り出してバッチを生成させるため、shuffleをTrueにしている\n","\n","- テスト用データの取り出しは、そのような必要はないため、Falseである\n","\n","次の実装例では、PyTorchが提供するデータセットを利用するので問題ないが、自分でデータセットを準備する場合、DataLoaderが読めるのは入力とラベルがセットになったデータである\n","  - 既にscikit-learnが準備するデータセットを用いた例において、TensorDatasetを使ってセットを作ることを学習済みである\n","\n","これには、次の命令を利用して事前に作成しておく必要がある\n","- `ds = TensorDataset(x, y)`\n","\n","なお、testデータの方は、特にミニバッチ法を取り入れる意味はないため、全数指定している\n","\n","- ミニバッチ法を採用しても、悪いということはない\n","\n","  - つまり`batch_size=len(mnist_test)`の部分をtrainと同様にしても問題はない"]},{"cell_type":"code","metadata":{"id":"81Il03RNAmbS"},"source":["import torch\n","from torchvision.datasets import MNIST\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","# 訓練データを取得\n","mnist_train = MNIST(\"./mydata\", # 保存先フォルダの指定\n","                    train=True, download=True,\n","                    transform=transforms.ToTensor())\n","# テストデータの取得\n","mnist_test = MNIST(\"./mydata\", # 保存先フォルダの指定\n","                   train=False, download=True,\n","                   transform=transforms.ToTensor())\n","print(\"訓練データの数:\", len(mnist_train), \"テストデータの数:\", len(mnist_test))\n","# DataLoaderの設定\n","batch_size = 256\n","train_loader = DataLoader(mnist_train, \n","                          batch_size=batch_size,\n","                          shuffle=True)\n","test_loader = DataLoader(mnist_test,\n","                         batch_size=len(mnist_test),\n","                         shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FalXNYaJPkoE"},"source":["## モデル構築\n","\n","前回は直接Sequentialによりモデルを構築したが、今回はより実践的に`nn.Module`モジュールを継承したクラスとしてモデルを構築する\n","\n","`.cuda()`により、モデルの計算はGPU上で行われる\n","\n","- Google ColaboratoryはGPUに対応している\n","\n","- メニューの編集、ノートブックの設定から、GPUを利用するのチェックが入っていることを確認しておく\n","\n","`__init__`つまりコンストラクタでトポロジを指定する\n","\n","- 親クラスのコンストラクタを呼び出すのを忘れないように\n","\n","forwardメソッドで活性化関数関数を指定する\n","\n","ネットワーク全体で見て、入力は画素数、出力は10個の数字である\n","\n","- 今回利用するMNISTの画像データの画素数は784である\n","  - scikit-learnの手書き文字認識が$8\\times 3=64$であったのに対して$28 \\times 28 = 784$と大幅に増えている\n","\n","  - 画素数も多いため、隠れ層のノード数も1024とかなり大きくしている\n","  - ただし層の数は少なめで、それほど精度は期待できないかもしれない\n","\n","ここでは、$28 \\times 28$の2次元画像構成を単純に$784$の1次元構成に変換するため、並び替えは伴わず、高速で効率の良いviewを利用する\n","\n","- transferでも動作し、この程度ならば殆ど変わらない\n"]},{"cell_type":"code","metadata":{"id":"SuqqZmsh_jNK"},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","class Net(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.fc1 = nn.Linear(784, 1024)  # 全結合層\n","    self.fc2 = nn.Linear(1024, 512)\n","    self.fc3 = nn.Linear(512, 10)\n","  def forward(self, x):\n","    x = x.view(-1, 784)  # バッチサイズ×入力の数\n","    x = F.relu(self.fc1(x))\n","    x = F.relu(self.fc2(x))\n","    x = self.fc3(x)\n","    return x\n","net = Net()\n","net.cuda()  # GPU対応\n","print(net)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qsW5zCKhQE9p"},"source":["## 学習\n","\n","モデルを訓練する\n","\n","**DataLoaderを使い、順次ミニバッチを取り出して訓練と評価を行う**\n","\n","1エポックの中で何度もミニバッチを使って訓練される\n","\n","なお、評価もミニバッチができるような記述であるが、全数を一気に取得するので、実際はミニバッチではない"]},{"cell_type":"code","metadata":{"id":"u6zwN3nArbGC"},"source":["from torch import optim\n","# 交差エントロピー誤差関数\n","loss_fnc = nn.CrossEntropyLoss()\n","# SGD\n","optimizer = optim.SGD(net.parameters(), lr=0.01)\n","# 損失のログ\n","record_loss_train = []\n","record_loss_test = []\n","# 学習\n","for i in range(20):  # 20エポック学習\n","  net.train()  # 訓練モードへ\n","  loss_train = 0\n","  for j, (x, t) in enumerate(train_loader):  # ミニバッチ(x,t)を取り出す\n","    x, t = x.cuda(), t.cuda()  # GPUのメモリに配置する\n","    y = net(x)\n","    loss = loss_fnc(y, t)\n","    loss_train += loss.item()  # ミニバッチなので、誤差を蓄積させていく\n","    optimizer.zero_grad() # RNNではためることもあるが普通は初めに勾配を0初期化必須\n","    loss.backward() # 逆伝播してパラメタを計算\n","    optimizer.step() # 計算した値でパラメタを更新\n","  loss_train /= j+1  # ループから抜けたらロスの平均を計算\n","  record_loss_train.append(loss_train)\n","  net.eval() # 評価モードへ\n","  loss_test = 0\n","  for j, (x, t) in enumerate(test_loader):  # ミニバッチ(x,t)の取り出し、trainと全く同じ\n","    x, t = x.cuda(), t.cuda()\n","    with torch.no_grad():\n","      y = net(x)\n","    loss = loss_fnc(y, t)\n","    loss_test += loss.item()\n","  loss_test /= j+1\n","  record_loss_test.append(loss_test)\n","  print(\"Epoch:\", i, \"Loss_Train:\", loss_train, \"Loss_Test:\", loss_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rJwwrWTw43rx"},"source":["## 誤差の推移\n","訓練データ、テストデータで誤差の推移をグラフ表示する\n","\n","TestよりもTrainの方が誤差が大きく出ているが、これはミニバッチにより複数の訓練の平均のロスと訓練後のロスを比較しているためである\n","- これは、仮にTestを全数ではなくミニバッチにしたとしても見られる現象である"]},{"cell_type":"code","metadata":{"id":"OaJx4swE45XI"},"source":["import matplotlib.pyplot as plt\n","plt.plot(range(len(record_loss_train)), record_loss_train, label=\"Train\")\n","plt.plot(range(len(record_loss_test)), record_loss_test, label=\"Test\")\n","plt.legend()\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Error\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iMrpac0m4Nct"},"source":["## 正解率\n","モデルの性能を把握するため、テストデータ使い正解率を測定する"]},{"cell_type":"code","metadata":{"id":"IRkGCYMM_N35"},"source":["correct = 0\n","total = 0\n","net.eval()\n","for i, (x, t) in enumerate(test_loader):\n","  x, t = x.cuda(), t.cuda()  # GPU対応\n","  x = x.view(-1, 784)\n","  with torch.no_grad():\n","    y = net(x)\n","  correct += (y.argmax(1) == t).sum().item()\n","  total += len(x)\n","print(\"正解率:\", str(correct/total*100) + \"%\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xWd77ZqlZYD_"},"source":["## `model.eval()`と`torch.no_grad()`について\n","\n","### `model.eval()`\n","\n","`.eval()`はevalモードでの動作でBatch Normalization(Normを含むnn.Moduleの層).やDropoutなどが評価用として動作する\n","\n","- これらの方法を用いた時の評価では評価結果に影響するため必須\n","\n","### `torch.no_grad()`\n","\n","一般に、\n","```\n","with torch.no_grad:\n","  modelの評価コード\n","```\n","といった利用の仕方となる\n","\n","- PyTorchは計算履歴を変数に保存するため、GPUを利用する場合、GPUメモリを浪費し、実行できなくなる場合がある\n","  - train時やforward計算時に勾配計算用パラメータを保存することでbackward計算を高速化するautograd機能のため\n","\n","- autogradを無効化し、メモリ使用量を減らして計算速度を向上させる\n","  - 必須ではなく、無くても評価結果に影響しない\n","  - メモリサイズがおよそ1/5になるため、GPUでは5倍といった大きなバッチを組むことができる\n","  - CPUはメモリが比較的十分にあり、全てのモデルが基本メモリ上で動作させるため、その効果は微妙になるといえる\n","\n","- 計算履歴を保存する必要がない場合、例えばbackwordが必要ない検証などは、保存しない設定で計算するとよい\n","\n","- optimizer.zero_grad()を呼び出す必要がなくなる\n","  - そもそも評価時なので不必要\n","\n","### `torch.no_grad`の具体例\n","\n","ついでに、```with torch.no_grad():```とするのと```torch.set_grad_enabled(False)```とするのが違うことも示す\n","\n","訓練時は勾配計算が必要であるが、評価時は不要である\n","\n","その時、\n","```\n","model.eval();torch.set_grad_enabled(False)\n","```\n","とすると、計算記録の追加は行われないが、過去の計算記録は残っているので、処理速度は速くなるが、メモリ占有量は変わらない\n","\n","次に、\n","```\n","with torch.no_grad():\n","  model.eval()\n","```\n","とすると、記録すらもなくなるため、メモリ占有量も減る\n","\n","> withの用法について\n",">\n","> withは、開始と終了がセットになった処理を行うという一般的な説明がされているが、これは、コンストラクタはともかく、デストラクタの呼び出しが曖昧なガベージコレクト型言語のpythonについて、明示的なデストラクタを呼び出すことを意味している\n",">\n","> ```\n","with open('filename', 'w', opener=opener) as f:\n","    print('test', file=f)\n","```\n","> などとして使う\n","> \n","> printが実行されるとデストラクタが明示的に呼び出され、fのメソッドとしてのデストラクタ、つまりcloseが呼び出されるため実質close不要\n","\n","### パラメータの変更について\n","\n","それ以外に、例えば、学習途中で、強制的に重みを変える、消失しそうだから、2倍にしたいとする(そういうことをする意味があるかどうかはさておき)\n","\n","```\n","for param in model.parameters():\n","  param.data *= 2.0\n","```\n","これは、lossの勾配を計算する際にエラーになる\n","\n","つまり、2倍するという計算の履歴すらも記録されてしまうからである\n","\n","次のようにするとよい\n","```\n","with torch.no_grad():\n","  for param in model.parameters():\n","    param.data *= 2.0\n","```\n","自ら正規化規則を記述するぐらいの気概がある場合は参考になるかもしれない"]},{"cell_type":"markdown","metadata":{"id":"KBP4ZImbZre_"},"source":["# PyTorchにおけるクロスエントロピーについて\n","\n","PyTorchのCrossEntropyLoss関数には癖があるのでここで説明する\n","\n","まず、PyTochにおける正解となるターゲットの記述の仕方に注意する\n","- One-hotで正解が3(ここでは0から数えて2)であるoutputと、正解ラベルである3(これも2)を格納したtargetを準備する\n","\n","まず、どちらも、**配列の1つの要素として**、one-hotの値や2が格納されていることに注意する\n","- 複数の演算を同時に行う必要から、配列の要素として扱う\n","  - 今は一つであるが、複数入れるとそれぞれで計算してくれる\n","- このあたりの配列の扱いに慣れていないと、すぐにパニックに陥る\n","- 動いている例を見つけたら、typeやshapeにより型をきちんとしらべること\n","\n","もう一つ、outputはFloatTensor、targetはLongTensorであることに注意すること\n","- ニューラルネットワークの出力は通常FloatTensorで、正解ラベルは通常整数であることから、このようになっている\n","  - 親切からそうなっているが、何も考えないで用いるとトラブルの要因の一つになりうる\n","\n","なお、FloatTensorを用いているが、`torch.Tensor([[0,0,1]]).to(dtype=torch.long)`というようにtoで変換してもよく、floatであれば`dtype=torch.float`となる\n"]},{"cell_type":"code","metadata":{"id":"AvsRsnV6PriZ"},"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","\n","output = torch.FloatTensor([[0,0,1]])\n","target = torch.LongTensor([2])\n","print(output, target)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gYbWlolscf84"},"source":["この場合、定義通りクロスエントロピーロスを計算しようとすると、次のようになる\n","\n","但し、極端な出力のため、$log 0$は計算できず、そのまま省略している\n","$$Loss(p,q)=-\\sum_i{p_i log q_i}$$\n","\n","必要な箇所だけ計算すると、"]},{"cell_type":"code","metadata":{"id":"63PFRvE5cmNf"},"source":["-(1*np.log(1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e8LaAuSceF7m"},"source":["となり、つまり0である\n","\n","しかしながら、PyTorchのクロスエントロピーは異なる値となる"]},{"cell_type":"code","metadata":{"id":"sXHYg5c-eOAY"},"source":["crossentropy = nn.CrossEntropyLoss()\n","crossentropy(output, target)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0EkDSKZIeYap"},"source":["なお、本来は`[0,0,1]`といった美しい確率分布になるような出力値がNNから得られることはまずない。\n","- これを確率とするには、一度Softmaxを介さなければならない"]},{"cell_type":"markdown","metadata":{"id":"65PMaBJ9e1mp"},"source":["次のような例でも問題ないことを考えれば、より分かりやすいのではないだろうか\n","\n","outputは、`[0,0,1]`としているが、別に次のような値でも構わない\n","- ニューラルネットワークからの出力は0から1の値とは限らず、理論的には実数域すべてを想定できる\n","- 配列の最後の要素がとびぬけていると値が小さくなる。"]},{"cell_type":"code","metadata":{"id":"Ro-Jynu7e_W0"},"source":["output2 = torch.FloatTensor([[2.0,35.2,-13.7]])\n","print(crossentropy(output2, target))\n","output3 = torch.FloatTensor([[2.0, -13.7, 35.2]])\n","print(crossentropy(output3, target))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dMEUSSHtjLfV"},"source":["さて、PyTorchのCrossEntropyLoss()の出力値が、実際のクロスエントロピーの定義に基づく式の演算結果と異なるのは、PyTorchのCrossEntropyLoss()は、Softmax()とNLLLoss()の合成で計算されるためである\n","\n","確認しよう\n","- まず、Softmaxの結果だけみてみよう"]},{"cell_type":"code","metadata":{"id":"5N8z8VhujKfg"},"source":["softmax = nn.Softmax(dim=1) # dim=1を忘れるとワーニングが出る\n","softmax(output)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tgl3te5JlvkK"},"source":["この結果のlogを取り、NLLLossを計算する\n","\n","NLLLossは、この例では$$-x[class]$$を計算するだけの関数\n","- 実際には、まとめて計算するため複数の要素が投入されるが、それらすべての平均を計算してくれる関数\n","- ミニバッチの場合、まとめて計算し、その平均でパラメータを更新するが、そのためにある関数"]},{"cell_type":"code","metadata":{"id":"rMQvb5o8kDNC"},"source":["nllloss = nn.NLLLoss()\n","nllloss(torch.log(softmax(output)), target)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kH5KFnKQmCN_"},"source":["このように答えが一致した\n","\n","つまり、もともとのクロスエントロピーの定義に基づけば、`nsloss(torch.log(output), target)` となる\n","\n","しかしながら実際の利用において、この形単独で利用するケースは限られることからPyTorchではsoftmax関数が内包されている\n","\n","さて、CrossEntropyを利用するのと、Softmax+log+NLLとするのと同じだから、どちらでもよいではないか？ということになりそうであるが、そうではない。\n","\n","専用関数は次のような恩恵に預かることができる\n","\n","- exp-normalize trickの恩恵を最大化するのは、CrossEntropyで一気にやる方法であること\n","- 内部で最適化されていること\n","\n","この、exp-normalize trickについて簡単に説明する\n","\n","普通にsoftmaxを定義通り計算すると、\n","$$\n","\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n","$$\n","となるが、$e^x$が大きすぎ、計算機上での計算において、計算結果がオーバーフローする可能性がある\n","\n","そこで、値を小さくして計算する手法として、\n","$$\n","e^{x_i}\\cdot e^{-b}\\cdot e^{b}=e^{x_i-b}\\cdot e^{b}\n","$$\n","となることから、\n","$$\n","\\text{softmax}(x_i) = \\frac{e^{x_i-b}e^{b}}{\\sum_j e^{x_j-b}e^{b}} = \\frac{e^{x_i-b}}{\\sum_j e^{x_j-b}}\n","$$\n","として計算しても同じ答えとなることがわかる\n","\n","PyTorchは、これらの処理も自動的に行っている\n","\n","log_softmaxについても同様で、log-sum-expと呼ばれる、名前だけ変わってやっていることは変わらないテクニックがある\n","\n","$$\n","\\begin{align}\n","\\text{log_softmax}(x_i) &= \\log\\left(\\frac{e^{x_i}}{\\sum_j e^{x_j}}\\right)\\\\\n","&= x_i - \\log \\left( \\sum_j e^{x_j} \\right) \\\\\n","&= x_i - \\log \\left( \\sum_j e^{x_j - b}e^b \\right) \\\\\n","&= x_i - \\log \\left( \\sum_j e^{x_j - b} \\right) - b \\\\\n","\\end{align}\n","$$\n","\n","とにかく、複合関数を使わないと、予期せぬ落とし穴にはまることがある"]},{"cell_type":"markdown","metadata":{"id":"nt5fwkLqbWMa"},"source":["# Pytorchの活性化関数\n","\n","PyTorchでは、以下の活性化関数が用意されている\n","\n","ELU, Hardshrink, Hardtanh, LeakyReLU, LogSigmoid, MultiheadAttention, PReLU, ReLU, ReLU6, RReLU, SELU, CELU, GELU, Sigmoid, Softplus, Softshrink, Softsign, Tanh, Tanhshrink, Threshold, Softmin, Softmax, Softmax2d, LogSoftmax, AdaptiveLogSoftmaxWithLoss"]},{"cell_type":"code","metadata":{"id":"eiDw5jm_s8ZL"},"source":["import torch\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","def drawGraph(x, y, y_dash, title):\n","  fig = plt.figure(figsize=(12, 4))\n","  ax1 = fig.add_subplot(1, 2, 1)\n","  ax2 = fig.add_subplot(1, 2, 2)\n","  ax1.plot(x, y)\n","  ax2.scatter(x, y_dash, s=1, color='orange')\n","  ax1.set_title(title)\n","  ax2.set_title(title+' Gradient')\n","  plt.show()\n","x=torch.linspace(-6, 6, 1000, dtype=torch.float, requires_grad=True)\n","y=F.relu(x)\n","z=y.sum()\n","z.backward()\n","z_dash=x.grad\n","drawGraph(x.detach().numpy(), y.detach().numpy(), z_dash.detach().numpy(), 'ReLU')\n","x=torch.linspace(-6, 6, 1000, dtype=torch.float, requires_grad=True)\n","y=F.hardtanh(x)\n","z=y.sum()\n","z.backward()\n","z_dash=x.grad\n","drawGraph(x.detach().numpy(), y.detach().numpy(), z_dash.detach().numpy(), 'HardTanh')\n","x=torch.linspace(-6, 6, 1000, dtype=torch.float, requires_grad=True)\n","y=F.relu6(x)\n","z=y.sum()\n","z.backward()\n","z_dash=x.grad\n","drawGraph(x.detach().numpy(), y.detach().numpy(), z_dash.detach().numpy(), 'ReLU6')\n","x=torch.linspace(-6, 6, 1000, dtype=torch.float, requires_grad=True)\n","y=F.elu(x)\n","z=y.sum()\n","z.backward()\n","z_dash=x.grad\n","drawGraph(x.detach().numpy(), y.detach().numpy(), z_dash.detach().numpy(), 'eLU')\n","x=torch.linspace(-6, 6, 1000, dtype=torch.float, requires_grad=True)\n","y=F.selu(x)\n","z=y.sum()\n","z.backward()\n","z_dash=x.grad\n","drawGraph(x.detach().numpy(), y.detach().numpy(), z_dash.detach().numpy(), 'SeLU')\n","x=torch.linspace(-1, 1, 1000, dtype=torch.float, requires_grad=True)\n","y=F.celu(x)\n","z=y.sum()\n","z.backward()\n","z_dash=x.grad\n","drawGraph(x.detach().numpy(), y.detach().numpy(), z_dash.detach().numpy(), 'CeLU')\n","x=torch.linspace(-6, 6, 1000, dtype=torch.float, requires_grad=True)\n","y=F.leaky_relu(x)\n","z=y.sum()\n","z.backward()\n","z_dash=x.grad\n","drawGraph(x.detach().numpy(), y.detach().numpy(), z_dash.detach().numpy(), 'LeakyReLU')\n","x=torch.linspace(-6, 6, 1000, dtype=torch.float, requires_grad=True)\n","y=F.prelu(x, torch.tensor([0.25]))\n","z=y.sum()\n","z.backward()\n","z_dash=x.grad\n","drawGraph(x.detach().numpy(), y.detach().numpy(), z_dash.detach().numpy(), 'PReLU')\n","x=torch.linspace(-6, 6, 1000, dtype=torch.float, requires_grad=True)\n","y=F.rrelu(x)\n","z=y.sum()\n","z.backward()\n","z_dash=x.grad\n","drawGraph(x.detach().numpy(), y.detach().numpy(), z_dash.detach().numpy(), 'RReLU')\n","x=torch.linspace(-6, 6, 1000, dtype=torch.float)\n","y=F.glu(x)\n","\n","# xを2つに分解する\n","x1=torch.linspace(-6, 0, 500, dtype=torch.float, requires_grad=True)\n","x2=torch.linspace(0, 6, 500, dtype=torch.float, requires_grad=True)\n","y=torch.mul(x1, torch.sigmoid(x2))\n","z=y.sum()\n","z.backward()\n","#x1 x2 で自動微分\n","z_dash1=x1.grad\n","z_dash2=x2.grad\n","z_dash3=torch.add(z_dash1, z_dash2)\n","drawGraph(x2.detach().numpy(), y.detach().numpy(), z_dash3.detach().numpy(), 'GLU')\n","x=torch.linspace(-6, 6, 1000, dtype=torch.float, requires_grad=True)\n","y=F.gelu(x)\n","z=y.sum()\n","z.backward()\n","z_dash=x.grad\n","drawGraph(x.detach().numpy(), y.detach().numpy(), z_dash.detach().numpy(), 'GeLU')\n","x=torch.linspace(-6, 6, 1000, dtype=torch.float, requires_grad=True)\n","y=F.logsigmoid(x)\n","z=y.sum()\n","z.backward()\n","z_dash=x.grad\n","drawGraph(x.detach().numpy(), y.detach().numpy(), z_dash.detach().numpy(), 'LogSigmoid')\n","x=torch.linspace(-6, 6, 1000, dtype=torch.float, requires_grad=True)\n","y=F.hardshrink(x)\n","z=y.sum()\n","z.backward()\n","z_dash=x.grad\n","drawGraph(x.detach().numpy(), y.detach().numpy(), z_dash.detach().numpy(), 'HardShrink')\n","x=torch.linspace(-6, 6, 1000, dtype=torch.float, requires_grad=True)\n","y=F.tanhshrink(x)\n","z=y.sum()\n","z.backward()\n","z_dash=x.grad\n","drawGraph(x.detach().numpy(), y.detach().numpy(), z_dash.detach().numpy(), 'TanhShrink')\n","x=torch.linspace(-6, 6, 1000, dtype=torch.float, requires_grad=True)\n","y=F.softsign(x)\n","z=y.sum()\n","z.backward()\n","z_dash=x.grad\n","drawGraph(x.detach().numpy(), y.detach().numpy(), z_dash.detach().numpy(), 'Softsign')\n","x=torch.linspace(-6, 6, 1000, dtype=torch.float, requires_grad=True)\n","y=F.softplus(x)\n","z=y.sum()\n","z.backward()\n","z_dash=x.grad\n","drawGraph(x.detach().numpy(), y.detach().numpy(), z_dash.detach().numpy(), 'Softplus')\n","x=torch.linspace(-6, 6, 1000, dtype=torch.float, requires_grad=True)\n","y=F.softmin(x,  0)\n","z=y.sum()\n","z.backward()\n","z_dash=x.grad\n","drawGraph(x.detach().numpy(), y.detach().numpy(), z_dash.detach().numpy(), 'Softmin')\n","x=torch.linspace(-6, 6, 200, dtype=torch.float)\n","y=F.softmax(x)\n","x=torch.linspace(-6, 6, 1000, dtype=torch.float, requires_grad=True)\n","y=F.softshrink(x)\n","z=y.sum()\n","z.backward()\n","z_dash=x.grad\n","drawGraph(x.detach().numpy(), y.detach().numpy(), z_dash.detach().numpy(), 'SoftShrink')\n","x=torch.linspace(-6, 6, 1000, dtype=torch.float, requires_grad=True)\n","y=F.log_softmax(x, 0)\n","z=y.sum()\n","z.backward()\n","z_dash=x.grad\n","drawGraph(x.detach().numpy(), y.detach().numpy(), z_dash.detach().numpy(), 'LogSoftmax')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xBbjHnpdm3BH"},"source":["# Pytorchの最適化手法\n","\n","次のような最適化手法があり、その基本は既に学習済である\n","- SGD : torch.optim.SGD\n","- Adagrad : torch.optim.Adagrad\n","- RMSprop : torch.optim.RMSprop\n","- Adadelta : torch.optim.Adadelta\n","- Adam : torch.optim.Adam\n","- AdamW : torch.optim.AdamW\n","\n","これらについて、関数$f(x,y)=x^2+y^2$ 平面上での最適化過程を比較する\n","- なお次のようなグラフであり、この底へ向かわせる"]},{"cell_type":"code","metadata":{"id":"bUgImhPwrGnK"},"source":["def func(x, y):\n","  return x**2+y**2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mTd3gv0ZngpP"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","x = y = np.linspace(-10, 10)\n","X, Y = np.meshgrid(x, y)\n","Z = func(X, Y)\n","fig = plt.figure(figsize=(8, 6))\n","ax = fig.add_subplot(1, 1, 1, projection='3d')  # 3D描画機能を持ったサブプロット作成\n","ax.plot_surface(X, Y, Z)   # 曲面を描画した3Dグラフを表示"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qSkWXwNwpGoG"},"source":["import torch\n","from torch import optim\n","import matplotlib.pyplot as plt\n","import numpy as np\n","# 次の全ての最適化手法を試すため名前をリストに登録\n","classes = ['SGD', 'Adagrad', 'RMSprop', 'Adadelta', 'Adam', 'AdamW']\n","x_list = {}\n","y_list = {}\n","f_list = {}\n","optimizers = {}\n","# それぞれの最適化手法毎に配列を準備\n","for key in classes:\n","  x_list[key] = []\n","  y_list[key] = []\n","  f_list[key] = []\n","# 各最適化手法を用いて実際に最適化を行う\n","for key in classes:\n","  print(\"Optimizer:\"+key)\n","  # 初期値(最適化を行う最初の場所)を統一しておく\n","  x = torch.tensor(-75.0, requires_grad=True)\n","  y = torch.tensor(-10.0, requires_grad=True)\n","  params = [x, y]\n","  # 学習率（SGDはいきなり最適に向かってしまう好都合な条件のため0.1とする）\n","  optimizers['SGD'] = optim.SGD(params, lr=0.1)\n","  optimizers['Adagrad'] = optim.Adagrad(params, lr=1.0)\n","  optimizers['RMSprop'] = optim.RMSprop(params, lr=1.0)\n","  optimizers['Adadelta'] = optim.Adadelta(params, lr=1.0)\n","  optimizers['Adam'] = optim.Adam(params, lr=1.0)\n","  optimizers['AdamW'] = optim.AdamW(params, lr=1.0)\n","  for i in range(5000): # 繰り返し\n","    # Autogradを用いて勾配を求めてoptimizerで更新する\n","    optimizers[key].zero_grad()\n","    outputs = func(x, y)\n","    outputs.backward()\n","    optimizers[key].step()\n","    x_list[key].append(x.item())\n","    y_list[key].append(y.item())\n","    f_list[key].append(outputs.item())\n","# 結果のグラフ作成\n","fig = plt.figure(figsize=(15, 10))\n","for i, key in enumerate(classes):\n","  ax = fig.add_subplot(2, 3, i+1)\n","  ax.scatter(x_list[key], y_list[key])\n","  ax.plot(-75, -10, '+')\n","  ax.set_title(key, fontsize=16)\n","  ax.set_xlabel('x', fontsize=16)\n","  ax.set_ylabel('y', fontsize=16)\n","  ax.set_xlim(-80, 80)\n","  ax.set_ylim(-15, 15)\n","plt.tight_layout()\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PCmet6R4tidc"},"source":["見方ですが、丁度先に示した3Dの図を上から見た時の移動を示している\n","- (x,y)=(0,0)が最小値でここに向かう過程を示している"]},{"cell_type":"markdown","metadata":{"id":"uNa2nq5Ysmir"},"source":["## SGD（Stochastic Gradient Decent : 確率的勾配降下法）\n","\n","代表的な最適化手法で、求めた勾配方向に求めた大きさだけパラメータを更新する\n","\n","Pytorchでは、MomentumといったSGCの改良アルゴリズムを利用する場合、`torch.optim.SGD()`のパラメータに指定する\n","- パラメータには momentum、 dampening、 nesterovなどを指定できる\n","\n","`torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)`\n","\n","欠点：複雑な状況では最適化が進みにくくなる"]},{"cell_type":"markdown","metadata":{"id":"Tq03o5W8t2pB"},"source":["## Adagrad（Adaptive Gradient Algorithm）\n","\n","学習率を学習過程の中で更新する手法\n","- パラメータの更新度合いで次の学習率を各パラメータ毎に調整\n","  - 大きく更新されたパラメータの学習率をより小さく調整\n","  - 大きな勾配の影響が小さくなり、効率的に最適化を行う\n","\n","欠点：学習が進むと学習率が小さくなり更新されなくなる"]},{"cell_type":"markdown","metadata":{"id":"OHzEJ3O6AdFO"},"source":["## RMSprop（Root Mean Square propagation）\n","\n","勾配情報をAdagrad同様に記憶するが、古い情報を落として、新しい勾配情報がより反映されるように工夫されている\n","- 学習が進んでも学習率の調整を適切に行う"]},{"cell_type":"markdown","metadata":{"id":"KQ6yhw67A5de"},"source":["## Adadelta\n","\n","AdagradやRMSpropがパラメータを勾配の単位で更新する点を改良している\n","- 過去の更新量の移動平均を過去の勾配の移動平均で割り、さらに現在の勾配を掛けた値を更新量とする\n","- つまり、常に同じ単位で更新される\n","- 学習率の設定が不要\n","\n","欠点：収束がやや遅い"]},{"cell_type":"markdown","metadata":{"id":"JcWq15f1ByRd"},"source":["## Adam（Adaptive Moment Estimation）\n","\n","MomentumとAdagradを併せ持つ\n","\n","蛇行して効率が悪いように見えるかもしれないが、複雑な形状ではよりよい方向を適切に探し出すことができる場合がある"]},{"cell_type":"markdown","metadata":{"id":"e2k9ulX5B--D"},"source":["## AdamW\n","\n","Adamの Weight decay（重み減衰）に関する式に変更を加えている\n","- 損失関数計算時およびパラメータ更新時にL2 正則化項を追加\n","- Adamよりも適切な Weight decay を与える"]},{"cell_type":"markdown","metadata":{"id":"aDf4DrjJAsgE"},"source":["\n","# 課題8 (TorchTensor)\n","\n","これまで同様にタイトル付のノートブックとして提出しなさい\n","\n","**問題1**\n","\n","次のようなPyTorchテンソル t を作成しなさい\n","`print(t)`とすると、\n","```\n","tensor([[[  1,   2],\n","         [  4,   5],\n","         [  7,   8]],\n","\n","        [[ -1,  -2],\n","         [ -4,  -5],\n","         [ -7,  -8]],\n","\n","        [[ 10,  20],\n","         [ 40,  50],\n","         [ 70,  80]],\n","\n","        [[-10, -20],\n","         [-40, -50],\n","         [-70, -80]]])\n","```\n","と返す\n","\n","**問題2**\n","\n","tの大きさをsizeメソッドを使って確認しなさい\n","\n","**問題3**\n","\n","`print(t[:,-1,:])`\n","の出力結果を記すと共に、なぜその結果になるかを説明しなさい\n","\n","**問題4**\n","\n","`print(t[:-1:])`\n","の出力結果を記すと共に、なぜその結果になるかを説明しなさい\n","\n","**問題5**\n","\n","問題6に対して、`t[]`の中に**数字を2箇所にだけ書き足す**ことで同じ出力結果が得られるようにしなさい\n","\n","要するに問題5は、省略形であるので、省略なしに記しなさいということである\n","- 単純に何の省略か？という問題にすると答えが複数現れるため、あえてこのような問いにしている\n","\n","**問題6**\n","\n","$y = 2x^3-3x^2-12x+1$を求める関数f(def文で、def f(x):などとする)を定義し、$x=1$の時の導関数の値をPyTorchのautograd機能を利用して計算しなさい\n","\n","また、手計算で結果が正しいことを確認しなさい"]}]}