{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dataai-text-C-PyTorch-転移学習・GAN.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"iClPGVu3SCIE"},"source":["---\n",">「『優れた芸術家はまねをし、偉大な芸術家は盗む』とピカソは言った。\n","だからすごいと思ってきたさまざまなアイデアをいつも盗んできた。」\\\n",">スティーブ・ジョブズ\n","---"]},{"cell_type":"markdown","metadata":{"id":"V4Vk2qzyem_b"},"source":["# 転移学習\n","\n","一般にデモで凄い！と思わせるようなAIアプリはともかく、シンプルなVGGやResNetといった画像認識でさえ、膨大なデータと計算量が必要である\n","- フルスクラッチ（ランダム重み）から学習できるひとは一握り\n","\n","では、あきらめるのか？というと、それに対する一つの答えが**転移学習(Transfer Learning)**である\n","- DLにおけるエコ、地球にやさしいDL\n","\n","転移学習とは**大規模データで学習済みのモデルを別のタスクに転移、つまり応用する技術全般を指す**"]},{"cell_type":"markdown","metadata":{"id":"cT_FBXUnem_d"},"source":["## 一般的な転移学習が扱う問題\n","\n","特徴空間とラベルの違い、また、完全に異なるのか、分布が異なるのかの違いの掛け合わせにより、次の4つの分類がある\n","\n","- 特徴空間が異なる問題\n","\n"," 知識の転移元がそもそも異なる、つまり特徴空間が異なる転移学習のことで、**異質(ヘテロ)転移学習(Heterogeneous Transfer Learning)**と呼ばれる\n","\n","- 周辺確率分布が異なる問題\n","\n"," 特徴空間は同じであるが、その特徴空間における周辺確率分布が異なる問題への転移学習を、**同質(ホモ)転移学習(Homogeneous Transfer Learning)**と呼ばれる\n","\n","- ラベル空間が異なる問題\n","\n"," ラベルの空間が異なる場合への適用例を指す\n","\n","- 条件付確率分布が異なる問題\n","\n"," ラベルの空間は同一であるが、その出現頻度が異なる場合への適用例を指す"]},{"cell_type":"markdown","metadata":{"id":"7ps7NFPgem_d"},"source":["## 一般的な転移学習の分類\n"]},{"cell_type":"markdown","metadata":{"id":"gFVU91yLTW8F"},"source":["### 問題設定に基づく分類\n","\n","**帰納的転移学習(Inductive Transfer Learning)**\n","\n","元の特徴量(ソースドメイン)と転移先の特徴量(ターゲットドメイン)が同じか否かに関わらず、元の変換(特に条件付確率モデル)(ソースタスク)と転移先の変換(ターゲットタスク)が異なる\n","\n","- 帰納的転移学習はターゲットドメインにラベルが存在する場合に定義可能\n","- ソースにラベルがある場合は**機能的転移学習**、ない場合は**自己教示学習**と呼ぶ\n","\n","**教師なし転移学習(Unsupervised Transfer Learning)**\n","\n","ソースタスクとターゲットタスクが異なり、教師なし学習であるクラスタリングや次元削減をターゲットドメインで解く手法\n","- ソースドメインとターゲットドメインの両方にラベルがない\n","\n","**トランスダクティブ転移学習(Trunsductive Transfer Learning)**\n","\n","ソースタスクとターゲットタスクが同一であるが、ドメインが異なる場合の手法\n","\n","- ソースドメインにおけるラベル付きデータの殆どが利用可能であるが、ターゲットドメインのラベル付きデータが利用できない場合を指す"]},{"cell_type":"markdown","metadata":{"id":"OW9aCqPzTcDG"},"source":["### アプローチに基づく分類\n","\n","何を転移するかによる分類\n","\n","**インスタンス転移**\n","- ソースドメインのインスタンス、つまりサンプルや データセットの特定部分について再度重み付けすることで、ターゲットドメインの学習に再利用する\n","\n","**特徴表現転移**\n","- ソースドメインとターゲットドメインの差や、分類・回帰モデルの誤差を軽減する都合の良い特徴表現を発見し利用する\n","\n","**パラメータ転移**\n","- ソースモデルとターゲットモデルそれぞれのハイパーパラメータが同一であるという前提のもと、そのパラメータや事前分布を発見し利用する\n","\n","**関係性のある知識の転移**\n","- ソースモデルにおける結果としてのデータ間の関係性をターゲットドメインに転移し利用する\n"]},{"cell_type":"markdown","metadata":{"id":"2N_KYVQ4em_e"},"source":["## ディープラーニングにおける転移学習の分類\n","\n","**ネットワークベース転移学習**\n","\n","ソースドメインのネットワーク(モデル)を再利用する手法で、DLにおける転移学習といえば、一般にこの手法を指す\n","\n","- **事前学習済みネットワークベース転移学習**\n","\n"," 最終層で最終出力が得られるが、その層まではそこに至る特徴を保存しているといえ、この特徴を再利用するために最終層付近のみ重みを再学習する、もしくは、ネットワークを組み替える\n","\n","- **事前学習済ネットワークの微調整(ファインチューニング)**\n","\n"," 単にファインチューニングと呼ばれることの多い手法で、学習済みネットワークの重みを初期値として、ターゲットドメインでモデル全体の重みを再学習する、もしくは、ネットワークの一部を組み替えて全体を再学習させる\n","  - 違いは、固定するパラメタがあるかないか\n","\n","なお、この違いについては、文献などにより様々存在しており、画一的な見解がなく、例えば次のような分類も存在する\n","\n","- 転移学習\n","  学習済みモデルの重みは更新せず、このモデルに新たな層を追加、この追加した層のみ学習させ重みを更新する\n","\n","- ファインチューニング\n","  学習済みモデルの一部の重みを更新せず、主に後段の層の重みを更新しつつ、追加した層も学習により重みを更新する\n","\n","この分類は「層を追加する」という観点で異なるが、いずれの場合も、「ファインチューニングの方が更新対象範囲が広い」という観点で類似している\n","\n","\n","**インスタンスベース転移学習**\n","\n","ソースドメインのインスタンス、つまりサンプルやデータセットの特定部分について再重み付けすることで、ターゲットドメインの学習に再利用する\n","\n","- 先に示したインスタンス転移を行うこと\n","\n","**地図ベース転移学習**\n","\n","ソースドメインとターゲットドメインのインスタンスを新しいデータ空間にマッピングして利用すること\n","\n","**敵対ベース転移学習**\n","\n","GANを活用しソースドメインとターゲットドメインの両方に適用可能で転移可能な表現を見つけ出して利用すること\n","- ソースドメインとターゲットドメインそれぞれから特徴量を抽出し、GANのDescriminator(識別ネットワーク)でどちらのドメインに属する特徴量かを判別させる\n","  - 判定精度が低ければ、両ドメインの特徴量の差が小さく、転送性が良いと判断する\n","  - 判別性能が高い場合は、特徴量の差が大きく、転送性が低いと判断する\n","\n","なお、以下の関連用語についてもここで纏めて奥\n","\n","**蒸留(Distillation)**\n","\n","大容量かつ深いモデルで学んだ知識を蒸留、すなわち縮約し、小さく軽量なモデルの学習に活用すること\n","\n","**マルチタスク学習**\n","\n","ソースとターゲットを区別せず、共有層を含む複数のタスクを同時に学習させること"]},{"cell_type":"markdown","metadata":{"id":"hzwkgbIvem_f"},"source":["## 転移学習のメリット・デメリット\n","\n","**メリット**\n","\n","- ある領域(ドメイン)で学習したモデルを別の領域に適用するため、サンプル数が限定されている場合でも比較的高精度なモデルを構築できる\n","\n","  - 高品質なデータを大量に取得することは、コスト的・時間的に難しい場合が多い\n","  - 大量かつ高品質なデータによって学習した質の高いモデル・知識領域を転移させることで、限定的なデータであっても高精度なモデルを構築できる可能性がある\n","\n","- モデルを短時間で構成\n","\n","  - 事前学習済みネットワークを利用する場合は、0から学習する必要がないため学習時間・コストを短縮できる\n","\n","- シミュレーター環境で訓練したモデルを現実に適応させる\n","\n","- これらの背景にはすでに学んだが、DNNにおいては、タスク共通の主要な特徴があり、これを共通化できるという特徴を利用している\n","  - 特に画像認識などの領域ではスタイルトランスファーのように、大きくとらえる・細かくとらえるといった認識範囲や粒度の特徴をうまく活用できる可能性がある\n","\n","**デメリット**\n","\n","- 転移が必ず精度改善などよい結果を生むとは限らない\n","  - **負の転移(negative transfer)**と呼ばれる状況が発生しうる\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"udlnqFaFem_g"},"source":["# 転移学習の実際\n","\n","ここでは、代表的な事前学習済みネットワークベース転移学習について学ぶ\n","\n","ファインチューニング(全パラメタを再学習)と、一般的な転移学習(入れ替えた層だけ学習)の二つを試す\n","\n","PyTorchでは、Alexnet、VGG、ResNet、SqueezeNet、Inception v3などの代表的なネットワークが利用できる。\n","\n","ここでは、ImageNetで学習した1000クラスの分類モデルを用いて、ウルトラマンの中でも、ウルトラマンと帰ってきたウルトラマンほどではないが、それなりに難しい分類を行う。"]},{"cell_type":"code","metadata":{"id":"tHIYP6Jfem_g"},"source":["cuda = \"cuda:0\"\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.autograd import Variable\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import os\n","import time\n","import copy\n","import numpy as np\n","import matplotlib.pyplot as plt\n","device = torch.device(cuda if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eyfzgrSHbgbw"},"source":["今回分類するのは、ウルトラマンタロウとウルトラマンレオである。\n","\n","ウルトラマンタロウとウルトラマンレオを分類するモデルを学習するための**少数の**画像を入手する\n","\n","知らない人は目視でも見分けるのが難しいといわれるが、角に注目すると、横に伸びるのがレオ、縦に曲がってとがっているのがタロウ、特に太郎は真ん中の角も目立つ。その他、胸のパターンが違うなど、よく見るとかなり違う。\n","\n"]},{"cell_type":"code","metadata":{"id":"ZZcYc6Flem_h"},"source":["if not os.path.exists('ultra.tar.gz'):\n","  #!wget \"https://drive.google.com/uc?export=download&id=1Oo-YhK2FTuqKMAAWkjwcVFjSJL9sXB6p\" -O ultra.tar.gz\n","  !wget https://keio.box.com/shared/static/smgue95s7l3b5z1augf34p3ecqrm0qqw -O ultra.tar.gz\n","  !tar xzf ultra.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"37pA2-uYem_i"},"source":["画像認識では、ネットワークの前段の方で、人間のニューロンが備えるようなある固定の形状に特異的に反応するニューロンクラスタが生成されているのではないかという仮説があり、このことの実験的解析・証明も行われた\n","\n","- 画像からタロウとレオを分類するモデルを学習する\n","- レオは237枚、タロウはどちらかというと人気が高いので集まりやすく289枚ある。なお、手作業で仕訳けているので多少のミスが混入している可能性がある。\n","\n","フルスクラッチで学習するには不十分な枚数であるが、転移学習であれば十分可能な枚数である\n","\n","今回の目標は、\n","- 学習済みImageNetを転移学習する\n","- ImageNet自体も蟻とハチの分類が可能であるが、転移学習により、その分類精度を向上させる\n","  - 今回はタロウとレオしか最後に判定しない(他が判定できないようにする)\n","\n","なお、PyTorchのチュートリアルには、ハチと蟻の分類があり、これに関して多くの参考例が存在する\n","- この場合は、オリジナルの1000のクラスにハチと蟻が既に含まれているため、若干議論が必要になる\n","- 本来は1000のクラスに**含まれない**データを扱うべき\n","- 含まれているが、精度が向上すればよいという考えは次の疑問に対して答える必要がある\n","  - 1000のクラスがありました\n","  - ハチと蟻はそのまま正解するでしょう\n","  - ハチと蟻しかないのであるから、ハチと蟻以外の残り998クラスに分類された絵に対して、各クラスの画像がハチと蟻、どちらに近いかだけ与えてしまえば、精度は向上するというクイックハックが想定できる\n","  - それよりも向上した、ということが示されなければ、**転移学習のネットワーク組み換えにより本当によくなったといえないのではないか？**\n","\n","今回はウルトラマンなので、このような疑問を挟む余地はない\n"]},{"cell_type":"markdown","metadata":{"id":"MrXW2CvEem_i"},"source":["## 展開フォルダから画像をロード\n","\n","PyTorchのImageFolderを用いてデータをロードする\n","- 画像データはPIL形式で読み込む必要がある\n","  - 画像フォルダからデータをPIL形式で読み込むにはtorchvision.datasets.ImageFolderを利用する\n","  - 既に習得済みのtransform機能がImageFolderにも存在し、データ拡張を行う変換関数群を指定できる\n","  - クラス名のサブフォルダ(train/ants, train/bees)を作成しておくと自動的にクラス(ants, bees)を割り付けることができる\n","    - ラベルはフォルダの順番に0, 1, 2, ...と割り当てられる\n","\n","この関数は全画像データをまとめてメモリにロードするわけではないため、大量かつ巨大な画像ファイルがあっても問題ない\n","\n","まずは、試しにデータをよみだし、データ数を確認する"]},{"cell_type":"code","metadata":{"id":"LtKBM47Mem_j"},"source":["image_dataset = datasets.ImageFolder(root=\"ultra\")\n","image, label = image_dataset[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WccNz6N1em_j"},"source":["一番最初の画像と自動的に振られたラベル(ハチは0)を確認する\n","\n","**ウルトラマンが嫌いな人には本当に申し訳ない**\n","- 正解しているのか不正解なのかの判別ができないかもしれない。"]},{"cell_type":"code","metadata":{"id":"PtZGB5Lmem_j"},"source":["plt.figure()\n","plt.axis(\"off\")\n","plt.imshow(image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-O6EL5iAem_k"},"source":["## データ拡張\n","\n","PyTorchにはさまざまなデータ拡張機能があるが、今回以下の機能を利用する\n","- 既に紹介済みの機能も改めて紹介する"]},{"cell_type":"markdown","metadata":{"id":"HM5bVrPSem_k"},"source":["### RandomResizedCrop\n","\n","PIL画像をランダムなサイズとアスペクト比にクロップする\n","- 実行ボタンを押すと、毎回異なる部位が現れる"]},{"cell_type":"code","metadata":{"id":"RnGoRaTYem_k"},"source":["t = transforms.RandomResizedCrop(224)\n","trans_image = t(image)\n","plt.figure()\n","plt.axis(\"off\")\n","plt.imshow(trans_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bH-IeZ68em_l"},"source":["### RandomHorizontalFlip\n","\n","与えられたPIL画像を0.5の確率でランダムに水平反転させる"]},{"cell_type":"code","metadata":{"id":"QJAuwVyHem_l"},"source":["t = transforms.RandomHorizontalFlip()\n","trans_image = t(image)\n","plt.figure()\n","plt.axis(\"off\")\n","plt.imshow(trans_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"93Smzm2fem_l"},"source":["### Resize\n","\n","PIL画像を指定されたサイズにリサイズする"]},{"cell_type":"code","metadata":{"id":"hfSV3jKfem_l"},"source":["t = transforms.Resize((int(torch.rand(1).item()*200+50), int(torch.rand(1).item()*200+50)))\n","trans_image = t(image)\n","trans_image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3cUpoIZAem_l"},"source":["### CenterCrop\n","\n","PIL画像を中央でトリミングする\n","- 何度も押して動作を確認すると良い"]},{"cell_type":"code","metadata":{"id":"ir14sgnNem_m"},"source":["t = transforms.CenterCrop(int(torch.rand(1).item()*200+50))\n","trans_image = t(image)\n","trans_image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wClrIVfpem_n"},"source":["## データ変換関数の作成\n","以上を全部用いたデータ変換関数を準備する\n","- 既に述べたが、**データをロードする度に変換される**\n","\n","- 訓練時と評価時ではデータ変換関数を変更している\n","  - 訓練時は汎化性能が上がるように RandomResizedCrop や RandomHorizontalFlip などデータ拡張する変換を用いる\n","  - 評価時はランダム性は入れずに入力画像のサイズがネットワークに合うようにサイズを変形する\n","  - ImageNetでは大きめ(256x256)にリサイズした後、中心部分の224x224を切り出すという手法を用いているため、それに従う\n","- Normalize()でImageNetの訓練データの平均と分散を用いて入力画像の画素値を平均0、分散1に正規化する\n","  - ImageNetにおいて、転移学習など、学習済のパラメタを使うときは、この変換を施す\n","  - 新たに加える画像の画素分布になるべく引っ張られないようにする\n","\n","変換関数などについて、trainとevalで参照できるようにしている\n","- `data_transforms['train']()`:訓練画像用変換関数\n","- `data_transforms['val']()`: 評価画像用変換関数\n","  - ダウンロードしたデータセットが'train'と'val'という2つのフォルダに画像を保存しているため、これに倣っている\n","\n","image_datasets: 画像データセット\n","- フォルダから画像をImageFolderで読み込んで画像データセットを作成する\n","- ImageFolderの第2引数にデータ変換用の関数を指定する\n"]},{"cell_type":"markdown","metadata":{"id":"oX5EFT8wem_n"},"source":["## ハイパーパラメタ\n","\n","ここでは、バッチサイズとエポック数だけ指定する\n","\n","簡単に変更できるので試してみると良い"]},{"cell_type":"code","metadata":{"id":"bItBkR3bem_n"},"source":["batch_size = 64\n","num_epochs = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"atq-KT1Lem_n"},"source":["data_transforms = {\n","  'train': transforms.Compose([\n","    transforms.RandomResizedCrop(224),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","  ]),\n","  'val': transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","  ]),\n","}\n","\n","image_datasets = {x: datasets.ImageFolder(\"ultra\", data_transforms[x])\n","                  for x in ['train', 'val']}\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x],\n","                  batch_size=batch_size, shuffle=True, num_workers=4)\n","                  for x in ['train', 'val']}\n","dataset_sizes = {x: len(image_datasets[x])\n","                  for x in ['train', 'val']}\n","class_names = image_datasets['train'].classes\n","acc_list = {x: [] for x in ['train', 'val']}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"86aTEj7aem_n"},"source":["## 訓練画像の可視化\n","\n","- データ変換においてToTensor()でPyTorchのテンソル形式に変換されている\n","  - これまでもそうであったが、このままでは描画できない\n","  - そのでテンソルをnumpy()でndarrayに変換しなおしてから描画する\n","- 画素値を正則化しているため、逆演算(標準偏差を積算し平均を加算)し元に戻す\n","\n","既におなじみと思われるが、`images.size()`として、バッチサイズ、チャネル(RGB)、画素x、画素y、さらにクラスのサイズが表示される\n","- ここでは、バッチサイズ数の画像が並ぶ\n","- また、次の'classes.size()として、バッチサイズ分のラベル値が表示される\n"]},{"cell_type":"code","metadata":{"id":"LmPShkrjem_n"},"source":["def imshow(images, title=None, size=10):\n","  images = images.numpy().transpose((1, 2, 0))\n","  mean = np.array([0.485, 0.456, 0.406])\n","  std = np.array([0.229, 0.224, 0.225])\n","  images = std * images + mean\n","  images = np.clip(images, 0, 1)\n","  plt.figure(figsize=(size,size))\n","  plt.imshow(images)\n","  plt.axis(\"off\")\n","#  if title is not None:\n","#    plt.title(title)\n","images, classes = next(iter(dataloaders['train']))\n","print(images.size(), classes.size())\n","images = torchvision.utils.make_grid(images)\n","imshow(images, title=[class_names[x] for x in classes])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YuetF3ztem_o"},"source":["一応確認すると、画像の分類に対応する1次元テンソルが表示される"]},{"cell_type":"code","metadata":{"id":"XurHxma5em_o"},"source":["classes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2no_dAgQem_o"},"source":["## 訓練用関数定義\n","\n","- 各エポックは訓練`trainとバリデーションデータに対する評価`val`を行う\n","  - 一般にそれぞれ別に記述するが、共通部分が多いためまとめた記述となっている\n","- PyTorchのloss関数はデフォルト(size_average=True)では、ミニバッチのサンプルあたりの平均lossを返す\n","  - 実際その値を用いて逆伝播が計算される\n","  - running_loss はミニバッチの平均lossをミニバッチのサンプル数倍し、これを全部足し集めた後、全サンプル数で割ることでサンプル当たりの平均ロスとしている\n","    - これまで通り、ミニバッチ毎や、エポック毎といった評価でも問題ない\n","\n","これまでとそれほど変わらないが、バリデーションデータでよい精度のモデルができるたびに自動的そのモデルを保存し、最終的に最高精度をたたき出すモデルを返す\n"]},{"cell_type":"code","metadata":{"id":"A58KddGBem_p"},"source":["def train_model(model, criterion, optimizer, scheduler, num_epochs=num_epochs):\n","  since = time.time()\n","  epoch_loss = 0.0\n","  epoch_acc = 0.0\n","  best_model_wts = copy.deepcopy(model.state_dict())\n","  best_acc = 0.0\n","  for epoch in range(num_epochs):\n","    print('Epoch {}/{}'.format(epoch, num_epochs - 1))  \n","    # 各エポックで訓練+バリデーションを実行\n","    for phase in ['train', 'val']:\n","      if phase == 'train':\n","        scheduler.step()\n","        model.train()\n","      else:\n","        model.eval()\n","      running_loss = 0.0\n","      running_corrects = 0\n","      for data in dataloaders[phase]:\n","        inputs, labels = data\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","        # 順伝播\n","        if phase == 'train':\n","          outputs = model(inputs)\n","        else:\n","          with torch.no_grad():\n","            outputs = model(inputs)\n","        _, preds = torch.max(outputs.data, 1) #_は無視、データを捨てる\n","        loss = criterion(outputs, labels)\n","        if phase == 'train':\n","          loss.backward()\n","          optimizer.step()\n","        running_loss += loss.data.item() * inputs.size(0)\n","        running_corrects += torch.sum(preds == labels.data)\n","      epoch_loss = running_loss/dataset_sizes[phase]\n","      epoch_acc = running_corrects.item()/dataset_sizes[phase]\n","      print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","      # よい精度のモデルを自動的に保存する、よくあるテクニック\n","      acc_list[phase].append(epoch_acc)\n","      if phase == 'val' and epoch_acc > best_acc:\n","        best_acc = epoch_acc\n","        best_model_wts = copy.deepcopy(model.state_dict())\n","  time_elapsed = time.time() - since\n","  print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed / 60, time_elapsed % 60))\n","  print('Best val acc: {:.4f}'.format(best_acc))\n","  model.load_state_dict(best_model_wts) #もっともよいモデルを読み直す\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kLI18Tevem_p"},"source":["## 学習済みモデルの読み込みとFine-tuning\n","\n","学習済みの大規模なネットワークとしてResNetを選択する\n","- 出力層部分のみ2クラス分類になるように置き換えて、重みを固定せずに新規データで全層を再チューニングする方針を選択する\n","\n","学習済みのResNet18をロードする"]},{"cell_type":"code","metadata":{"id":"aReD8k7-em_p"},"source":["model_ft = models.resnet18(pretrained=True)\n","model_ft"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wTfvjdBMem_q"},"source":["本格的なResNetの構造を見ると流石に巨大である\n","\n","最終層の`fc`は出力がImageNetの1000クラス分類であるため、1000の出力がある\n","  - この部分を2クラスに置き換えればアリとハチの分類に利用できる\n","\n","次のようにして置き換える\n","- 置き換え方は直観的にわかる通り、モデルのメソッドfcを指定しなおすだけ\n","\n","ネットワークを表示させて、最後が置き換わっていることを確認する"]},{"cell_type":"code","metadata":{"id":"fbLCtMTlem_q"},"source":["num_features = model_ft.fc.in_features\n","# fc層を置き換える\n","model_ft.fc = nn.Linear(num_features, 2)\n","model_ft"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VI4EIOdMem_q"},"source":["model_ft = model_ft.to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n","\n","# 7エポックごとに学習率を0.1倍する\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n","model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=num_epochs)\n","torch.save(model_ft.state_dict(), 'model_ft.pkl')\n","acc_list_ft = acc_list['val'].copy()\n","acc_list['val'] = []"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RMQ-DLv5em_q"},"source":["## 学習済パラメタを固定\n","\n","先ほどは重みを固定せずにResNet18の全レイヤの重みを更新対象にしていた\n","- その結果をメモしておこう\n","\n","ここでは、全体をFine-tuningせず、最終層だけ重みを調整する\n","\n","次のようにする\n","- `require_grad = False` として重みを固定する\n","- `optimizer`に更新対象のパラメータのみ渡す\n","  - `model_conv.parameters()`といった具合に固定パラメータを含めるとエラーとなる\n","\n","先ほどよりも速く学習が進む\n","- GPUの威力で、それなりに速く求めてしまう……\n","- backwardの勾配計算を最終段のみ計算すればよいため\n","- 但し、lossを計算する必要があることから、forwardはすべて計算する"]},{"cell_type":"code","metadata":{"id":"Tmrr5ASdem_q"},"source":["# 訓練済みResNet18をロード\n","model_conv = torchvision.models.resnet18(pretrained=True)\n","# 全パラメータを固定\n","for param in model_conv.parameters():\n","  param.requires_grad = False\n","# 最後のfc層を置き換える(requires_grad=Trueでありパラメータ更新の対象)\n","num_features = model_conv.fc.in_features\n","model_conv.fc = nn.Linear(num_features, 2)\n","model_conv = model_conv.to(device)\n","criterion = nn.CrossEntropyLoss()\n","# Optimizerの第1引数には更新対象のfc層のパラメータのみ指定\n","optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7n2F18hHem_q"},"source":["model_conv = train_model(model_conv, criterion, optimizer_conv,\n","                       exp_lr_scheduler, num_epochs=num_epochs)\n","acc_list_tr = acc_list['val'].copy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sk4PcTefem_r"},"source":["先程よりもよくなったであろうか？\n","\n","- バッチサイズを変える\n","- 最適化手法を変える\n","\n","などして、どちらがよくなるか、調べてみると良いであろう\n","\n","なお、今回は10エポック程度でも十分であったといえるが、このように学習速度も速い"]},{"cell_type":"code","metadata":{"id":"zLLTQ609em_r"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","plt.figure()\n","plt.plot(range(num_epochs), acc_list_ft, label='FT Loss')\n","plt.plot(range(num_epochs), acc_list_tr, label='TR Loss')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.grid()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UnAM4DJXem_r"},"source":["## 分類結果の可視化"]},{"cell_type":"code","metadata":{"id":"CR7wIyfTem_r"},"source":["model_ft.load_state_dict(torch.load('model_ft.pkl', map_location=lambda storage, loc: storage))\n","def visualize_model(model, num_images=6):\n","  images_so_far = 0\n","  fig = plt.figure()\n","  model.eval()\n","  for i, data in enumerate(dataloaders['val']):\n","    inputs, labels = data\n","    inputs = inputs.cuda().to(device)\n","    inputs.requires_grad = False\n","    labels = labels.cuda().to(device)\n","    labels.requires_grad = False\n","    with torch.no_grad():\n","      outputs = model(inputs)\n","    _, preds = torch.max(outputs.data, 1)\n","    for j in range(inputs.size()[0]):\n","      images_so_far += 1\n","      plt.subplot(num_images // 2, 2, images_so_far)\n","      plt.axis('off')\n","      plt.title('predicted: {}'.format(class_names[preds[j]]))\n","      imshow(inputs.cpu().data[j], size=4)\n","      plt.show()\n","      if images_so_far == num_images:\n","        return\n","visualize_model(model_ft)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W8FnNt3oem_r"},"source":["学習をGPUマシン行い、可視化や分析をCPUで行うことはよくある\n","\n","最初にCPU用とGPU用、両方のモデルを保存するという方法があるが、上記のようにmap_locationを記述することで変更できる\n","\n","その他の変更ルールは次を参考のこと\n","\n","- CPU - > CPUやGPU - > GPU\\\n","`torch.load('p.pth'）`\n","\n","- CPU - > GPU1\\\n","`torch.load('p.pth', map_location = lambda storage, loc: storage.cuda(1))`\n","\n","- GPU1 - > GPU0\\\n","`torch.load('p.pth', map_location = {'CUDA:1':'CUDA:0'})`\n","\n","- GPU - > CPU\\\n","`torch.load('p.pth', map_location = lambda storage, loc storage)`"]},{"cell_type":"markdown","metadata":{"id":"B8I8lP1cem_r"},"source":["新しいレイヤを作成して代入するだけなのですごく簡単にできる\n","- 結果的に2クラス分類である\n"]},{"cell_type":"markdown","metadata":{"id":"wuyMjsJ5eJ0T"},"source":["# 演習課題\n","\n","次のどちらかの課題に取り組みなさい\n","\n","- 2クラス分類であるから、出力層のユニット数を1にして活性化関数をsigmoid、Loss関数をbinary cross entropyにしてもよいだろうか、実際に実装して確認しなさい\n","\n","- イノシシと黒豚、月とスッポン、鳶と鷹など、似ていて非なるモノを用いて同様に分類しなさい\n","  - データセットは自分で作成すること\n","  - 判別率が65%を超えれば何でもい\n","  - 一般に似て非なるものを扱うこと"]},{"cell_type":"markdown","metadata":{"id":"VENXB91VZYP8"},"source":["# GAN(Generative Adversarial Networks )\n","\n","GANが使われるシーンは、例えば、\n","- 誰かに似せた**自然な**絵を自動で描かせたい\n","- よくある**偽物とわかりにくい**フェイク画像や動画、音声などを作らせたい\n","- **あたかもそこにありそうなもの**をないところから追加したい\n","- 逆にあって邪魔なものを**あたかもなかったかのように自然に**消し去りたい\n","\n","といった用途が思い当たる\n","\n","簡単に共通するのは、\n","- 人間が判断して違和感がなく自然であること\n","- 自由に生成できること\n","\n","であろう\n","\n","この魔法を実現するような内容から、AI関連で話題をさらうのは主にGAN応用であることもうなづける\n","\n","GANでは2つのモデルを競合させるように学習させる**巧妙**な手法である\n","\n","2つのモデルは、紙幣の偽造者(counterfeiter)と、それを見抜く警察(police)によく例えられるが、実際にはGeneratorとDiscriminatorと呼ばれる\n","- Generatorの目的は、本物に似せた画像(偽札)を作って警察を欺くことである\n","- Discriminatorの目的は、本物の画像か偽物の画像か(真の紙幣と偽の紙幣)の集合から本物(真)を区別することである\n","\n","この対立する目的を持つモデル同士を競わせる過程をAdversarial(敵対的) Processと呼ぶ\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/ganfig.png\" width=300>\n","\n","Generatorは一様分布や正規分布などからサンプリングしたノイズベクトル$z$をもとに、アップサンプリングして画像を生成する\n","- この辺りが何を言っているか？というのが最初はつかみにくいかもしれない\n","- やりたいことは、やりたいことは、先に述べた「自由に生成できること」であるため、この「自由に」を表現するには乱数がどうしても必要となる\n","  - つまり、乱数から何かが作れたら、乱数だから毎回違うものができ、これが「自由な生成」ということになる\n","\n","をアップサンプリングして画像とする．\n","\n","一方，Discriminatorは単純な分類問題を解くネットワークでGeneratorが生成した画像と本物の画像を分類する\n","\n","この２つのネットワークを交互に学習すれば、Generatorは本物のデータに近いデータを生成するようになる\n","\n","特に、Generatorの入力が乱数で、かつ、DiscriminatorはReal画像と、Generator画像(偽画像)を交互(もしくはランダムに)受け付けて、出力はその真偽のみという、入出力が極めてシンプルかつ謎めいた構成を持つ\n","\n","もちろん、これだけではうまく動作しない\n"]},{"cell_type":"markdown","metadata":{"id":"KhHWfJg3FkOA"},"source":["## DiscriminatorとGeneratorの目的\n","\n","Descriminatorの目的は明瞭で、予測値と実際の値が一致すればよく、より具体的にはデータを適切に分類する決定境界を見つけることが目標である\n","\n","しかしながら、Generatorは、乱数だけ入力され、それに対して勝手に生成した謎なデータを出力し、その出力が妥当なデータでなければならない\n","- つまり、Generatorは**正解となる出力値がない**状況で、妥当な出力を出す必要がある\n","- これでは、学習は進まず、何か目的・目標が必要となる\n","\n","Generatorは、**データに近いモデル分布を見つける**ことを目標とする\n","\n","- すなわち、Generatorは「今観測できているデータは、なんらかの確率分布に基づいて生成されている」という仮定に基づき、データを生成する確率分布そのものをモデル化しようと試みることであるといえる\n"]},{"cell_type":"markdown","metadata":{"id":"NL9VEVUyGzMG"},"source":["## GANの定式化\n","\n","定式化にあたり、先ほどの図における入力としてのDataset、乱数、判定結果を次のように定める\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/gan1.svg\" width=300>\n","\n","このように定めると、Datasetとしての入力の確率変数$x$に対して、Real dataとしてDiscriminatorに入力される$p_d(x)$と、Fake dataとして入力されるデータ分布と$p_g(x)$で与えられるモデル分布の2つの確率分布の「距離」を近づけることを目的とするといえる\n","\n","もちろんであるが、Generatorは明確に$x$の入力を持っておらず、その分布は明示的に与えられていない\n","- よって、例えば生成結果とデータ分布との尤度を直接計算するなどして、生成結果とデータ分布との近さを測るなどということはできない\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZrrpvX8_bAe1"},"source":["### Descriminatorの定式化\n","\n","では、実際にデータ分布$p_d(x)$を求めるとはどういうことかをみてみよう\n","\n","GANでは直接尤度を測る代わりに、データ分布とモデル分布の密度比$r(x)$を考える\n","$$\n","r(x) = \\frac{p_d(x)}{p_g(x)}\n","$$\n","\n","ここで、データ分布あるいはモデル分布から生成されたラベル付きのデータ集合$\\{(x1,y1),⋯,(xN,yN)\\}$を考え、データ分布により生成されたデータのラベルをy=1、モデル分布により生成されたデータのラベルをy=0とすると、それぞれの分布は次のように表される\n","\n","$$\n","p_d(x) = p(x|y = 1)\\\\\n","p_g(x) = p(x|y = 0)\n","$$\n","\n","この時、密度比$r(x)$はベイズの式により次のように表すことができる\n","\n","$$\n","\\begin{align}\n","r(x) &= \\frac{p(x|y = 1)}{p(x|y = 0)}\\\\\n","&= \\frac{p(y = 1| x)p(x)}{p(y=1)}\\cdot\\frac{p(y=0)}{p(y=0|x)p(x)}\n","\\end{align}\n","$$\n","\n","ここで、$\\pi = p(y=1)$とすると、\n","$$\n","\\begin{align}\n","r(x) &= \\frac{p(y = 1|x)}{p(y = 0| x)}\\cdot\\frac{1-\\pi}{\\pi}\n","\\end{align}\n","$$\n","\n","となる\n","\n","$\\pi$は実際のデータ数の比で近似できる\n","\n","ラベルはy=0かy=1のみであるため、$p(y=1∣x)$を推定できれば、密度比$r(x)$が求まる\n","- そこで、$p(y=1∣x)$を近似する分布を、例えばNNを用いて求めることを考えて、パラメータ$\\varphi$を用いて$q_\\varphi(y=1∣x)$とする\n","\n","これを式で書くと\n","\n","$$\n","p(y = 1| x) \\approx q_\\varphi(y = 1| x)\n","$$\n","となる\n","\n","$q_\\varphi(y = 1| x)$を見出すモデルをDescriminatorと呼び、$D(\\phi; x)$と表す\n","\n","本来密度比を考える問題が、分類問題と同じ確率的分類器の最適化問題に置き換わった\n","\n","この最適化に用いる誤差関数$U(D)$は、例えば、交差エントロピーを想定すれば、\n","$$\n","U(D) = -E_{p(x,y)}[y \\ln D(\\phi; x) + (1-y) \\ln (1-D(\\phi; x))]\n","$$\n","として平均を考えればよい\n","\n","これを変形すると、\n","$$\n","\\begin{align}\n","U(D) &= -E_{p(x,y)}[y \\ln D(\\phi; x) + (1-y) \\ln (1-D(\\phi; x))]\\\\\n","&= -E_{p(x|y)p(y)}[y \\ln D(\\phi; x) + (1-y) \\ln (1-D(\\phi; x))]\\\\\n","&= -E_{p(x|y=1)p(y=1)}[\\ln D(\\phi; x)] + E_{p(x|y=0)p(y=0})[ \\ln (1-D(\\phi; x))]\\\\\n","&= \\pi \\cdot E_{p_d(x)}[\\ln D(\\phi;x)]+(1-\\pi)\\cdot E_{p_g(x)}[\\ln (1-D(\\phi;x))]\n","\\end{align}\n","$$\n","\n","となる\n","\n","各ラベルのデータを与えるとき、データが丁度同数づつ混ざっていれば、y=0およびy=1となるラベルのデータ数が等しい場合$\\pi = \\frac{1}{2}$となることから、Descriminatorの目的関数$V(D)$は、\n","\n","$$\n","V(D) = E_{p_d(x)}[\\ln D(\\phi;x)]+E_{p_g(x)}[\\ln (1-D(\\phi;x))]\n","$$\n","\n","となり、これが最大となるように訓練することになる"]},{"cell_type":"markdown","metadata":{"id":"WqIHPKw4_jX-"},"source":["### Generatorの定式化\n","\n","潜在変数$z$を仮定すると、\n","\n","$$\n","p_g(x) = \\int{p(x|z)p(z)}dz\n","$$\n","\n","となる\n","\n","先ほどと同様に、$p(x|z)$を近似する分布として$q_\\theta(x|z)$を導入すると\n","\n","$$\n","p(x|z) \\approx q_\\theta(x|z)\n","$$\n","\n","となり、この$q_\\theta(x|z)$を推定するモデルとGeneratorと呼び$G(\\theta;z)$と表す\n","\n","さて、Generatorの目的関数を求めるにあたり、Descriminator $D(\\varphi;x)$について最適なDescriminator $D^*(\\varphi;x)$が得られたとすると、\n","\n","$$\n","V(D^*, G) = E_{p_d(x)}[\\ln D^*(x)]+E_{p(z)}[\\ln (1-D^*(G(\\theta;z))]\n","$$\n","\n","となる\n","\n","ここで、Generatorは、この$V(D^*, G)$を最小化することが目的であることに注意する\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oGm-CpZxK45T"},"source":["### GAN全体の定式化\n","\n","さて、実際に用いる目的関数は次の通りとなる\n","\n","Discriminatorは、Generator$G(\\theta;z)$を固定したうえで、\n","$$\n","\\mathop{\\rm max}\\limits_{\\phi}\n","E_{p_d(x)}[\\ln D(\\phi;x)]+E_{p(z)}[\\ln (1-D(\\phi;G(\\theta;z))]\n","$$\n","を計算する\n","\n","Generatorは、Descriminator$D(\\phi;x)$を固定したうえで、\n","$$\n","\\mathop{\\rm min}\\limits_{\\theta}\n","E_{p(z)}[\\ln (1-D(\\phi;G(\\theta;z))]\n","$$\n","\n","を計算する\n","\n","DとGは包含関係にあり、このことがいわゆる2つのネットワークを互いに競わせるように学習するという意味である\n","\n","より簡潔には、\n","$$\n","\\mathop{\\rm min}\\limits_{G}\\mathop{\\rm max}\\limits_{D} V(D, G)\n","$$\n","と表すことができ、学習が進むと、生成器$G(\\theta;z)$が生成するデータは、実際のデータに近くなる\n"]},{"cell_type":"markdown","metadata":{"id":"II-vUZE2Bh7T"},"source":["## GANの評価指標\n","\n","GANは教師なし学習であり、教師あり学習で用いられるAccuracy, F1 scoreといった評価指標がない。妥協案として、次の2つがしばしば利用される。"]},{"cell_type":"markdown","metadata":{"id":"FWA_MR5MU-tz"},"source":["### Frechet Inception Distance(FID)\n","\n","生成された画像の分布と元の画像の分布がどれだけ近いかを測る指標があればよいが、この近さをどのように表現するかが問題となる。そこで、人間を超える画像認識精度をもつようになった機械学習モデルを用い、画像を低次元の潜在空間で表現し、その空間で距離を測るというコンセプト。\n","\n","- 実際には、Inception V3と呼ばれるモデルで低次元な潜在空間表現、ここではPoolingの出力を用いてWasserstein-2距離を算出して利用する。次の式で求める。なお、m,cは埋め込み空間上での平均ベクトルおよび共分散行列である。添字wは生成画像を意味し、何もついていないものは実画像を意味する。距離を表すため、値は小さいほど実画像に近いことを意味し、Generator性能がより優れていることを示す。\n","$$\n","||m-m_w||^2_2+T_r(C+C_w-2(CC_2)^{1/2})\n","$$"]},{"cell_type":"markdown","metadata":{"id":"o_ZMtMCPVA5l"},"source":["## Perceptual Path Length(PPL)\n","\n","人間の感覚、つまり視覚・知覚的に潜在空間上で画像が滑らかに変化するかを表す指標です。FIDと同様に学習済みモデルにおける潜在空間での距離を利用する。\n","\n","- 画像を生成する種となる潜在空間上で、画像の変化は『知覚的』に短距離で変化しているか」を表す指標。\n","  - モーフィングのようにずれることなくダイレクトかつまっすぐに変化すれば小さな値をとるため、潜在空間がどれだけ適切に構築されているかを評価できる。\n","\n","- 例えば画像認識モデルであるVGGを使用し、その上での特徴量ベクトルの距離を用いる。解析的に求めることができないため、多くの画像を用意し、実際に距離を求めて平均値を算出することでPPLを得る。\n","\n","- この値が小さいほど潜在空間が知覚的に滑らかであることを意味する。"]},{"cell_type":"markdown","metadata":{"id":"ZsS1Nm5SaxvE"},"source":["## GANの実際\n","\n","MNISTを用いて、MNISTっぽいデータを生み出すGANを構成する\n","\n","今回は正確にはGeneratorとDiscriminatorに畳み込みニューラルネットを利用しており、**DCGAN(Deep Convolutional Generative Adversarial Networks)**と呼ばれる\n","\n","今回の実装におけるDiscriminatorとGeneratorは次の通りである\n","\n","**Discriminator**\n","\n","一般的な畳み込みニューラルネットワークを利用している\n","- ただし、MaxPoolingを使わずにstride=2として画像サイズを半分にする\n","\n","**Generator**\n","入力$z$は62次元の乱数ベクトル、これをシードとして画像を生成する\n","\n","最初に全結合網を用いてサイズを拡大する\n","- 6272次元まで拡張し7x7x128のテンソルに変換\n","- ConvTranspose2Dでチャネルを減らしつつ、画像サイズをMNISTの28x28ピクセルまで拡張する\n","- 出力は1チャンネル28x28ピクセルの画像となる\n","\n"]},{"cell_type":"code","metadata":{"id":"XFx9UiDIZsFI"},"source":["import os\n","import pickle\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image\n","import matplotlib.pyplot as plt\n","device = torch.device(cuda if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jvDCbNymUIkB"},"source":["ハイパーパラメータは次の通り"]},{"cell_type":"code","metadata":{"id":"_jO3eCJ-cIbN"},"source":["# hyperparameters\n","batch_size = 128\n","lr = 0.0002\n","z_dim = 62\n","num_epochs = 25\n","sample_num = 16\n","log_dir = './logs'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MXFu4OjAccVH"},"source":["MNISTデータの読み込みとDataLoaderの設定\n","\n","今回は、訓練用とテスト用に分ける必要はない"]},{"cell_type":"code","metadata":{"id":"C5W-2yYwcLkd"},"source":["transform = transforms.ToTensor()\n","dataset = datasets.MNIST('data/mnist', train=True, download=True, transform=transform)\n","data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F2zy8KR0UNQd"},"source":["GeneratorとDiscriminatorの定義"]},{"cell_type":"code","metadata":{"id":"XCD_udZqcEGk"},"source":["class Generator(nn.Module):\n","  def __init__(self):\n","    super(Generator, self).__init__()\n","    self.fc = nn.Sequential(\n","      nn.Linear(62, 1024),\n","      nn.BatchNorm1d(1024),\n","      nn.ReLU(),\n","      nn.Linear(1024, 128 * 7 * 7),\n","      nn.BatchNorm1d(128 * 7 * 7),\n","      nn.ReLU(),\n","    )\n","    self.deconv = nn.Sequential(\n","      nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n","      nn.BatchNorm2d(64),\n","      nn.ReLU(),\n","      nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n","      nn.Sigmoid(),\n","    )\n","    initialize_weights(self)\n","  def forward(self, input):\n","    x = self.fc(input)\n","    x = x.view(-1, 128, 7, 7)\n","    x = self.deconv(x)\n","    return x\n","class Discriminator(nn.Module):\n","  def __init__(self):\n","    super(Discriminator, self).__init__()\n","    self.conv = nn.Sequential(\n","      nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),\n","      nn.LeakyReLU(0.2),\n","      nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n","      nn.BatchNorm2d(128),\n","      nn.LeakyReLU(0.2),\n","    )\n","    self.fc = nn.Sequential(\n","      nn.Linear(128 * 7 * 7, 1024),\n","      nn.BatchNorm1d(1024),\n","      nn.LeakyReLU(0.2),\n","      nn.Linear(1024, 1),\n","      nn.Sigmoid(),\n","    )\n","    initialize_weights(self)\n","  def forward(self, input):\n","    x = self.conv(input)\n","    x = x.view(-1, 128 * 7 * 7)\n","    x = self.fc(x)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KVnABtknUXCO"},"source":["Discriminatorのウェイトの初期化について、\n","\n","まず、PyTorchのパラメーター初期化について簡単に説明する\n","\n","これまでは、Noneとしてゼロ初期化のみ議論してきた\n","\n","パラメータ(重み)は`nn.Linear()`などとしたときに設計され(インスタンス化した時に実態ができる)、その値はインスタンス化した後weightメソッドを使うことでを見ることができる\n","```\n","linear = nn.Linear(5, 2)\n","linear.weight\n","```\n","また、逆にweightに対してnn.init の中のメソッドを適用すれば初期化できる\n","\n","この初期化においては、次のような分布や定数の利用が想定でけいるので、例を示す\n","\n","- 正規分布:`normal_(weight, mean, std)`\n","```\n","linear = nn.Linear(5, 2)\n","nn.init.normal_(linear.weight, 0.0, 1.0)\n","```\n","- 一様分布:`uniform_(weight, a, b)`\n","```\n","linear = nn.Linear(5, 2)\n","nn.init.normal_(linear.weight, 0.0, 1.0)\n","```\n","\n","- 定数:`constant_(weight, c)`\n","```\n","linear = nn.Linear(5, 2)\n","nn.init.constant_(linear.weight, 1.0)\n","```\n","\n","- Xavierの初期値:`xavier_normal_(weight, gain=1)`\n","```\n","linear = nn.Linear(5, 2)\n","nn.init.xavier_normal_(linear.weight)\n","```\n","\n","- Heの初期値\n","```\n","kaiming_normal_(weight, a=0, mode='fan_in', nonlinearity='leaky_relu')\n","linear = nn.Linear(5, 2)\n","nn.init.kaiming_normal_(linear.weight)\n","```\n","\n","ここでは、全て正規分布で初期化しており、専用の関数(メソッド)を定義している\n","- Discriminatorクラスから呼び出される"]},{"cell_type":"code","metadata":{"id":"z6zIAKB7cF6q"},"source":["def initialize_weights(model):\n","  for m in model.modules():\n","    if isinstance(m, nn.Conv2d):\n","      m.weight.data.normal_(0, 0.02)\n","      m.bias.data.zero_()\n","    elif isinstance(m, nn.ConvTranspose2d):\n","      m.weight.data.normal_(0, 0.02)\n","      m.bias.data.zero_()\n","    elif isinstance(m, nn.Linear):\n","      m.weight.data.normal_(0, 0.02)\n","      m.bias.data.zero_()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ejT17b9rbcpL"},"source":["ネットワークのインスタンス化とオプティマイザの指定\n","- 今回はAdamで、ハイパーパラメタは微妙にチューニングしている\n","- また、ロス関数は真偽のみを議論するためバイナリクロスエントロピーを用いる"]},{"cell_type":"code","metadata":{"id":"C1LSNCaQcJ2z"},"source":["G = Generator().to(device)\n","D = Discriminator().to(device)\n","G_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n","D_optimizer = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n","criterion = nn.BCELoss()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8V69LC1Ug9mb"},"source":["実際の処理内容\n","\n","コードを見ると実際何をしているかがよくわかる"]},{"cell_type":"code","metadata":{"id":"2t1TTYFScOeh"},"source":["def train(D, G, criterion, D_optimizer, G_optimizer, data_loader):\n","  # 訓練モードへ\n","  D.train()\n","  G.train()\n","  # 本物ラベルは1\n","  y_real = Variable(torch.ones(batch_size, 1))\n","  # 偽物ラベルは0\n","  y_fake = Variable(torch.zeros(batch_size, 1))\n","  y_real = y_real.to(device)\n","  y_fake = y_fake.to(device)\n","  D_running_loss = 0\n","  G_running_loss = 0\n","  for batch_idx, (real_images, _) in enumerate(data_loader):\n","    # 一番最後のデータがバッチサイズに満たない場合は無視してエラーを避ける\n","    if real_images.size()[0] != batch_size:\n","      break\n","    # 潜在変数としての入力(変な言い方だが)を乱数で初期化\n","    z = torch.rand((batch_size, z_dim))\n","    real_images, z = real_images.to(device), z.to(device)\n","    # Discriminatorの勾配の初期化\n","    D_optimizer.zero_grad()\n","    # Discriminatorは実画像データを1=Trueと認識するほどよい\n","    D_real = D(real_images)\n","    D_real_loss = criterion(D_real, y_real)\n","    # DiscriminatorはGeneratorが生成した偽画像を0=Falseと認識するほどよい\n","    # fake_imagesをDiscriminatorが学習しないようにdetach()する\n","    fake_images = G(z)\n","    D_fake = D(fake_images.detach())\n","    D_fake_loss = criterion(D_fake, y_fake)\n","    # 2つのlossの和を最小化する\n","    D_loss = D_real_loss + D_fake_loss\n","    D_loss.backward()\n","    D_optimizer.step()  # Dだけ更新(Gのパラメータは更新しない)\n","    D_running_loss += D_loss.data.item()\n","    # Generatorの更新\n","    G_optimizer.zero_grad()\n","    # GeneratorにとってGeneratorが生成した画像の認識結果は1（本物）に近いほどよい\n","    fake_images = G(z)\n","    D_fake = D(fake_images)\n","    G_loss = criterion(D_fake, y_real)\n","    G_loss.backward()\n","    G_optimizer.step()\n","    G_running_loss += G_loss.data.item()\n","  D_running_loss /= len(data_loader)\n","  G_running_loss /= len(data_loader)\n","  return D_running_loss, G_running_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cRlmjcdVhPwf"},"source":["学習途中で、お試しに画像を保存するため、適当な$z$から画像を生成させる"]},{"cell_type":"code","metadata":{"id":"jBWd7wk7cQNs"},"source":["def generate(epoch, G, log_dir='logs'):\n","  G.eval()\n","  if not os.path.exists(log_dir):\n","    os.makedirs(log_dir)\n","  # 生成のもとになる乱数を生成\n","  sample_z = torch.rand((64, z_dim))\n","  sample_z = sample_z.to(device)\n","  # Generatorでサンプル生成\n","  samples = G(sample_z).data.cpu()\n","  save_image(samples, os.path.join(log_dir, 'epoch_%03d.png' % (epoch)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ipsoLldwh7Hj"},"source":["実際にGANを学習させる\n","- 10分程度必要"]},{"cell_type":"code","metadata":{"id":"CbCuUP3ocRwO"},"source":["history = {}\n","history['D_loss'] = []\n","history['G_loss'] = []\n","for epoch in range(num_epochs):\n","  D_loss, G_loss = train(D, G, criterion, D_optimizer, G_optimizer, data_loader)\n","  print('epoch %d, D_loss: %.4f G_loss: %.4f' % (epoch + 1, D_loss, G_loss))\n","  history['D_loss'].append(D_loss)\n","  history['G_loss'].append(G_loss)    \n","  # 特定のエポックでGeneratorから画像を生成してモデルも保存\n","  if (epoch+1)%5 == 0:\n","    generate(epoch + 1, G, log_dir)\n","    torch.save(G.state_dict(), os.path.join(log_dir, 'G_%03d.pth' % (epoch + 1)))\n","    torch.save(D.state_dict(), os.path.join(log_dir, 'D_%03d.pth' % (epoch + 1)))\n","# 学習履歴を保存\n","with open(os.path.join(log_dir, 'history.pkl'), 'wb') as f:\n","  pickle.dump(history, f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hCu4ethDg7od"},"source":["DiscriminatorとGeneratorのロス曲線は次の通り\n","\n","まだまだサチュレーションには至っておらず、より正確な画像を生成でそうである\n","\n","Discriminatorのロスが減少し、Generatorのロスが増大していることから、設計通りである\n","\n","また、学習の初期段階で不安定になることも知られており、最初ダメだからと言ってあきらめない方が良い\n","- このあたりがGANの難しいところ\n"]},{"cell_type":"code","metadata":{"id":"qYmL8SMEcTV3"},"source":["with open(os.path.join(log_dir, 'history.pkl'), 'rb') as f:\n","    history = pickle.load(f)\n","D_loss, G_loss = history['D_loss'], history['G_loss']\n","plt.plot(D_loss, label='D_loss')\n","plt.plot(G_loss, label='G_loss')\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.legend()\n","plt.grid()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wjQwsXnvr6ft"},"source":["徐々にきれいなMNISTを生成するようになっているのがわかるだろう"]},{"cell_type":"code","metadata":{"id":"mJcGt6zpjXx2"},"source":["from IPython.display import Image, display_png\n","for i in list([5, 10, 15, 20, 25]):\n","    fname = 'logs/epoch_{:03}.png'.format(i)\n","    print(20*'-')\n","    print(fname)\n","    display_png(Image(fname))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2-GkmxF87-r2"},"source":["余力がある人は、さらに計算パワーが必要なCelebAを試してみると良い"]},{"cell_type":"markdown","metadata":{"id":"LvbqksL8kfWv"},"source":["様々なGANが存在し、最も基本な形は、適当なベクトルから学習した画像を生成する構成である\n","\n","以下、その代表例をしめす\n","\n","- pix2pix  \n","画像の対応のペアを入力とし、それがペアとして正しいかどうかを判定して汎用的な画像の変換を行うGAN\n","  - 通常のGANはベクトルを入力に一枚の画像を生成、判定するが、pix2pixは画像を入力とし、また条件として出力画像を作成し、入力画像と出力画像のペアについて真贋を判定する\n","  - 画像を条件に画像を生成するためConditionalGANの一種である\n","\n","- pix2pixHD  \n","2048×1024の高解像画像を生成するGAN\n","  - Generatorを2段構造とし、1024×512を生成してから、解像度上げる別のGeneratorを用いる\n","\n","- CycleGAN  \n","例えばりんごとオレンジの画像など2つの画像変換するように学習したGAN\n","  - 学習時はこの場合りんごとオレンジの画像群のみあればよい\n","    - りんごと同じ姿勢、似たような形のオレンジの画像がひつようとなるわけではない\n","  - ネットワークがループ構造を持ち、オレンジの画像からりんごの画像を生成し、そのりんごの画像を再度オレンジ画像に戻したときの精度が高くなるように学習させる\n","  - 普通の馬がシマウマになったりできる\n","\n","- StarGAN  \n","CycleGANは2つの変換専用に構成するが、複数に対応する\n","  - 複数のドメイン間を学習するようにAとBのCycleGAN、BとCのCycleGANとせず、1つのGANで複数のGAN間を往来できるようにする\n","\n","- DCGAN  \n","  - CNNで構成するGAN\n","  - 既に役割は終えている\n","\n","- PGGAN(Progressive Growing of GANs)  \n","  - 高解像度の学習を段階的に行うように学習過程を構成することで、1024×1024という高解像度画像を生成する\n","\n","- ACGAN  \n","Discriminatorが真偽判定だけでなく、クラス判定もするように学習させる\n","  - 効果として、DCGANよりも綺麗な画像生成が可能となる\n","\n","- SAGAN  \n","Spectrum NormalizationやSelf Attention機構を導入することで現時点で最も高画質な画像生成を行う\n","  - 局所的な情報だけでなくSelf Attentionで大域的な視野をもって生成する\n","\n","- ConditionalGAN  \n","生成して欲しい画像のラベルも同時に入力に与えることで、指定した画像を生成する条件付き画像生成手法\n","  - ラベルも画像化するため、one-hotで正解は完全に白の画像、不正解は完全に黒の画像で構成し、チャネル数を拡張する\n","\n","- InfoGAN  \n","相互情報量を評価関数に導入し、画像の何らかの特徴と意味の取れた関係を持つように学習する\n","  - Gの入力およびDの出力に潜在空間表現を与え、Dはその潜在空間が何かも判定する\n","  - この潜在空間には何か意味を与えるなどしなくとも、例えば、回転に対応した要素や線の太さに対応した要素などが現れる\n","\n","- StackGAN  \n","pix2pixHDと同様、StackGANは2段のGANで構成されており、文章から画像を生成する段と、その画像を高精度にする段で構成される\n","  - 高精度に文章から画像を生成できる\n","\n","- AnoGAN  \n","GANはサンプルデータから生成モデルを学習するが、本物の正常入力画像は、それを生成するGeneratorの入力$z$は存在する、もしくは既知の値になるが、その周辺の$z$を使って画像生成し、その生成画像と入力画像との差があった部分を異常個所として判定する\n","  - 異常個所を色分けして表示できる\n","\n","- WGAN  \n","  - GANの損失関数にJSダイバージェンスを使わず、Wasserstein距離とすることで、学習を高速化及び安定させる\n","  - こちらが既に主流"]},{"cell_type":"markdown","metadata":{"id":"woxTTB70FSbW"},"source":["## StyleGAN\n","\n","様々あるGANの中でも著名なモデルが2018年に発表されたStyleGANである\n","- 下図はすべてStyleGANで自動的に生成された顔画像であり、実際にはその人物は存在しない\n","- StlyGANを用いた版権や肖像権のない人物画像によるポスターやwebページ制作が普通に行われている\n","- StyleGANよりもさらに高精細なStyleGAN2が存在する(後述)\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/styleganex.png\" width=500>\n","\n","StyleGANのネットワークの特徴\n","- Progressive Growingを用いた高解像画像生成\n","- AdaINを用いて各層に画像のStyleを取り込む\n","\n","これらの特徴について説明し、StyleGANの詳細構造に移る"]},{"cell_type":"markdown","metadata":{"id":"Q3e6MzRIVsuk"},"source":["### Progresive Growing\n","\n","Progressive-Growing GANで提案された高解像度画像の生成手法\n","- 低解像画像の生成から始めて徐々に高解像用のGenerator,Discriminatorを追加することで、最終的に高解像度画像を生成する手法\n","\n","下図で、最初に4x4の画像生成から始め、徐々に解像度を上げて最終的には1024x1024の高解像度画像を生成する\n","- 解像度を上げるネットワークを追加しても、低解像画像を生成するGと判別するDはパラメータを固定せずに学習させ続ける\n","- 学習過程でネットワークを修正させるためDefine-by-Runではないか？ということになるが、Define-by-Runは計算グラフ（NNのネットワーク）構築をデータを流しながら行うことを意味するためそうではない\n","  - データを流し終わってから、つまりエポック単位でネットワークを切り替えるため正確にはDefine-by-Runではない\n","  - PyTorchでなくとも、学習させたパラメータを保存、ネットワークを再定義し、学習させたパラメータを再度必要な個所に読み込むなどすれば、同様のモデルを構築できる\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/ProgressiveGrowing.png\" width=500>\n"]},{"cell_type":"markdown","metadata":{"id":"awo3XMarVzeu"},"source":["### AdaIN\n","\n","スタイル変換のための正規化手法の一つ\n","- 元々は、コンテンツ入力とスタイル入力について、平均と分散を用いて正規化する手法\n","- AdaINによるスタイル変換の例を下図に示す\n","  - ビル群がコンテンツ画像、下の絵画がスタイル入力\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/AdaIN.png\" width=500>\n","\n","AdaINは、Instance Normalizationなどの正規化手法と異なり、スタイルとコンテンツ画像の統計量(標準偏差と平均値)のみで正規化を行い、学習パラメータを使用しない\n","- よって、訓練データとして利用していないスタイルを用いた場合でもスタイル変換が可能\n","\n","$$\n","AdaIN(x,y) = \\sigma(y)\\frac{x-\\mu(x)}{\\sigma(x)}+\\mu(y)\n","$$\n","\n","StyleGANでは、同様に学習パラメータを利用しない\n","- AdaINのオリジナルと似た式を用いるが、標準偏差と平均値の代わりに、スタイルベクトルWに線形変換を加えた$y_s$と$y_b$という値を利用\n","\n","$$\n","AdaIN(\\bf x_i,y) = y_{s,i}\\frac{x_i-\\mu(x_i)}{\\sigma(x_i)}+y_{b,i}\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"ieEFX12XV2h9"},"source":["### Mixing Regularization\n","\n","StyleGANは学習中にStyleに用いられる潜在変数を2つ混ぜるという正則化手法を利用\n","- 例えば、潜在変数$z_1$と$z_2$から得られる$w_1$と$w_2$のスタイルベクトルについて、$w_1$を4x4の画像生成に、$w_2$を8x8の画像生成に用いることができる\n","- 結果、2つの画像のStyleをうまく混ぜることができる\n","- 図のような合成が可能となる\n","\n","図はSource AとSourceBの画像それぞれを生成する潜在変数をAとBを準備し、最初はAを使い、ある解像度からBを使った場合の結果を表す\n","- 切り替える解像度を低解像(4² ~ 8²)・中解像(16² ~ 32²)・高解像(64~1024²)の3通りとしている\n","- なお、低解像から入れたStyleの影響が大きくなる\n","- また、低解像からBの潜在変数を使うと顔の形や肌の色、性別年齢などがBに近く、高解像で使うと背景や髪の色などしか影響を与えない\n","- Mixing Regularizationは学習過程での話であり、学習済みモデルを用いたモーフィングとは異なることに注意する\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/MixingRegularization.png\" width=500>\n"]},{"cell_type":"markdown","metadata":{"id":"dXKsSRlAPXnu"},"source":["## StyleGAN2\n","\n","StyleGANの改良版\n","- AdaINの代わりにCNNのWeightを正規化して用いる\n","  - StyleGANにあったゴミであるdropletの除去(画像にゴミが乗るのを防ぐ)\n","- Progressive Growingをやめることで自然なモードの改善\n","  - 顔が横を向いても歯並びが前を向いてしまうといったトラブルを避ける\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/stylegan1-t.png\" width=600>\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/stylegan2-t.png\" width=600>\n","\n","- 潜在空間で連続性を持たせることで画像品質向上している\n","- StyleGANに比べてFID等が大きく向上する。\n","\n","Progressive Growingをやめる代わりに、GeneratorとDiscriminatorの表現力を上げることで高解像度生成ができるようにしている\n","- もともと、個々のGeneratorが独立しているため頻出する特徴を生成する傾向にあり、その結果顔が動いても歯が追随していかないという結果が生まれていた\n","\n","下図はすべてStyleGAN2で生成された画像である。なお、それぞれは異なる画像データより作成された異なるモデルであることに注意する。\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/StyleGAN2-1.png\" width=600>\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/StyleGAN2-2.png\" width=600>\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/StyleGAN2-3.png\" width=600>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"24PdJPnRX3o0"},"source":["# pix2pix"]},{"cell_type":"markdown","metadata":{"id":"2vNM8QP6Ssvd"},"source":["## pix2pixを用いた画像生成\n","\n","ここでは、pix2pixを用いた白黒画像のカラーを行う\n","\n","その学習は次の通りであり、GANであるためGeneratorとDiscriminatorの2つのネットワークを利用する\n","\n","- あるカラー画像$p$に対して、グレーススケール変換$g$により$g(p)$を生成\n","- グレースケール画像からカラー画像を生成するGeneratorを$G$とする。つまりFake画像は$G(g(p))$として与えられる。\n","  - $G(g(p))$と$p$を例えばMSEなどで評価しても$G$を生成できるが、このようにして生成したGは、$p$にしか適用できない変換となる\n","    - ここで構成したいのは未知の絵に対してもカラー画像にすることができる$G$\n","- Discriminatorを$D$とすると、$D$は一般的な2値ラベルの分類器で、$G(g(p))$と元画像の2つについて、真贋すなわち、$G(g(p))$か$p$かを判別する。\n","  - 従って、$D$は、普通に$G(g(p))$や$p$を入力してそれぞれのラベルを学習させる、一般的な学習を行う\n","- $G$の目標は、とにかく自分の画像、つまり$G(g(p))$を選んでもらう確率を上げること\n","  - なお一般的には、$G(g(p))$と$p$は$1:1$で混入する\n","  - 乱数など用いず交互に導入することが多い\n","\n","このようにして、GとDの両方を学習させていく\n"]},{"cell_type":"markdown","metadata":{"id":"teGKWCBEYhRY"},"source":["## U-net\n","ここで、Generatorとしてセマンティックセグメンテーションにも用いられるU-netを利用する\n","\n","U-netは、AutoEncoderと同様、Encoder-Decoderを行うネットワークであり、エンコードの結果、今回は潜在空間$z$を256次元としている\n","\n","さらに、U-netの特徴は、Image Transferと同様の考え方になるが、画像の特徴を細部から全体特徴について各層がそれぞれ保持していると考えられることから、EncoderとDecoderをおおよそ同一の構造とし、そのパラメータを再利すると、Encoderにより抽出された元画像がもつ情報を効率的にDecoderに提供できる\n","\n","ここで、入力画像はグレースケール、出力画像はカラーであるため、完全に同一ではない\n","\n","要するに、パラメータを対象となる層でも利用しやすくする\n","  - これにはいくつ可能方法があるが、まず最初に行うのが、Encoderにおいて、一つ前の層の情報と、Encoder側の対象となる層の情報の2つの情報を混ぜるという作業であり、これには単純に入力数を増やして2つの入力を用いるという工夫がなされる\n","  - この時、この例では完全に同じ情報を利用しているが、何かしら別の層を介してもよい\n","  - これでは過学習になりそうだが、Batch Normalizationを用いてこれを回避している\n","  - 2つの入力を結合するには、torch.catを用いる\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Wax7FVvQcHYc"},"source":["## Patch GAN\n","\n","Descriminatorは、Generatorよりも小さな画像を入力する\n","\n","- 1枚の$G(g(p))$について真贋を得るとすると、それなりに大規模なネットワークが必要となる。また、真贋の鑑定に画像全体を見る必要性はあまりなく、部分的にみて不都合なところを見つけ出せばよい\n","- そこで、$G(g(p))$や$p$を分割し、その小さな画像について、それぞれで真贋を判定する\n","  - これをpatch GANと呼ぶ\n","- GANのDiscremenatorでは一般的であるがLeakey ReLUを用いる\n","- さらにInstance Normalizationを利用する\n","  - Bach Normalizationは、ミニバッチの各演算間で分布が揃うように値を移動するという処理を施す\n","  - 各イテレーションの平均の平均、各イテレーションでの分散の平均を求め、これをもとに分布の移動と拡大を行う\n","インスタンスノームは1つのデータの1つのチャネルに対して正規化する方法で、GANではしばしば用いられる\n","  - 特にInstance Normalizationでなければならないかは実験が必要\n","\n"]},{"cell_type":"code","metadata":{"id":"ws3MhnGaHVtz"},"source":["import os\n","import glob\n","import pickle\n","import torch\n","import torch.nn.functional as F\n","import torchvision\n","import torch.utils.data as data\n","import torchvision.transforms as transforms\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from torch import nn\n","from skimage import io\n","import datetime\n","\n","usedevice = \"cuda\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jMDBVE1LggoF"},"source":["データを読み込むが相当にサイズが大きく、ダウンロードとデータの展開だけで2分程度必要である\n","\n","ファイルサイズが大きいと、Google Driveは直リンクダウンロードを拒否するので、pythonでゴリ押しする"]},{"cell_type":"code","metadata":{"id":"UaIt7tJzxLuC"},"source":["import requests\n","\n","def download_file_from_google_drive(id, destination):\n","    URL = \"https://docs.google.com/uc?export=download\"\n","\n","    session = requests.Session()\n","\n","    response = session.get(URL, params = { 'id' : id }, stream = True)\n","    token = get_confirm_token(response)\n","\n","    if token:\n","        params = { 'id' : id, 'confirm' : token }\n","        response = session.get(URL, params = params, stream = True)\n","\n","    save_response_content(response, destination)    \n","\n","def get_confirm_token(response):\n","    for key, value in response.cookies.items():\n","        if key.startswith('download_warning'):\n","            return value\n","\n","    return None\n","\n","def save_response_content(response, destination):\n","    CHUNK_SIZE = 32768\n","\n","    with open(destination, \"wb\") as f:\n","        for chunk in response.iter_content(CHUNK_SIZE):\n","            if chunk: # filter out keep-alive new chunks\n","                f.write(chunk)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zoZJrKEea4qS"},"source":["import os\n","if not os.path.exists('cocog2c.tgz'):\n","  #file_id = '1OhWZKb1EcIMWTKJb8h6iRoTDx9mIQLFX'\n","  #destination = 'cocog2c.tgz'\n","  #download_file_from_google_drive(file_id, destination)\n","  !wget https://keio.box.com/shared/static/7ogtorp8uhgjjbtltyfpamkgp6kwb1si -O cocog2c.tgz\n","  !tar xzf colog2c.tgz\n","trainfiledir = \"coco/train/*\"\n","testfiledir = \"coco/test/*\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mie6bUWnyrR-"},"source":["サイズを確認する\n","- `-rw-r--r-- 1 root root 1627290651 MON DAY HH:MM cocog2c.tgz` となるはずである"]},{"cell_type":"code","metadata":{"id":"LI2RXCOyfuqS"},"source":["!ls -laF cocog2c.tgz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YESNeUsFgct7"},"source":["Generatorは[batch_size, 3, 128, 128]で、\n","チャネル数はRGBの3、128$\\times$128の画像となる\n","\n","Discriminatorは[batch_size, 1, 4, 4]で、4$\\times$4の各場所について真贋を判定し、チャネル数は1の出力となる\n","\n"]},{"cell_type":"code","metadata":{"id":"DhLPHbJ4LIgj"},"source":["class Generator(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2)\n","    self.bn1 = nn.BatchNorm2d(32)\n","\n","    self.av2 = nn.AvgPool2d(kernel_size=4)\n","    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n","    self.bn2 = nn.BatchNorm2d(64)\n","\n","    self.av3 = nn.AvgPool2d(kernel_size=2)\n","    self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n","    self.bn3 = nn.BatchNorm2d(128)\n","\n","    self.av4 = nn.AvgPool2d(kernel_size=2)\n","    self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n","    self.bn4 = nn.BatchNorm2d(256)\n","\n","    self.av5 = nn.AvgPool2d(kernel_size=2)\n","    self.conv5 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","    self.bn5 = nn.BatchNorm2d(256)\n","\n","    self.un6 = nn.UpsamplingNearest2d(scale_factor=2)\n","    self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","    self.bn6 = nn.BatchNorm2d(256)\n","\n","    #conv7にはconv6の出力とconv4の出力を流す, input channelが2倍\n","    self.un7 = nn.UpsamplingNearest2d(scale_factor=2)\n","    self.conv7 = nn.Conv2d(256 * 2, 128, kernel_size=3, stride=1, padding=1)\n","    self.bn7 = nn.BatchNorm2d(128)\n","\n","    #conv8にはconv7の出力とconv3の出力を流す, input channelが2倍\n","    self.un8 = nn.UpsamplingNearest2d(scale_factor=2)\n","    self.conv8 = nn.Conv2d(128 * 2, 64, kernel_size=3, stride=1, padding=1)\n","    self.bn8 = nn.BatchNorm2d(64)\n","\n","    #conv9にはconv8の出力とconv2の出力を流す, input channelが2倍\n","    self.un9 = nn.UpsamplingNearest2d(scale_factor=4)\n","    self.conv9 = nn.Conv2d(64 * 2, 32, kernel_size=3, stride=1, padding=1)\n","    self.bn9 = nn.BatchNorm2d(32)\n","\n","    self.conv10 = nn.Conv2d(32 * 2, 3, kernel_size=5, stride=1, padding=2)\n","    self.tanh = nn.Tanh()\n","\n","  def forward(self, x):\n","    #x1-x4はtorch.catする必要があるので,残しておく\n","    x1 = F.relu(self.bn1(self.conv1(x)), inplace=True)\n","    x2 = F.relu(self.bn2(self.conv2(self.av2(x1))), inplace=True)\n","    x3 = F.relu(self.bn3(self.conv3(self.av3(x2))), inplace=True)\n","    x4 = F.relu(self.bn4(self.conv4(self.av4(x3))), inplace=True)\n","    x = F.relu(self.bn5(self.conv5(self.av5(x4))), inplace=True)\n","    x = F.relu(self.bn6(self.conv6(self.un6(x))), inplace=True)\n","    x = torch.cat([x, x4], dim=1)\n","    x = F.relu(self.bn7(self.conv7(self.un7(x))), inplace=True)\n","    x = torch.cat([x, x3], dim=1)\n","    x = F.relu(self.bn8(self.conv8(self.un8(x))), inplace=True)\n","    x = torch.cat([x, x2], dim=1)\n","    x = F.relu(self.bn9(self.conv9(self.un9(x))), inplace=True)\n","    x = torch.cat([x, x1], dim=1)\n","    x = self.tanh(self.conv10(x))\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gtk9Irc0LOAy"},"source":["class Discriminator(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2)\n","    self.in1 = nn.InstanceNorm2d(16)\n","\n","    self.av2 = nn.AvgPool2d(kernel_size=2)\n","    self.conv2_1 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n","    self.in2_1 = nn.InstanceNorm2d(32)\n","    self.conv2_2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n","    self.in2_2 = nn.InstanceNorm2d(32)\n","\n","    self.av3 = nn.AvgPool2d(kernel_size=2)\n","    self.conv3_1 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n","    self.in3_1 = nn.InstanceNorm2d(64)\n","    self.conv3_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n","    self.in3_2 = nn.InstanceNorm2d(64)\n","\n","    self.av4 = nn.AvgPool2d(kernel_size=2)\n","    self.conv4_1 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n","    self.in4_1 = nn.InstanceNorm2d(128)\n","    self.conv4_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n","    self.in4_2 = nn.InstanceNorm2d(128)\n","\n","    self.av5 = nn.AvgPool2d(kernel_size=2)\n","    self.conv5_1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n","    self.in5_1 = nn.InstanceNorm2d(256)\n","    self.conv5_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","    self.in5_2 = nn.InstanceNorm2d(256)\n","\n","    self.av6 = nn.AvgPool2d(kernel_size=2)\n","    self.conv6 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n","    self.in6 = nn.InstanceNorm2d(512)\n","\n","    self.conv7 = nn.Conv2d(512, 1, kernel_size=1)\n","\n","  def forward(self, x):      \n","    x = F.leaky_relu(self.in1(self.conv1(x)), 0.2, inplace=True)\n","    x = F.leaky_relu(self.in2_1(self.conv2_1(self.av2(x))), 0.2, inplace=True)\n","    x = F.leaky_relu(self.in2_2(self.conv2_2(x)), 0.2, inplace=True)\n","    x = F.leaky_relu(self.in3_1(self.conv3_1(self.av3(x))), 0.2, inplace=True)\n","    x = F.leaky_relu(self.in3_2(self.conv3_2(x)), 0.2, inplace=True)\n","    x = F.leaky_relu(self.in4_1(self.conv4_1(self.av4(x))), 0.2, inplace=True)\n","    x = F.leaky_relu(self.in4_2(self.conv4_2(x)), 0.2, inplace=True)\n","    x = F.leaky_relu(self.in5_1(self.conv5_1(self.av5(x))), 0.2, inplace=True)\n","    x = F.leaky_relu(self.in5_2(self.conv5_2(x)), 0.2, inplace=True)\n","    x = F.leaky_relu(self.in6(self.conv6(self.av6(x))), 0.2, inplace=True)\n","    x = self.conv7(x)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XhLjtf7egEME"},"source":["DataLoaderは、PyTorchのDatasetクラスを継承して機能拡張している\n","- カラー画像を取得する\n","- データ拡張を行う\n","- これをそのまま、真贋の真のデータとする\n","\n","ここには含まれていないが、併せてグレースケール変換を行う"]},{"cell_type":"code","metadata":{"id":"wYk0_XOtLUCt"},"source":["class DataAugment():\n","  # データ拡張\n","  def __init__(self, resize):\n","    self.data_transform = transforms.Compose([\n","      transforms.RandomResizedCrop(resize, scale=(0.9, 1.0)),\n","      transforms.RandomHorizontalFlip(),\n","      transforms.RandomVerticalFlip()])\n","  def __call__(self, img):\n","    return self.data_transform(img)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LL-lEFWNlHH9"},"source":["もう一つのDataLoaderは、単にデータの正規化を行う"]},{"cell_type":"code","metadata":{"id":"odQPsSgeLVa1"},"source":["class ImgTransform():\n","  # データの正規化\n","  def __init__(self, resize, mean, std):\n","    self.data_transform = transforms.Compose([\n","      transforms.Resize(resize),\n","      transforms.ToTensor(),\n","      transforms.Normalize(mean, std)])\n","  def __call__(self, img):\n","    return self.data_transform(img)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h4Alv3CNlhUJ"},"source":["グレースケール変換を行うクラス"]},{"cell_type":"code","metadata":{"id":"s1KTKB94LZT7"},"source":["class MonoColorDataset(data.Dataset):\n","  def __init__(self, file_list, transform_tensor, augment=None):\n","    self.file_list = file_list\n","    self.augment = augment     #PIL to PIL\n","    self.transform_tensor = transform_tensor  #PIL to Tensor\n","\n","  def __len__(self):\n","    return len(self.file_list)\n","\n","  def __getitem__(self, index):\n","    #index番号のファイルパスを取得\n","    img_path = self.file_list[index]\n","    img = Image.open(img_path)\n","    img = img.convert(\"RGB\")\n","    if self.augment is not None:\n","      img = self.augment(img)\n","    #モノクロ画像用のコピー\n","    img_gray = img.copy()\n","    #カラー画像をモノクロ画像に変換\n","    img_gray = transforms.functional.to_grayscale(img_gray, num_output_channels=3)\n","    #PILをtensorに変換\n","    img = self.transform_tensor(img)\n","    img_gray = self.transform_tensor(img_gray)\n","    return img, img_gray"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nqssBB_uh0jL"},"source":["テスト用のデータローダ"]},{"cell_type":"code","metadata":{"id":"t7QZ2i5FLbHO"},"source":["def load_train_dataloader(file_path, batch_size):\n","  size = (128,128)             #画像の1辺のサイズ\n","  mean = (0.5, 0.5, 0.5) #画像の正規化した際のチャンネル毎の平均値\n","  std = (0.5, 0.5, 0.5)  #画像の正規化した際のチャンネル毎の標準偏差\n","\n","  #データセット\n","  train_dataset = MonoColorDataset(file_path_train, \n","    transform_tensor=ImgTransform(size, mean, std), \n","    augment=DataAugment(size))\n","  #データローダー\n","  train_dataloader = data.DataLoader(train_dataset,\n","    batch_size=batch_size,\n","    shuffle=True)\n","  return train_dataloader"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oW2wloAgiJix"},"source":["PyTorchのtensor表現された画像をタイル状に描画する\n","- nrowでタイルの1辺の数を決定できる"]},{"cell_type":"code","metadata":{"id":"eo6EfS2d6pgg"},"source":["def mat_grid_imgs(inum, imgs, nrow, save_path = None):\n","  imgs = torchvision.utils.make_grid(\n","    imgs[0:(nrow**2), :, :, :], nrow=nrow, padding=5)\n","  imgs = imgs.numpy().transpose([1,2,0])\n","  imgs -= np.min(imgs)   #最小値を0\n","  imgs /= np.max(imgs)   #最大値を1\n","\n","  plt.imshow(imgs)\n","  plt.xticks([])\n","  plt.yticks([])\n","  plt.show()\n","\n","  if save_path is not None:\n","    io.imsave(save_path, imgs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dYchMPkmiY5n"},"source":["テスト画像をロードして、グレースケール画像とフェイク画像をタイル状に描画する"]},{"cell_type":"code","metadata":{"id":"2TyyGS2gLgYP"},"source":["def evaluate_test(file_path_test, model_G, device=usedevice, nrow=4):\n","  \"\"\"\n","  test画像をロード,gray画像とfake画像をタイル状に描画\n","  \"\"\"\n","  model_G = model_G.to(device)\n","  size = (128,128)\n","  mean = (0.5, 0.5, 0.5)\n","  std = (0.5, 0.5, 0.5)\n","  test_dataset = MonoColorDataset(file_path_test, \n","    transform_tensor=ImgTransform(size, mean, std), \n","    augment=None)\n","  test_dataloader = data.DataLoader(test_dataset,\n","     batch_size=nrow**2,\n","     shuffle=False)\n","  #データローダーごとに画像を描画\n","  for i, (img, img_gray) in enumerate(test_dataloader):\n","    mat_grid_imgs(i, img_gray, nrow=nrow)\n","    img = img.to(device)\n","    img_gray = img_gray.to(device)\n","    #img_grayからGeneratorを用いて,FakeのRGB画像\n","    img_fake = model_G(img_gray)\n","    img_fake = img_fake.to(\"cpu\")\n","    img_fake = img_fake.detach()\n","    mat_grid_imgs(i, img_fake, nrow=nrow)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zMvgtS5diepr"},"source":["学習前に、一番最初の状態を確認する\n","\n","ここでは、ノイズのかかった画像になる\n","- 通常の画像を扱うGANでは砂嵐画像から学習を開始するが、U-netの結合によりなんとなく形は残るようになる"]},{"cell_type":"code","metadata":{"id":"XE_tksfF-iRM"},"source":["g = Generator()\n","file_path_test = glob.glob(testfiledir)\n","evaluate_test(file_path_test, g)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GUKu47YGi0gN"},"source":["画像をえり好みしてグレースケールに変換する\n","\n","- 初めからグレースケールであったり、あまり色の変化がない画像は学習の役に立たないので採用しないようにしている\n","\n","- えり好みした画像を専用の場所に保存する\n","\n","この変換と保存作業はかなり重く、ここだけで4分程度必要である"]},{"cell_type":"code","metadata":{"id":"iFBQ57dcxC6L"},"source":["from skimage import io, color, transform\n","\n","def color_mono(image, threshold=150):\n","  #入力画像がカラーか否かを判別する(thresholdでカラーぐらいの閾値を与える)\n","  image_size = image.shape[0] * image.shape[1]\n","  #チャネル0と1、0と2、1と2について差分を求める\n","  diff = np.abs(np.sum(image[:,:, 0] - image[:,:, 1])) / image_size\n","  diff += np.abs(np.sum(image[:,:, 0] - image[:,:, 2])) / image_size\n","  diff += np.abs(np.sum(image[:,:, 1] - image[:,:, 2])) / image_size\n","  if diff > threshold:\n","    return \"color\"\n","  else:\n","    return \"mono\"\n","\n","def bright_check(image, ave_thres = 0.15, std_thres = 0.1):\n","  try:\n","    #白黒に変換する\n","    image = color.rgb2gray(image)\n","    #明るすぎる画像、暗すぎる画像、明るさに差がない画像を除く\n","    if image.shape[0] < 144:\n","      return False    \n","    if np.average(image) > (1.-ave_thres): #明るすぎる画像\n","      return False\n","    if np.average(image) < ave_thres: #暗すぎる画像\n","      return False\n","    if np.std(image) < std_thres: #明るさに差がない画像\n","      return False\n","    return True\n","  except:\n","    return False\n","\n","paths = glob.glob(trainfiledir)\n","numpics = 0\n","maxpics = 9990\n","for i, path in enumerate(paths):\n","  image = io.imread(path)\n","  save_name = \"./trans/mscoco_\" + str(i) +\".png\"\n","  x = image.shape[0] #xピクセル数\n","  y = image.shape[1] #yピクセル数\n","  try:\n","    #xとyの内、短い方の1/2\n","    clip_half = min(x, y)/2\n","    #画像を正方形で切り出し\n","    image = image[int(x/2 -clip_half): int(x/2 + clip_half),\n","      int(y/2 -clip_half): int(y/2 + clip_half), :]\n","    if color_mono(image) == \"color\":\n","      if bright_check(image):\n","        image = transform.resize(image, (144, 144, 3),\n","                                 anti_aliasing = True)\n","        image = np.uint8(image*255)\n","        io.imsave(save_name, image)\n","        numpics += 1\n","        if numpics > maxpics:\n","          break\n","  except:\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sAdav6vljMee"},"source":["学習をそれぞれ行う\n","\n","ロス計算において、true_labelsおよびfalse_labelsは、4$\\times$4のブロック毎に判定していることに注意する\n"]},{"cell_type":"code","metadata":{"id":"BLgVQt6VxPAq"},"source":["def train(model_G, model_D, epoch, epoch_plus):\n","  device = usedevice\n","  batch_size = 100\n","  tstart = datetime.datetime.now()\n","\n","  model_G = model_G.to(device)\n","  model_D = model_D.to(device)\n","\n","  params_G = torch.optim.Adam(model_G.parameters(),\n","    lr=0.0002, betas=(0.5, 0.999))\n","  params_D = torch.optim.Adam(model_D.parameters(),\n","    lr=0.0002, betas=(0.5, 0.999))\n","  #loss計算のためのラベル\n","  true_labels = torch.ones(batch_size, 1, 4, 4).to(device)    #True\n","  false_labels = torch.zeros(batch_size, 1, 4, 4).to(device)  #False\n","  #loss_function\n","  bce_loss = nn.BCEWithLogitsLoss()\n","  mae_loss = nn.L1Loss()\n","  log_loss_G_sum, log_loss_G_bce, log_loss_G_mae = list(), list(), list()\n","  log_loss_D = list()\n","\n","  for i in range(epoch):\n","    #ロスを記録\n","    loss_G_sum, loss_G_bce, loss_G_mae = list(), list(), list()\n","    loss_D = list()\n","\n","    train_dataloader = load_train_dataloader(file_path_train, batch_size)\n","\n","    for real_color, input_gray in train_dataloader:\n","      batch_len = len(real_color)\n","      real_color = real_color.to(device)\n","      input_gray = input_gray.to(device)\n","      #Generatorの訓練\n","      fake_color = model_G(input_gray) #偽カラー画像生成\n","      fake_color_tensor = fake_color.detach()\n","      #偽画像のロスを計算\n","      LAMBD = 100.0 # BCEとMAEの係数\n","      #fake画像を識別器に入れたときの出力\n","      out = model_D(fake_color)\n","      #Dの出力に対するロス\n","      loss_G_bce_tmp = bce_loss(out, true_labels[:batch_len])\n","      #Gの出力に対するロス\n","      loss_G_mae_tmp = LAMBD * mae_loss(fake_color, real_color)\n","      loss_G_sum_tmp = loss_G_bce_tmp + loss_G_mae_tmp\n","\n","      loss_G_bce.append(loss_G_bce_tmp.item())\n","      loss_G_mae.append(loss_G_mae_tmp.item())\n","      loss_G_sum.append(loss_G_sum_tmp.item())\n","\n","      params_D.zero_grad()\n","      params_G.zero_grad()\n","      loss_G_sum_tmp.backward()\n","      params_G.step()\n","\n","      #Discriminatorの訓練\n","      real_out = model_D(real_color)\n","      fake_out = model_D(fake_color_tensor)\n","\n","      #ロスの計算\n","      loss_D_real = bce_loss(real_out, true_labels[:batch_len])\n","      loss_D_fake = bce_loss(fake_out, false_labels[:batch_len])\n","\n","      loss_D_tmp = loss_D_real + loss_D_fake\n","      loss_D.append(loss_D_tmp.item())\n","\n","      params_D.zero_grad()\n","      params_G.zero_grad()\n","      loss_D_tmp.backward()\n","      params_D.step()\n","\n","    i = i + epoch_plus\n","    telapsed = datetime.datetime.now() - tstart\n","    print(i, \"loss_G\", np.mean(loss_G_sum), \"loss_D\", np.mean(loss_D), \" time:\", telapsed)\n","    log_loss_G_sum.append(np.mean(loss_G_sum))\n","    log_loss_G_bce.append(np.mean(loss_G_bce))\n","    log_loss_G_mae.append(np.mean(loss_G_mae))\n","    log_loss_D.append(np.mean(loss_D))\n","\n","    file_path_test = glob.glob(testfiledir)\n","    evaluate_test(file_path_test, model_G, device)\n","\n","  return model_G, model_D, [log_loss_G_sum, log_loss_G_bce, log_loss_G_mae, log_loss_D]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HsxCzFcdjjtE"},"source":["学習させつつ、実際に白黒画像をカラー化する"]},{"cell_type":"code","metadata":{"id":"k9_46WX8xUN_"},"source":["file_path_train = glob.glob(testfiledir)\n","model_G = Generator()\n","model_D = Discriminator()\n","model_G, model_D, logs = train(model_G, model_D, 40, 0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5RUXx5xExnRv"},"source":["# VAEとGAN\n","\n","VAEとGANを単純に画質で比較すれば、VAEはぼやっとした画像となり、GANはくっきりした画像になる\n","- VAEの画像には詳細が描かれない\n","\n","VAEは、潜在空間の中に連続的に画像が存在する事が損失関数の形から要請されている\n","- 結果として潜在空間の近い点同士の画像は似ていなくてはならない\n","- 2つの画像が似ているというのは、行列としての画像の同じ要素の値が近いということを意味する\n","- つまり、模様のような一つ一つの画像固有の部分を細かく学習することは、モデルに取って不利になる\n","\n","学習という面でGANには問題が沢山ある\n","- 2つのモデルを学習するときに、損失関数が学習度合の絶対値の指標にならない\n","- モデルが複雑になるため、学習コストが破綻的に必要となる\n","- これらを解決するGANの改良が次々と提案されている\n","\n","一方で、VAEも負けていない\n","- 最新の研究でVQ-VAE2が提案され、GAN最強といわれるBigGANに劣らない性能をより少ないコストの計算で達成するとしており、BigGANを超えたとさえも言われている"]},{"cell_type":"markdown","metadata":{"id":"SkoIrdqvfEH0"},"source":["# 課題 C\n","\n","Pix2pixによるカラー化において、自分で準備、もしくは作成した白黒画像を実際にカラー化しなさい\n","- 本内容で得られたコード、学習結果を利用してよい\n","- 解像度は問わない"]}]}