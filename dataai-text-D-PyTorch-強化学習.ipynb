{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dataai-text-D-PyTorch-強化学習.ipynb","private_outputs":true,"provenance":[{"file_id":"1quqOzh8F3nQygNWiSXkd61FDzMwtGRID","timestamp":1609282296170}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"gU-Arpc9zElH"},"source":["---\n",">「ムダが必要だと思うんだ。いろいろ練習して、自分の体に合ったものを見つける」\\\n",">中野 浩一\n","---"]},{"cell_type":"markdown","metadata":{"id":"UJP_ip5qtk1p"},"source":["# 強化学習\n","\n","強化学習はReinforcement Learningの訳であり試行錯誤しながら最適な制御を実現する機械学習手法のひとつ\n","- 強化学習の概念は古く、1950年代に自立制御として存在、その後深層強化学習と呼ばれるディープラーニングを応用した手法が登場した\n","- 強化学習は教師あり学習に似ているが、明確な答えは提示されず、行動の選択肢と、その行動に対する報酬が与えられる\n","\n","強化学習の流れは次の通りである\n","- **エージェント(Agent)**がある**環境(Environment)**に置かれ、その環境に対して**行動(Action)**を起こす\n","  - エージェントはある条件で行動する主体であり、環境はエージェントを取り巻く周辺要素でエージェントに作用するものを指し、エージェントの環境に対するインタラクションを行動と呼ぶ\n","  \n","- 環境がエージェントに、行動により更新された**状態(State)**と**報酬(Reward)**をフィードバックする\n","  - 状態はエージェントが存在する仮想空間の情報で行動により更新される\n","  - エージェントの行動指針として報酬を利用する\n","    - 望ましい行動に対して正の報酬、望ましくない行動に対して負の報酬を与え、その程度を値の大きさで表現する\n","\n","- ここで、「答え=報酬」ではないことに注意する\n","  - 強化学習においては、行動と報酬は直結しておらず、行動や行動の連続により環境がどのように変化したかなど、多角的に報酬が与えられる\n","\n","- 環境からのフィードバックを元に**方策(Policy)**を修正する。\n","  - エージェントは、行動に対する状態と報酬のフィードバックを元に、将来得られる価値を最大化する方策を導き出す\n","  - 方策の導出に用いられるのが**状態価値関数**と**行動価値関数**である\n","    - 状態価値関数は、ある状態$s$において、エージェントが方策$\\pi$を実行した際に得られる価値を求める\n","    - 行動価値関数は、エージェントがランダムな行動$a$を実行した後に方策πを実行した際に得られる価値を求める\n","  - 両方ともに最終的に方策$\\pi$を実行しているが、行動価値関数はランダムな行動$a$をとるが、この行動により得られる価値が増えるならば、行動$a$を方策の一部として採用して方策を修正する\n","\n","- これまでの一連の行動の結果として変化した環境の中で、再びエージェントが環境に対して行動を起こす\n","\n","以上の、行動し、その行動による環境の変化と報酬のフィードバックをえて、その行動を取った場合と取らなかった場合の価値を比較し方策を修正するというサイクルを繰り返すことで強化学習が進む"]},{"cell_type":"markdown","metadata":{"id":"4xfwqN_9Bzex"},"source":["## 価値反復法\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kyxHmKM7OzaR"},"source":["### 定式化\n","\n","次のように定式化する\n","- 状態空間(環境がとりうる状態の集)を$\\mathcal{S}=\\{s_1,\\ldots,s_m\\}$、行動空間(エージェントがとりうる行動の集合)を$\\mathcal{A}=\\{a_1,\\ldots,a_n\\}$、施策(ポリシー)を$\\pi(s,a)=P(a_t=a|s_t=s)$、報酬関数を$r_{t+1}=r(s_t,a_t)$とする\n","- エージェントは時刻$t$の状態$s_t$において、政策$\\pi$にしたがって行動$a_t$を選択し、次の状態$s_{t+1}$に遷移して、報酬$r_{t+1}$を得るとする\n","\n","行動$a$を重ねるごとに受け取る報酬$r$について、時刻やステップなど、時間的遷移を付与して$r_t$と表現する\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2mG2eMz_O2UO"},"source":["### 報酬の表現\n","\n","その将来受け取るであろう全報酬を**報酬和**$G_t$と呼び、\n","$$\n","G_t = r_{t+1}+r_{t+2}+r_{t+3}+\\cdots\n","$$\n","となる\n","\n","ここで、今すぐもらえる報酬と、将来もらえる報酬を等価に扱うことが学習を進めるうえでよいことか？ということを考える必要がある\n","- 将来の成功はその都度うまくいったという積み重ねであり、将来を狙っても状況の変換に追従できなければ元も子もない\n","- さらに、例えば10ステップで実現できる場合と、5ステップで実現できる場合で、ステップを踏むことに負の報酬がない場合は、ステップに関わらずどちらも同じ報酬を得ることになり、早く報酬を得たことに対するメリットが得られない\n","- これらを解決手法として**時間割引**があり、具体的には**時間割引率**$\\gamma(0<\\gamma<1)$を導入して、次の**割引報酬和**を用いる\n","\n","$$\n","G_t = r_{t+1}+\\gamma r_{t+2}+\\gamma^2 r_{t+3}+\\cdots\n","$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DuxOwejlO5Lz"},"source":["### 行動価値と状態価値\n"]},{"cell_type":"markdown","metadata":{"id":"2Pu3gOgWdAZb"},"source":["\n","#### 行動価値\n","\n","時刻$t$においてある状態$s$にあり、行動$a$を選択したときに得られる報酬が$r(s,a) = r_{t+1}$とすると、行動価値関数$Q$は、$Q(s,a) = r_{t+1}$となる \n","エージェントは最もQ値の高い行動を選択することになる\n","\n","例えば、先に示した迷路を解くという問題では、Qテーブルは、S1からS9の全状態について、それぞれ4つの行動が想定されており、これを表にして期待できる報酬を記載したテーブルである。\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/Qtable.png\" width=300>\n","\n","Q学習は強化学習の一種で、各状態と行動の組み合わせにQ値を設定したテーブルを用いて学習する \n","エージェントは最もQ値の高い行動を選択し、Q-Tableの各値が最適化されることで学習が進む\n"]},{"cell_type":"markdown","metadata":{"id":"dw2P-EgGdCAo"},"source":["#### 状態価値\n","\n","状態$s$において方策$\\pi$に従って行動することで、その後将来に渡って得られることが期待される割引報酬和$G_t$を状態価値と呼ぶ\n","\n","これを一般的に表現したのがベルマンの方程式であり次のように表す\n","\n","$$\n","V^\\pi(s) = max_a\\mathbb{E}[r_{s,a}+\\gamma\\times V^\\pi(s_{(s,a)})]\n","$$\n","\n","ここで、\n","- $V^\\pi(s)$: 状態$s$での状態価値$V$であり、左辺値が最も大きくなる行動$a$を選択したときに期待される値\n","- $s_{(s,a)}$: 状態$s$で行動$a$を選択して移動した次の状態$s_{t+1}$\n","\n","であり、ベルマン方程式は、新たな状態$s_{t+1}$における状態価値$V$に1ステップ分の時間割引率を掛けた項に即時報酬$r_{s,a}$を加えた値の最大値を現在の状態価値とする式\n","- ベルマン方程式が成り立つためには学習過程がマルコフ決定過程であることが必要\n","- マルコフ決定過程とは、定常性(常にその発生確率などは一定)、独立性(過去や未来の事象の発生が現在に影響しない)、希少性(同時に複数のイベントが発生しない)を満たす過程\n"]},{"cell_type":"markdown","metadata":{"id":"IQf1uqO5SYDE"},"source":["## SarsaとTD誤差\n","\n","行動価値関数$Q$が望ましい値になるように更新学習するための一手法\n","\n","ベルマンの方程式において、行動価値関数$Q(s,a)$が正しく求まっていれば、\n","\n","$$\n","Q_(s_t, a_t) = r_{t_+1}+\\gamma Q(s_{t_1}, a_{t+1})\n","$$\n","\n","と表すことができる\n","\n","学習途中では行動価値関数が決定できていない場合は、この式の等式が成り立たず、誤差が生じる\n","- この誤差(右辺-左辺)をTD誤差(Temporal difference error)と呼び\n","$$\n","TD_{error} = r_{t+1}+\\gamma Q(s_{t_1}, a_{t+1})-Q(s_t, a_t)\n","$$\n","と表現される\n","\n","TD誤差が0になれば行動価値関数が求まったことになるが、この$Q$の更新式は学習率$\\eta$とすると次のように表すことができる\n","\n","$$\n","Q(s_t, a_t) \\leftarrow Q(s_t, a_t)+\\eta \\times (r_{t+1}+\\gamma Q(s_{t+1}, a_{t+1})-Q(s_t, a_t))\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"T8ZEPdm4ke_T"},"source":["## $\\varepsilon$-greedy法\n","\n","$Q$値が最も大きくなる行動を選択しようとする場合、より良い選択をどのように探索する手法が必要\n","\n","そこで、エピソード初期はあえてランダムに行動を選択し、エピソードを重ねるに従いQ値に基づいて選択する代表的な方法が$\\varepsilon$-greedy法\n","\n","この$\\varepsilon$-greedy法として、例えば\n","- 確率$\\varepsilon$でランダムに行動選択する\n","- 確率$1-\\varepsilon$で$Q$が最大になる行動を選択する\n","\n","などの方法がある"]},{"cell_type":"markdown","metadata":{"id":"bwfi3DgnfHVy"},"source":["## Q学習\n"]},{"cell_type":"markdown","metadata":{"id":"8rpMLNTZdKta"},"source":["### Q学習の概要\n","\n","一般に次のような形態となり、プログラム構造もこれに倣って設計される\n","\n","主な構成要素は次の3つ\n","- Agent (行動を司る実態)\n","- Environment (Agentが行動する環境)\n","- Brain (行動を決定する頭脳)\n","\n","1. 行動: Agentが環境に実際に行動する\n","1. 状態更新・報酬: 環境が変化しそれによって次の状態を伝えると共に報酬を返す\n","1. 状態通知: AgentがBrainに現在の状態・行動・次の状態・報酬等の情報を伝える\n","1. 判断: 状態通知内容をもとに行動決定の施策を練ることで、つがい的にはQテーブルの更新などが該当する\n","1. 行動指示: BrainがAgentにとるべき行動を支持する"]},{"cell_type":"markdown","metadata":{"id":"werGUiK7VBGv"},"source":["### 迷路の解法におけるQ学習\n","\n","例えば、強化学習で迷路を解くという問題を例に説明する \n","次のような迷路を想定する\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/maze1.png\" width=200> \n","\n","ここでエージェントは迷路を解く人を意味し、環境は迷路である\n","\n","- 行動：まず、迷路を解く上では、行動とはエージェント(人)が迷路を移動することに該当する\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/maze2.png\" width=200> \n","\n","この場合、いけない場所があるかもしれないが、検討する移動方向は上下左右の4つであり、この中から一つの行動を選択する\n","\n","- 状態：迷路を解く上での状態とは、迷路の中でどこにいるかを表し、行動により変化する\n","  - ここではS1からS9の状態をとる \n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/maze3.png\" width=200>\n","\n","- 報酬：例えばエージェントがゴールに到達すれば+1の報酬を手に入れ、途中の罠にはまれば-1の報酬を手に入れる\n","  - これで、罠を避けつつゴールに行くようになるであろう\n","  - 最短コースを選択するようにするのであれば、歩数を報酬に入れるなども関東される\n","  - この例では答えが報酬になってしまっているが、これは必須ではない\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/maze4.png\" width=200>\n","\n","- 方策：状態を考慮してエージェントがどのように行動するべきかを定めたルールであり、あとから出てくるQテーブルが該当する\n","  - この場合のQテーブルは次のようなイメージであり、例えばS1の状態では、右に進めば罠に近づくためマイナスの値となっており、下に進めばゴールに近づくためプラスの値を与える\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/Qtable.png\" width=300>\n"]},{"cell_type":"markdown","metadata":{"id":"Yq6eENrPaEch"},"source":["### Q学習の更新式\n","\n","Saraの更新式は\n","\n","$$\n","Q(s_t, a_t) \\leftarrow Q(s_t, a_t)+\\eta \\times (r_{t+1}+\\gamma Q(s_{t+1}, a_{t+1})-Q(s_t, a_t))\n","$$\n","\n","と与えられ、行動価値関数$Q$の更新に次の行動$a_{t+1}$を利用する\n","- これを方策オン型と呼ぶ\n","\n","Q学習の更新式は\n","\n","$$\n","Q(s_t, a_t) \\leftarrow Q(s_t, a_t)+\\eta \\times (r_{t+1}+\\gamma max_a Q(s_{t+1}, a)-Q(s_t, a_t))\n","$$\n","\n","と与えられ、状態$s_{t+1}$の行動価値関数の値のうち最も大きい場合で更新する\n","- これを方策オフ型と呼ぶ\n","- Q値の更新量 = 学習係数x(**報酬**+割引率x**次の状態で最大のQ値**-現在のQ値)である\n","  - 学習係数$\\eta$は0.1といった値が用いられる\n","- 行動の結果得られた報酬と次の状態で最大のQ値（割り引かれる）から現在のQ値を差し引くことを意味する\n","- $\\varepsilon$-greedy法をのようなランダム性が更新式に含まれないため、就職がSarsaよりも速くなる"]},{"cell_type":"markdown","metadata":{"id":"JorPB2eHpbET"},"source":["## Q学習の例\n","\n","ここでは、深層強化学習ではなく、Qテーブルのみ利用した簡単なQ学習を実装し、機械学習の仕組みについて理解する \n","- 深層ではないため、PyTorchは用いず、NumPyとmatplotlibのみ用いる \n","- 一般にQテーブルは巨大になるためQテーブルの学習にDNNを応用するが、この応用については後述する\n","\n","ここでは、飛べ！飛行船ゲームにトライする\n","- エージェントは飛行船であり、基本的には自由落下するが、ジェット噴射で上向き加速度を与えることができる\n","  - 選択できる行動は上下方向のみで、行動は2つ、自由落下行動0と、ジェット噴射行動1のみ\n","- フィールドを左から右へ等速度で運動する\n","- フィールドの下端は墜落、上端は空気が薄いため、どちらもゲームオーバー\n","\n","このゲームを訓練するが、報酬は飛行船が右端まで到達したときに+1、ゲームオーバーの時-1とする\n"]},{"cell_type":"code","metadata":{"id":"-sw4eAa1k-Ll"},"source":["cuda = \"cuda:0\"\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib import animation, rc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i8BBKCnfp6r3"},"source":["## Brainクラス\n","まず、エージェントの頭脳となるBrainクラスを設計する\n","\n","より多くの報酬が得られるようにQ-TableのQ値を調整、Q値の更新量は次式で表現可能\n","- `Q値の更新量 = 学習係数 x ( 報酬 + 割引率 x 次の状態で最大のQ値 - 現在のQ値 )  `\n","  \n","Brainクラスの`get_action`メソッドは、ある状態における行動をε-greedy法により選択\n","- 従って、学習初期はランダムに行動が選択され、学習が進むと徐々にQ値の高い行動が選択されるようになる\n"]},{"cell_type":"code","metadata":{"id":"2eLczBSCq12y"},"source":["class Brain:\n","    def __init__(self, n_state, w_y, w_vy, n_action, gamma=0.9, r=0.99, lr=0.01):\n","        self.n_state = n_state  # 状態の数\n","        self.w_y = w_y  # 位置の刻み幅\n","        self.w_vy = w_vy  # 速度の刻み幅\n","        self.n_action = n_action  # 行動の数\n","        self.eps = 1.0  # ε\n","        self.gamma = gamma  # 割引率\n","        self.r = r  # εの減衰率\n","        self.lr = lr  # 学習係数\n","        self.q_table = np.random.rand(n_state*n_state, n_action)  # Q-Tableを乱数で初期化\n","\n","    def quantize(self, state, n_state, w):  # 状態の値を整数のインデックスに変換\n","        min = - n_state / 2 * w\n","        nw = (state - min) / w\n","        nw = int(nw)\n","        nw = 0 if nw < 0 else nw\n","        nw = n_state-1 if nw >= n_state-1 else nw\n","        return nw\n","\n","    def train(self, states, next_states, action, reward, terminal):  # Q-Tableを訓練\n","        i = self.quantize(states[0], self.n_state, self.w_y)  # 位置のインデックス\n","        j = self.quantize(states[1], self.n_state, self.w_vy)  # 速度のインデックス\n","        q = self.q_table[i*self.n_state+j, action]  # 現在のQ値\n","\n","        next_i = self.quantize(next_states[0], self.n_state, self.w_y)  # 次の位置のインデックス\n","        next_j = self.quantize(next_states[1], self.n_state, self.w_vy)  # 次の速度のインデックス\n","        q_next = np.max(self.q_table[next_i*self.n_state+next_j])  # 次の状態で最大のQ値\n","\n","        if terminal:\n","            self.q_table[i*self.n_state+j, action] = q + self.lr*reward  # 終了時は報酬のみ使用\n","        else:\n","            self.q_table[i*self.n_state+j, action] = q + self.lr*(reward + self.gamma*q_next - q)  # Q値の更新式\n","\n","    def get_action(self, states):\n","        if np.random.rand() < self.eps:  # ランダムな行動\n","            action = np.random.randint(self.n_action)\n","        else:  # Q値の高い行動を選択\n","            i = self.quantize(states[0], self.n_state, self.w_y)\n","            j = self.quantize(states[1], self.n_state, self.w_vy)\n","            action = np.argmax(self.q_table[i*self.n_state+j])\n","        if self.eps > 0.1:  # εの下限\n","            self.eps *= self.r\n","        return action"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VUxvMa_eq8j_"},"source":["## エージェントのクラス\n","\n","エージェントクラスは、実際に行動し報酬を受け取るように実装\n","\n","- x座標が-1から1、y座標が-1から1の正方形の領域を考え、エージェントの初期位置は左端中央とする\n","- エージェントが右端に達した際は報酬として1を与え終了とする\n","- エージェントが上端もしくは下端に達した際は報酬として-1を与え、終了とする\n","- x軸方向には等速度で移動\n","- 行動は自由落下とジャンプの2種類\n"," - 自由落下の場合は重量加速度をy速度に加える\n"," - ジャンプの場合はy速度を予め設定した値に変更"]},{"cell_type":"code","metadata":{"id":"vODVCQGqrcxx"},"source":["class Agent:\n","    def __init__(self, v_x, v_y_sigma, v_jump, brain):\n","        self.v_x = v_x  # x速度\n","        self.v_y_sigma = v_y_sigma  # y速度、初期値の標準偏差\n","        self.v_jump = v_jump  # ジャンプ速度\n","        self.brain = brain\n","        self.reset()\n","\n","    def reset(self):\n","        self.x = -1  # 初期x座標\n","        self.y = 0  # 初期y座標\n","        self.v_y = self.v_y_sigma * np.random.randn()  # 初期y速度\n","\n","    def step(self, g):  # 時間を1つ進める g:重力加速度\n","        states = np.array([self.y, self.v_y])\n","        self.x += self.v_x\n","        self.y += self.v_y\n","\n","        reward = 0  # 報酬\n","        terminal = False  # 終了判定\n","        if self.x>1.0:\n","            reward = 1\n","            terminal = True\n","        elif self.y<-1.0 or self.y>1.0:\n","            reward = -1\n","            terminal = True\n","        reward = np.array([reward])\n","\n","        action = self.brain.get_action(states)\n","        if action == 0:\n","            self.v_y -= g   # 自由落下\n","        else:\n","            self.v_y = self.v_jump  # ジャンプ\n","        next_states = np.array([self.y, self.v_y])\n","        self.brain.train(states, next_states, action, reward, terminal)\n","\n","        if terminal:\n","            self.reset()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CiJfy0AHreqM"},"source":["## 環境のクラス\n","\n","環境をクラスとして実装\n","- 役割は重力加速度を設定し時間を前に進めることのみ"]},{"cell_type":"code","metadata":{"id":"nKcZDM98rfm9"},"source":["class Environment:\n","    def __init__(self, agent, g):\n","        self.agent = agent\n","        self.g = g  # 重力加速度\n","\n","    def step(self):\n","        self.agent.step(self.g)\n","        return (self.agent.x, self.agent.y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gzn5P5mZrhEK"},"source":["## アニメーション\n","今回は、matplotlibを使ってエージェントの飛行をアニメーションで表します。  \n","アニメーションには、matplotlib.animationのFuncAnimation関数を使用します。  "]},{"cell_type":"code","metadata":{"id":"j_GjTw5Aridm"},"source":["def animate(environment, interval, frames):\n","    fig, ax = plt.subplots()\n","    plt.close()\n","    ax.set_xlim(( -1, 1))\n","    ax.set_ylim((-1, 1))\n","    sc = ax.scatter([], [])\n","    def plot(data):\n","        x, y = environment.step()\n","        sc.set_offsets(np.array([[x, y]]))\n","        return (sc,)\n","    return animation.FuncAnimation(fig, plot, interval=interval, frames=frames, blit=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XrRY0cDArj41"},"source":["## ランダムな行動\n","\n","比較としてエージェントがランダムに行動する例を試す\n","- `r`の値を1に設定しεが減衰しないようにする\n"," - これでエージェントは完全にランダムな行動を選択する\n","\n","実行には、10分程度必要\n","- 結果を再生すると、運良く右端に到達することもあるが多くの場合ゲームオーバーとなることがわかる"]},{"cell_type":"code","metadata":{"id":"iAzf-1IcrlvT"},"source":["n_state = 50\n","w_y = 0.2\n","w_vy = 0.2\n","n_action = 2\n","brain = Brain(n_state, w_y, w_vy, n_action, r=1.0)  # εが減衰しない\n","\n","v_x = 0.05\n","v_y_sigma = 0.1\n","v_jump = 0.2\n","agent = Agent(v_x, v_y_sigma, v_jump, brain)\n","\n","g = 0.2\n","environment = Environment(agent, g)\n","\n","anim = animate(environment, 50, 1024)\n","rc(\"animation\", html=\"jshtml\")\n","anim"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G5NHaB2SrqCB"},"source":["## Q学習の導入\n","\n","`r`の値を0.99に設定しεが減衰するようにする\n","- Q学習の結果が行動に反映されるようになる"]},{"cell_type":"code","metadata":{"id":"gsOC7SgfrrZM"},"source":["n_state = 50\n","w_y = 0.2\n","w_vy = 0.2\n","n_action = 2\n","brain = Brain(n_state, w_y, w_vy, n_action, r=0.99)  # εが減衰する\n","\n","v_x = 0.05\n","v_y_sigma = 0.1\n","v_jump = 0.2\n","agent = Agent(v_x, v_y_sigma, v_jump, brain)\n","\n","g = 0.2\n","environment = Environment(agent, g)\n","\n","anim = animate(environment, 50, 1024)\n","rc(\"animation\", html=\"jshtml\")\n","anim"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_enFs-ndrs7K"},"source":["学習が進むと、上下の端にぶつらずに右端まで飛べるようになる"]},{"cell_type":"markdown","metadata":{"id":"4WktY8P62Cnr"},"source":["## 深層強化学習\n","\n","ディープラーニングは、特に複雑な特徴量入力から自律的に特徴抽出を行うことができるが、強化学習における環境からの複雑な入力から特徴を抽出し、行動を得ることで、自律的特徴抽出と自律制御に応用できる\n","\n","エージェントは環境の知識は事前に持たないため、自ら探索し情報収集することが必要であり、これが強化学習における特徴量抽出に相当する\n","\n","さらに、エージェントは環境を把握した上で長期的な価値を最大化するであろう行動を時系列と各行動の相互の影響を考慮して行動を決定し、これが強化学習における時系列データ生成に相当する\n","\n","この両方に効果的に利用できるのがディープラーニングである\n","- 入力データが画像である場合、特徴量抽出にCNNが用いられ、時系列データである倍RNNが用いられることが多いが、様々な層構造の応用が想定される"]},{"cell_type":"markdown","metadata":{"id":"eZUh7Blktobg"},"source":["# Deep Q-Network(DQN)\n","\n","- すべての状態数と行動数が把握できるのであれば、これをすべて記述してテーブルをこうせいすればよいが、一般に状態数や行動数は大きく、組み合わせの数も膨大となり計算が困難となる\n","- そこでDQNでは$Q$をテーブルではなく、関数や学習モデルで近似する\n","- 構築する深層学習モデルは、状態×行動を入力とせず、状態を入力して行動のQ値を出力するモデルとなる\n"]},{"cell_type":"markdown","metadata":{"id":"tvNp4SBMdTWi"},"source":["## Q-Netowrk\n","\n","DQNにおけるニューラルネットワークの構造をQ-Networkと呼ぶ\n","- 特に深層ニューラルネットワークを用いる場合にDQNと呼ぶ\n"]},{"cell_type":"markdown","metadata":{"id":"EyqAis74dVkm"},"source":["## TD学習とQ-learning\n","\n","ここで扱う手法はTD学習(Temporal Difference learning)と呼ばれる\n","\n","TD学習は強化学習の手法の一つで、価値ベースの手法である\n","- 次の式のように行動価値関数$Q$についてTD誤差を求め、これが0になるようにする\n","- Q-learningは行動価値関数$Q$で価値の高い行動を選択し、状態・報酬の観測を繰り返すことで、次の状態での$Q$の値と現時点での$Q$の値の間に生じるTD誤差を用い最適な行動価値関数を推定する手法\n","\n","$$\n","Q(s_t,a_t){\\leftarrow}(1-\\eta)Q(s_t,a_t)+\\eta(r_{t+1}-\\gamma\\max_{a_{t+1}}Q(s_{t+1},a_{t+1}))\n","$$\n","\n","左辺はQ値の更新値、右辺は現状態のQ値$Q(s_t,a_t)$、報酬$r_{t+1}$、行動後の状態のQ値の最大値$\\gamma\\max_{a_{t+1}}Q(s_{t+1},a_{t+1})$を意味する\n","\n","この式を変形すると、\n","\n","$$\n","Q(s_t,a_t){\\leftarrow}Q(s_t,a_t)+\\eta(r_{t+1}+\\gamma\\max_{a_{t+1}}Q(s_{t+1},a_{t+1})-Q(s_t,a_t))\n","$$\n","\n","となり、$\\eta(r_{t+1}+\\gamma\\max_{a_{t+1}}Q(s_{t+1},a_{t+1})-Q(s_t,a_t))$の部分をTD誤差と呼ぶ\n","\n","$Q(s_t,a_t)$を$r_{t+1}+\\gamma\\max_{a_{t+1}}Q(s_{t+1},a_{t+1})$に近づけるため、後者を教師信号targetとして、現在の$Q$との誤差関数$L(s,a)$を使ってネットワークを学習させる\n","\n","$$\n","target=r_{t+1}+\\gamma\\max_{a_{t+1}}Q(s_{t+1},a_{t+1})\n","$$\n","$$\n","L(s,a)=\\frac{1}{2}(target-Q(s,a))^2\n","$$"]},{"cell_type":"markdown","metadata":{"id":"dEu5bilL8rpp"},"source":["## DQNにおけるテクニック\n","\n","DLにおいても様々なテクニックが利用され、その進歩に貢献したように、DQNにおいても次のようなテクニックが提案され、利用されている\n"]},{"cell_type":"markdown","metadata":{"id":"MNUbUYoYd98x"},"source":["### **Experience Replay**\n","\n","エージェントが繰り返し行動し、得られた経験(Experience)を時系列に獲得するが、経験に相関があるとネットワークが過学習するためこれを避ける\n","- 時刻$t$と時刻$t+1$のの学習内容は類似し、類似した部分のみ学習が進行しやすい\n","- 解決のためにミニバッチに類似した方針として、経験をメモリに保存し、そのメモリの中から経験をランダムに選んで(Replay)学習させる\n"]},{"cell_type":"markdown","metadata":{"id":"gbDx1U3heBmp"},"source":["### **Prioritized Experience Replay**\n","\n","Experience Replayについて、ランダムに選択せず、優先順位を付けて選択する\n","- 例えば、教師信号との差を求め、その差が大きい学習内容を優先的に選択するなど\n","\n","具体的にはTD誤差に相当する、次の項\n","$$\n","r_{t+1}+\\gamma Q_t(s_{t+1},a_{t+1})-Q(s_t,a_t)\n","$$\n","\n","の値が大きいtransitionを優先的にExperience Replay時に学習させる\n","\n","実際には、値の大きいtransisionを探すために探索問題を解くことになり、例えば2分木探索を行う手法が提案されている\n"]},{"cell_type":"markdown","metadata":{"id":"wjgOFQgPeIgs"},"source":["### **Fixed Target Q-Network**\n","\n","ニューラルネットワークとして行動を決定するmain-networkと誤差関数の計算時に行動価値を決めるtarget-networkの2種類を用いる\n","\n","- DQNでは価値関数$Q(s_t,a)$を更新するが、それには次の時刻の状態$s_{t+1}$での価値関数$Q)S_{t+1}, a)$が必要となり、Q関数の更新のために同じQ関数を利用すると学習が不安定になりやすい\n","\n","- そこで更新に必要な$max_a Q(s_{t+1}, a)$を求める際に、時刻を遡った(ネットワークパラメータの更新タイミングにおいて前の)別のQ関数(Fixed Target Q-network)を用いて計算する\n","  - target-networkは定期的にmain-networkで上書きする\n","\n","- Fixed Target Q-Netowrkは、Freezing the target networkとも呼ばれ、より具体的には、TD誤差の目標値に古い$Q$(target Q-network)を使うことである\n","  - $L_{\\theta}(s,a)=\\frac{1}{2}(r_{t+1}+\\gamma\\max_aQ_{\\theta^{-1}}(s_{t+1}, a)-Q_{\\theta}(s,a))^2$  \n","  とする\n","  - 一定周期で学習中の$Q_\\theta$のパラメータと同期させるため、次のようにする$(\\theta^{-1}\\leftarrow\\theta)$\n"]},{"cell_type":"markdown","metadata":{"id":"6_JBRgsLeMcy"},"source":["### **DDQN (Double-DQN)**\n","\n","Fixed Target Q-Networkでは同一ネットワークの時刻差を用いるが、DDQNでは完全に独立した2つのネットワークを利用する\n","\n","まず、TD学習でも利用した式を、main Q-Netowrkを$Q_m$、target Q-Netowrkを$Q_t$として明確に分けて記述すると次のようになる\n","\n","$$\n","Q_m(s_t,a_t){\\leftarrow}Q_m(s_t,a_t)+\\eta(r_{t+1}+\\gamma\\max_{a_{t+1}}Q_t(s_{t+1},a_{t+1})-Q_m(s_t,a_t))\n","$$\n","\n","このように、次の状態$s_{t+1}$での$Q$値が最大となる行動$a = a_{t+1}$およびその時の$Q$値の2つをTarget Q-Networkから求める\n","\n","DDQNではこれをさらに安定させるため、次のような更新式を利用する\n","\n","$$\n","a_m = arg \\max_a Q_m(s_{t+1}, a_{t+1})\n","$$\n","\n","$$\n","Q_m(s_t,a_t){\\leftarrow}Q_m(s_t,a_t)+\\eta(r_{t+1}+\\gamma Q_t(s_{t+1},a_m)-Q_m(s_t,a_t))\n","$$\n","\n","このように、次の状態$s_{t+1}$での$Q$値が最大となる行動$a_m$はMain Q-Networkから求め、その行動$a_m$での$Q$値はTarget Q-Netowrkから求める\n","\n","- Main Q-Networkの更新量を2つのネットワークを使用して求めるためDouble DQNと呼ばれる\n"]},{"cell_type":"markdown","metadata":{"id":"N4rx_BBqeQfh"},"source":["### **Huber関数**\n","\n","二乗誤差関数を用いずHuber関数を用いて計算する手法\n","\n","- Huber関数は次の通りで、例えば$\\epsilon=1$として、$-1<x<1$の範囲で$y=\\frac{1}{2}x^2$、それ以外で$y=|x|-\\frac{1}{2}$とすることで、誤差が大きいときに誤差関数の結果が二乗と大きい場合学習が安定しにくくなる問題を解決する\n","\n","$$\n","H(x)=\\left\\{\\begin{array}{ll}x^2/2 & \\text{if }|x|\\le\\epsilon\\\\\\epsilon|x|-\\epsilon^2/2&\\text{otherwise}\\end{array}\\right. |x|\\le\\epsilon\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"l5ECvjI0eV5Q"},"source":["### **Clipping Rewards**: 報酬のスケールを、正スコア+1、負スコア−1などと統一する\n","- 結果として学習対象に依存せず、同じハイパーパラメータでDQNを実行できる\n"]},{"cell_type":"markdown","metadata":{"id":"iEvrjMpqebaH"},"source":["### **Dueling Network**\n","\n","行動価値関数$Q(s,a)$の出力層において状態価値$V(s)$とアドバンテージ関数$A(s,a) = Q(s,a)-V(s)$を配置する手法\n","- 数式表現上はこれで正しいが、実際にはネットワーク構造上の表現で理解する方がわかりやすく、次のような層構造となる\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/Q2Dueling.png\" width=\"500\">\n","\n","- 行動に依存せず状態のみで定まる状態価値$V(s)$の学習が可能となるため、学習性能が乞向上する\n","- $V(s)$は行動価値が最大となる行動を採用したときの$Q$値ではなく、全行動の平均の$Q$値となる\n","- meanすなわち平均を引いているが、これは単純に引き算した状態では、行動の種類によりそれぞれ異なるバイアスが加わった状態で学習が進む結果、安定しなくなる可能性を排除するため\n","  - 誤差逆伝播においてどのような指令でも必ず全ネットワークを更新でき、同程度の重みで指令を生成できるようになる\n"]},{"cell_type":"markdown","metadata":{"id":"4mHmSB7BeePD"},"source":["### **A3C**\n","\n","Asynchronous Advantage Actor-Criticの略\n","\n","3つのAから始まる手法を用いることを意味する\n","- Asynchronous: GORILA(General Reinforcement Learning Architecture)で提案されたように、複数のエージェントを用意して分散学習させる手法を利用する\n","- Advantage: Q学習の更新において1ステップ先の状態を使用するが、2ステップ先以上を用いて更新するAdvantage手法を利用する\n","\n","更新式を定式化すると次の通りとなる\n","$$\n","Q(s_t,a_t) = r_{t+1}+\\gamma r_{t+2}+\\gamma^2\\max_a Q(s_{t+2}, a)\n","$$\n","\n","- Actor-Critic: 強化学習における方策反復法と価値反復法の両方を組み合わせた手法を利用する  \n","  Actor側出力: 方策反復法と同じで状態$s_t$に対して行動それぞれがどの程度お勧めかを示す\n","  - これをsoft max関数に掛ければ行動の採用確率のように表現できる\n","  - Actor学習に次の方策のエントロピー項が加えられることが多い\n","$$Actor_{entropy} = \\sum^a[\\pi_\\theta(a|s)log \\pi_\\theta(a|s)]\n","$$\n","  ここで、行動の種類について総和を計算しており、方策が行動をランダムに選択する学習初期では最大の値となり、どれか一つの行動しか選択しない場合最小となるように設計されており、このエントロピーをActorの誤差関数から引くことで学習初期はパラメータ学習がゆっくりと進み、局初回に落ちるのを避ける狙いがある\n","\n","  Critic側出力: 状態の価値$V^\\pi_{s_t}$であり、状態$s_t$になった場合にその先得られるであろう割引報酬和の期待値となる\n","    - この割引報酬和$J(\\theta, S_t)$は、$\\mathbb{E}$をミニバッチ平均で期待値を表すとすると\n","$$\n","J(\\theta, S_t) = \\mathbb{E}[log \\pi_\\theta(a|s)(Q^\\pi(s,a)-V^\\pi_s)]\n","$$\n","となる\n","    - $log \\pi_\\theta(a|s)$は状態$s$の時に行動$a$を選択する条件付確率の$log$を意味する\n","    - $Q^\\pi(s,a)$は状態$s$で行動$a$を選択する場合の行動価値を表し、行動$a$に対する定数であり、Advantage学習で求める\n","  状態価値$V^\\pi_s$を正しく出力できるように学習させたいため、実際に行動して得られる行動価値$Q^\\pi(s,a)$と出力$V^{\\pi}_{s}$が一致するようにすることから、損失関数は$Loss_{critic}=(Q^\\pi(s,a)-V^\\pi_s)^2$とする\n"]},{"cell_type":"markdown","metadata":{"id":"SxfEf6zBehXE"},"source":["### **UNREAL**\n","\n","UNsupervised Reinforcement and Auxiliary Learningの略\n","\n","- 最終目的となる課題(環境)を用いて学習を完結させるのではなく、その傍題となるより簡易な補助問題を用いて学習させ、この補助問題を徐々に高度化させることで最終的に本来の問題に対応させる手法\n","- 要するにいきなり難しい問題に取り組むのではなく、徐々に難しくする手法、この問題を考えて与えるのは人間\n","\n","**Skipping frames**: 計算コストを削減するという観点から毎フレーム行動選択を行わず、数フレームおきに行動選択する\n"]},{"cell_type":"markdown","metadata":{"id":"EtA5HQYsGAKS"},"source":["# OpenAI Gymによる強化学習環境設計とDQNによる強化学習\n","\n","〇×ゲーム(英語でTicTacToe)程度であれば一から構成可能であるが、今後の拡張を考えて非営利団体 OpenAIが提供する強化学習の開発・評価用のプラットフォームOpenAI Gymを利用する\n","\n","強化学習は、与えられた環境(Environment)の中で、エージェントが試行錯誤しながら報酬を最大化する行動を学習する機械学習アルゴリズムである\n","- エージェントの学習アルゴリズムに加えて環境も重要な要素となることから、強化学習用の環境を共通インターフェイスを提供する\n","- 実験結果をアップロードし他のアルゴリズムと比較することができる\n","- 完全に準備されていればそれを用いるが、準備されていない場合でも独自環境を構築することで上記メリットをえることができる\n","\n","まず最初に必要なモジュールを読み込む"]},{"cell_type":"code","metadata":{"id":"pjPyg25JR8Zn"},"source":["#import click\n","#import sys\n","import numpy as np\n","import random\n","import gym\n","from gym import spaces\n","from collections import namedtuple\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.distributions import Categorical"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nIsVICHzJmhY"},"source":["## 環境構築\n","\n","gym.Envを継承し、次の関数を準備したのち、`gym.envs.registration.register`関数を用いてgymに登録することで環境が利用可能となる\n","\n","次のメソッドを揃えたクラスを設計する\n","- `_step(self, action)`: action を実行し、その結果を返すメソッド(必須)\n","- `_reset(self)`: 環境の状態を初期化し最初の観測値を返すメソッド(必須)\n","- `_render(self, mode='human', close=False)`: 環境を可視化するメソッド(必須)、の引数の`mode`はclass毎に受け取れる任意の文字列を指定し、慣習として以下を列挙する\n","  - `human`: 人が操作できるように画面表示を行うこと意味し、戻り値はない  \n","  特に実際のゲーム画面を利用する場合は、画像表示に`gym.envs.classic_control.rendering.SimpleImageViewer`が利用でき、`rgb_array`で画面の画像情報をnumpy.array型(x, y, 3)のRGBピクセル配列で返し、`ansi`では文字列もしくはStringIOで返す\n","- `_close(self)`:\t環境を閉じて後処理をするメソッド(必須ではない)\n","- `_seed(self, seed=None)`: 乱数にシードを与えるメソッド(必須ではない)\n","\n","また、次のメンバ変数をプロパティとして与える\n","- `action_space`: Actionが及ぶ空間(入力)\n","- `observation_space: Observationが及ぶ空間(出力)\n","- `reward_range: 報酬の最小値と最大値のリスト\n"]},{"cell_type":"markdown","metadata":{"id":"NNySpd_bCFZ3"},"source":["## 状態空間と行動空間の型\n","\n","OpenAI Gymの空間として、**Box**(連続値)と**Discrete**(離散値)があり、状態空間は多くがBoxで表現され、行動空間はDiscreteの方がより学習が容易となる\n","- Box: 範囲[low、high]の連続値、Float型のn次元配列  \n","`gym.spaces.Box(low=-100, high=100, shape=(2,))`\n","\n","- Discrete: 範囲[0、n-1]の離散値、Int型の数値  \n","`gym.spaces.Discrete(4)`\n","\n","- MultiBinary: ステップ毎に任意の行動を任意の組み合わせで使用できる行動リスト  \n","`gym.spaces.MultiBinary(5)`\n","\n","- MultiDiscrete: ステップ毎に各離散セットの1つの行動のみ使用できる行動リスト  \n","`gym.spaces.MultiDiscrete([-10,10], [0,1])`\n"]},{"cell_type":"markdown","metadata":{"id":"ByxlZxsXBVx6"},"source":["## 実際の環境の宣言\n"]},{"cell_type":"markdown","metadata":{"id":"qwqanT0PdeNn"},"source":["### プロパティの宣言\n","- `reward range` で報酬の値の範囲を設定、ここでは制限なし\n","- `observation_space`で盤面の入力を定義\n","  - 0が駒無し、1がCOMの駒、2がPLAYERの駒であり3種類、よって、9x9の盤面で、ワンホットになっている\n","- `action_space`は、どこに駒を置くかを指定するため9個のワンホットで指定する\n","- `winning_streaks`は全ての勝ちパターンであり、ここに並べた駒の位置のいずれかで3つ並びが完成する\n"]},{"cell_type":"markdown","metadata":{"id":"M2cT3rh7dg34"},"source":["### `__init__`\n","- `summary`にゲームの結果を保存するためその初期化を行う\n","- 一般的にはここで`_reset`を呼び出す場合が多いが、この場合は不要である\n"]},{"cell_type":"markdown","metadata":{"id":"i96fw9p2djJy"},"source":["### `one_hot_board`\n","- `board`には盤面情報が保存されており、0が空白、1が注目プレイヤーで、2が相手プレイヤー1である(例えば`[1 0 0 0 2 1 0 2 1]`)\n","- この0,1,2をワンホットに変換するためにnp.eye(3)で3x3単位行列を作り、その第i行目を取得、これをブロードキャストし例えば`[[0. 1. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 1. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 1. 0.]]`)、さらに`reshape(-1)`で1次元配列に変換している\n","- プレイヤーが0つまりCOMか、1つまりHumanかで、注目プレーヤーを変更する。COMの場合はそのままでよいが、1の場合は上記の1と2を入れ替える必要がある(例えば、`[0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1.]`)\n","  - 実際に学習に用いるのはこのワンホットのボード情報であり、これが9x3の27個の数列であり、`observation_space`の宣言と一致する"]},{"cell_type":"markdown","metadata":{"id":"CMUyeBQrdml6"},"source":["### `_reset`\n","まずプレーヤーを0つまりCOMとし、boardを全て空白に初期化、one_hot_boardを生成する\n","- boardを変更したら必ずone_hot_boardを更新する\n"]},{"cell_type":"markdown","metadata":{"id":"nbLgbGPBdoOT"},"source":["### `_step`\n","引数actionsで、取られた行動を取得する\n","- これは、学習中はNNの出力であったり乱数であったりし、Humanであればインタラクティブな入力の値となる\n","- 次に報酬計算を行う\n","  - 置いてはいけない場所に置いた場合-10ポイントとする\n","    - これはboardが0ではないことを確認すればよい\n","  - 置けるので、おいてしまう\n","  - 次のステップで相手が勝てるかどうかを3つ並び条件それぞれで調べて(for)、相手が勝つ場合-2ポイントとする\n","    - ブロードキャストを用いて巧妙に描かれているが、`2 - self.current_playe`は相手を表し、`self.board[streak]`は3つ並ぶパターンに入っている駒をブロードキャストで全て取り出しつつ、それが相手に一致すると1、一致しない場合は0であることから、一致数が2以上となり、残りが0つまり空白であれば相手が勝つことになる\n","    - この残りが0であることを調べるために、`(self.board[streak] == 0).any()`とし、anyつまりいずれか1つが0となることを確認している\n","  - 勝利していれば+1ポイント\n","    - `self.board[streak] == self.current_player + 1`はもうわかるであろう\n","  - 引き分けの場合は0ポイントとする\n","    - `(self.board != 0).all()`ももうわかるであろう\n","- プレーヤーを0(COM)でああれば1(Human)に、1であれば0にする\n","- 最後にワンホットのボートの状態と、報酬、結果を返す"]},{"cell_type":"code","metadata":{"id":"89bYsOmKR29x"},"source":["class TicTacToe(gym.Env):\n","    reward_range = (-np.inf, np.inf)\n","    observation_space = spaces.MultiDiscrete([2 for _ in range(0, 9 * 3)])\n","    action_space = spaces.Discrete(9)\n","    winning_streaks = [\n","        [0, 1, 2],\n","        [3, 4, 5],\n","        [6, 7, 8],\n","        [0, 3, 6],\n","        [1, 4, 7],\n","        [2, 5, 8],\n","        [0, 4, 8],\n","        [2, 4, 6],\n","    ]\n","    def __init__(self, summary: dict = None):\n","        super().__init__()\n","        if summary is None:\n","            summary = {\n","                \"total games\": 0,\n","                \"ties\": 0,\n","                \"illegal moves\": 0,\n","                \"player 0 wins\": 0,\n","                \"player 1 wins\": 0,\n","            }\n","        self.summary = summary\n","\n","    def _seed(self, seed=None):\n","        pass\n","\n","    def one_hot_board(self):\n","        if self.current_player == 0:\n","            return np.eye(3)[self.board].reshape(-1)\n","        if self.current_player == 1:\n","            # permute for symmetry\n","            return np.eye(3)[self.board][:, [0, 2, 1]].reshape(-1)\n","\n","    def _reset(self):\n","        self.current_player = 0\n","        self.board = np.zeros(9, dtype=\"int\")\n","        return self.one_hot_board()\n","\n","    def _step(self, actions):\n","        exp = {\"state\": \"in progress\"}\n","        # get the current player's action\n","        action = actions\n","        reward = 0\n","        done = False\n","        # illegal move\n","        if self.board[action] != 0:\n","            reward = -10  # illegal moves are really bad\n","            exp = {\"state\": \"done\", \"reason\": \"Illegal move\"}\n","            done = True\n","            self.summary[\"total games\"] += 1\n","            self.summary[\"illegal moves\"] += 1\n","            return self.one_hot_board(), reward, done, exp\n","        self.board[action] = self.current_player + 1\n","        # check if the other player can win on the next turn:\n","        for streak in self.winning_streaks:\n","            if ((self.board[streak] == 2 - self.current_player).sum() >= 2) and \\\n","                (self.board[streak] == 0).any():\n","                reward = -2\n","                exp = {\n","                    \"state\": \"in progress\",\n","                    \"reason\": \"Player {} can lose on the next turn\".format(\n","                        self.current_player\n","                    ),\n","                }\n","        # check if we won\n","        for streak in self.winning_streaks:\n","            if (self.board[streak] == self.current_player + 1).all():\n","                reward = 1  # player wins!\n","                exp = {\n","                    \"state\": \"in progress\",\n","                    \"reason\": \"Player {} has won\".format(self.current_player),\n","                }\n","                self.summary[\"total games\"] += 1\n","                self.summary[\"player {} wins\".format(self.current_player)] += 1\n","                done = True\n","        # check if we tied, which ends the game\n","        if (self.board != 0).all():\n","            reward = 0\n","            exp = {\n","                \"state\": \"in progress\",\n","                \"reason\": \"Player {} has tied\".format(self.current_player),\n","            }\n","            done = True\n","            self.summary[\"total games\"] += 1\n","            self.summary[\"ties\"] += 1\n","        # move to the next player\n","        self.current_player = 1 - self.current_player\n","        return self.one_hot_board(), reward, done, exp\n","\n","    def _render(self, mode: str = \"human\"):\n","        print(\"{}|{}|{}\\n-----\\n{}|{}|{}\\n-----\\n{}|{}|{}\".format(*self.board.tolist()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mYwhJBOYfjIw"},"source":["DQNでは訓練にエクスペリエンス・リプレイ・メモリを利用する\n","- エージェントが観測した遷移を記憶し、後で再利用できるようにする\n","- ランダムに並び替えてバッチで利用される\n","  - これにより、DQNの学習手順が安定化する\n","- ここで、次の2つのクラスを用いて遷移を表現する\n","  - Transition: 記憶の一つの要素であり、ある一つの遷移を表す名前付きタプル\n","    - 基本的には状態とアクションのペアをnext_stateとrewardの結果にマッピングする\n","  - ReplayMemory: 最近のTransitionを記憶するメモリで、サイズが制限されたサイクリックバッファとして実装される\n","    - 学習用のランダムなバッチを構成するため`sample()`メソッドが用意される\n","\n","なお、Transitionは、最初に`Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))`と宣言されており、このnamedtupleについては理解を深める必要がある\n","\n","namedtupleはイミュータブル(変更不能)な組込データ型のtupleの拡張である\n","- tupleはシンプルなデータ型で、任意オブジェクトをグループ化することができる\n","- しかしながら、これでは要素番号でしかアクセスできないため、名前でアクセスできるようにしたのがnamedtupleである\n","- コメントの行も含め3つの書き方を示しているがすべて同じ意味\n","  - 2つ目は`from typing import NamedTuple`が必要であるが、現在は不要である\n","- アンパック代入(a, b = c)や、可変長引数(*)も利用できる\n","- namedtupleクラスにメソッドやプロパティを追加でき、継承も可能\n","\n","このnamedtupleオブジェクトは、内部的に、pythonの通常のクラスとして実装されておりメモリ効率がよい\n","- namedtupleはメモリ効率がよいイミュータブルなクラスであり、クラス定義のショートカットとなる\n","\n","ここでは、イミュータブルなstate, action, next_state, rewardのプロパティを持つクラスTransactionを定義しているかのように考えればよい\n","\n","なお、この部分の記述はPyTorchのTutorialと同一でありself.positionがサイクリックにアクセスするように設定されている"]},{"cell_type":"code","metadata":{"id":"2G3vIYWrflT1"},"source":["#Transition = namedtuple(\"Transition\", ['state', 'action', 'next_state'. 'reward'])\n","#Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n","Transition = namedtuple(\"Transition\", \"state action next_state reward\")\n","\n","class ReplayMemory(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    def push(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        self.memory[self.position] = Transition(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2dpWZi997jmg"},"source":["## ポリシーのモデル\n","\n","RNNを用いてもよいが、明確な周期性があるとは言い切れないため、全結合網で表現している\n","\n","`self.forward(state).max(1)[1].view(1, 1)`について、まず3x3x3のワンホット入力に対して、3x3の行動を求め、その中で最も大きな値を持つ、最も望ましい行動の候補を選び、maxがvalues(値)とindices(場所)の情報を返すため、場所の情報[1]だけ取り出し、これをバッチも考えて2重配列に変更している"]},{"cell_type":"code","metadata":{"id":"_51GRXzQApIV"},"source":["class Policy(nn.Module):\n","    def __init__(self, n_inputs=3 * 9, n_outputs=9):\n","        super(Policy, self).__init__()\n","        self.fc1 = nn.Linear(n_inputs, 128)\n","        self.fc2 = nn.Linear(128, 256)\n","        self.fc3 = nn.Linear(256, 128)\n","        self.fc4 = nn.Linear(128, n_outputs)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = F.relu(self.fc3(x))\n","        x = F.relu(self.fc4(x))\n","        return x\n","\n","    def act(self, state):\n","        with torch.no_grad():\n","            return self.forward(state).max(1)[1].view(1, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NkpF1h5m96zi"},"source":["## 訓練によるモデル最適化\n","\n","こちらもPyTorchのチュートリアルそのままである\n","\n","引数は次の通り\n","\n","- device (torch.device): cpuかgpuか\n","- optimizer (torch.optim.Optimizer): Optimizerの指定\n","- policy (Policy): Policy用モデルの指定\n","- target (Policy): Target用モデルの指定\n","- memory (ReplayMemory) -- Replay記憶領域\n","- batch_size (int): バッチサイズの指定\n","- gamma (float): 報酬の時間割引率\n","\n","その処理内容は次の通り\n","\n","1. まずバッチをサンプリングする\n","2. またトリッキーだが、  \n","`original = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]`  \n","この形を  \n","`result = (['a', 'b', 'c', 'd'], [1, 2, 3, 4])`  \n","この形に変換する技として、`zip(*original)`がある"]},{"cell_type":"code","metadata":{"id":"36oy9uDPsYpW"},"source":["original = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]\n","([ a for a,b in original ], [ b for a,b in original ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dtIvUy4WsjzM"},"source":["tuple([list(tup) for tup in zip(*original)])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"or6C8_OYwHXM"},"source":["\n","  - このテクニックを使って、ミニバッチサイズ(ここでは128)個のboardを同時に管理して手を進めていくが、中には勝負がついている(終了状態)のboardと、そうでないboardが混在する\n","- 計算するのは勝負がついていないboradだけでよいので、終了しているboradを除くためのTrueとFalseのテーブル(マスク)を作成する\n","  - `tuple(map(lambda s: s is not None, batch.next_state))`では、batch.next_stateで勝負がついている場合None、ついていない場合はboard情報が格納されている\n","  - 's is not None'であるため、バッチ全体について勝負がついている場合はFalse、ついていない場合はTrueのタプルが取得される\n","  - `torch.cat([s for s in batch.next_state if s is not None])`では、同様にTrue/Falseを1/0で表現する\n","\n","3. $Q(s_t, a)$を計算する\n","\n","- `policy(state_batch).gather(1, action_batch)`のgatherは、PyTorchが提供するデータ列選択手法である\n","  - dimension=1であるとき、[1, 2, 3]に対して[1]は、0オリジンで1番目、つまり[2]を選択し、これをバッチ全体で行う、つまりここでは、次にとる選択肢を順に選び出していることになる\n","\n","  - 例えば、[[10 11][12 13]]に対して[[0 0][1 0]]とインデキシングした場合、最初は[0 0]つまり、[10 11]に対して0列目を選び出すので、[10 10]となり、次の[1 0]は[12 13]に対して1列目、0列目と選択するので[13 12]となり、最終的に[[10 10][13 12]]となる\n","\n","- モデルは$Q(s_t)$を計算し、次に、取られたアクションの列を選択する。これらは、policy_netに従って各バッチ状態に対して取られたであろうアクションを選択することになる\n","\n","4. ミニバッチの全ての要素について、次の状態$てV(s_{t+1})$を計算する\n","  - 4-1でミニバッチ分確保し初期化する\n","  - 4-2で実際に行動を選択するが、non_final_next_statesの行動の期待値を過去のtarget_netに基づいて計算し、max(1)[0]で最も高い報酬を選択する\n","  - この時勝負が終了しているかどうかのマスクに基づいて値が代入される\n","\n","5. Q値の期待値を求める\n","- 時間割引率$\\gamma$を掛けて、その行動による報酬を加える\n","\n","6. Hubarロスを計算する\n","- F.smooth_l1_lossを利用している\n","\n","7. パラメータが大きくなりすぎないように$-1<p<1$でクランプする\n"]},{"cell_type":"code","metadata":{"id":"gGdAmWxqAeoY"},"source":["def optimize_model(\n","    device: torch.device,\n","    optimizer: optim.Optimizer,\n","    policy: Policy,\n","    target: Policy,\n","    memory: ReplayMemory,\n","    batch_size: int,\n","    gamma: float):\n","\n","    if len(memory) < batch_size:\n","        return\n","    transitions = memory.sample(batch_size) #(1)\n","    batch = Transition(*zip(*transitions)) #(2)\n","    non_final_mask = torch.tensor(\n","        tuple(map(lambda s: s is not None, batch.next_state)),\n","        device=device,\n","        dtype=torch.bool,\n","    )\n","    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n","    state_batch = torch.cat(batch.state)\n","    action_batch = torch.cat(batch.action)\n","    reward_batch = torch.cat(batch.reward)\n","    state_action_values = policy(state_batch).gather(1, action_batch) #(3)\n","    next_state_values = torch.zeros(batch_size, device=device) #(4-1)\n","    next_state_values[non_final_mask] = target(non_final_next_states).max(1)[0].detach() #(4-2)\n","    expected_state_action_values = (next_state_values * gamma) + reward_batch #(5)\n","    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1)) #(6)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    for param in policy.parameters():\n","        param.grad.data.clamp_(-1, 1) #(7)\n","    optimizer.step()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tcKAq5YTfFjv"},"source":["これを実行することでわかってもらえると思うが、TicTacToeでも1時間程度学習する\n","- ブロック崩しやマリオのゲームを強化学習で解くといっているが、並列でゲーム盤面を進めることも難しく、学習が極めて困難であることがわかるであろう\n","- 強化学習はエグイ、膨大な経験に基づいて、あらゆる手法の良し悪しを判断しつつ簡単に学ぶ人間は本当にすごい"]},{"cell_type":"markdown","metadata":{"id":"VQ94GzTQgUqf"},"source":["乱択するための関数、でたらめな行動をとる"]},{"cell_type":"code","metadata":{"id":"zrzJGXfhAjQV"},"source":["def select_dummy_action(state: np.array):\n","    state = state.reshape(3, 3, 3)\n","    open_spots = state[:, :, 0].reshape(-1)\n","    p = open_spots / open_spots.sum()\n","    return np.random.choice(np.arange(9), p=p)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bhAkXKnMg4_s"},"source":["行動の選択を行う関数を定義する\n","\n","引数は次の通り\n","- device (torch.device): cpuかgpuか\n","- model (Policy): モデルの指定\n","- state (torch.tensor): 現在のboard(盤面)の状態\n","- eps (float): 乱択する確率\n","\n","戻り値(torch.tensor, bool)のタプルで、行動と行動が乱宅であるかどうかのブール値である\n","\n","処理内容は、乱数がepsの値よりも大きければモデルの出力を、それ以外であれば乱択する"]},{"cell_type":"code","metadata":{"id":"2btrrvx3AmQz"},"source":["def select_model_action(\n","    device: torch.device, model: Policy, state: torch.tensor, eps: float):\n","    sample = random.random()\n","    if sample > eps:\n","        return model.act(state), False\n","    else:\n","        return (\n","            torch.tensor(\n","                [[random.randrange(0, 9)]],\n","                device=device,\n","                dtype=torch.long,\n","            ),\n","            True,\n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5XuNQ8CzovhO"},"source":["実際の学習処理を行う\n","\n","- 最初は全て乱数、その後確率10%で乱択するまでeps_steps毎徐々に確率を減らしていく\n","- ここでは、Fixed Target Q-Networkを利用しており、同一ネットワークでtargetとpolicyの2つのモデルを準備するが、target_update毎にtargetは既に学習を済ませた過去のpolicyのパラメタを読み出している\n","\n","実行には、40分程度必要"]},{"cell_type":"code","metadata":{"id":"-aqwrlgIgPq0"},"source":["n_steps: int = 500000\n","batch_size: int = 128\n","gamma: float = 0.99\n","eps_start: float = 1.0\n","eps_end: float = 0.1\n","eps_steps: int = 200000\n","device = torch.device(cuda if torch.cuda.is_available() else \"cpu\")\n","print(\"Beginning training on: {}\".format(device))\n","target_update = int((1e-2) * n_steps)\n","policy = Policy(n_inputs=3 * 9, n_outputs=9).to(device)\n","target = Policy(n_inputs=3 * 9, n_outputs=9).to(device)\n","target.load_state_dict(policy.state_dict())\n","target.eval()\n","\n","optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n","memory = ReplayMemory(50_000)\n","\n","env = TicTacToe()\n","state = torch.tensor([env._reset()], dtype=torch.float).to(device)\n","old_summary = {\n","    \"total games\": 0,\n","    \"ties\": 0,\n","    \"illegal moves\": 0,\n","    \"player 0 wins\": 0,\n","    \"player 1 wins\": 0,\n","}\n","_randoms = 0\n","summaries = []\n","\n","for step in range(n_steps):\n","    t = np.clip(step / eps_steps, 0, 1)\n","    eps = (1 - t) * eps_start + t * eps_end\n","\n","    action, was_random = select_model_action(device, policy, state, eps)\n","    if was_random:\n","        _randoms += 1\n","    next_state, reward, done, _ = env._step(action.item())\n","    # player 2 goes\n","    if not done:\n","        next_state, _, done, _ = env._step(select_dummy_action(next_state))\n","        next_state = torch.tensor([next_state], dtype=torch.float).to(device)\n","    if done:\n","        next_state = None\n","    memory.push(state, action, next_state, torch.tensor([reward], device=device))\n","    state = next_state\n","    optimize_model(\n","        device=device,\n","        optimizer=optimizer,\n","        policy=policy,\n","        target=target,\n","        memory=memory,\n","        batch_size=batch_size,\n","        gamma=gamma)\n","    if done:\n","        state = torch.tensor([env._reset()], dtype=torch.float).to(device)\n","    if step % target_update == 0:\n","        target.load_state_dict(policy.state_dict())\n","    if step % 5000 == 0:\n","        delta_summary = {k: env.summary[k] - old_summary[k] for k in env.summary}\n","        delta_summary[\"random actions\"] = _randoms\n","        old_summary = {k: env.summary[k] for k in env.summary}\n","        print(\"{} : {}\".format(step, delta_summary))\n","        summaries.append(delta_summary)\n","        _randoms = 0\n","print(\"Complete\")\n","torch.save(policy.state_dict(), 'tictactoe_model.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-GHC5ePQqewO"},"source":["学習が終了したので、実際に学習が済んだモデルと勝負してみる\n","上手く学習が進んでいれば、モデルがミスしない限り勝つことは相当困難であり、引き分けの山になるので時間の無駄なのであきらめた方が良い\n","- ここで負けるようでは〇×ゲームがわかっていないことになる"]},{"cell_type":"code","metadata":{"id":"8i3hml71idP0"},"source":["device = torch.device(cuda if torch.cuda.is_available() else \"cpu\")\n","env = TicTacToe()\n","model = Policy(n_inputs=3*9, n_outputs=9).to(device)\n","model_state_dict = torch.load(\"tictactoe_model.pt\", map_location=device)\n","model.load_state_dict(model_state_dict)\n","model.eval()\n","done = False\n","obs = env._reset()\n","exp = {}\n","player = 0\n","while not done:\n","    print(\"Commands:\\n{}|{}|{}\\n-----\\n{}|{}|{}\\n-----\\n{}|{}|{}\\n\\nBoard:\".format(*[x for x in range(0, 9)])) \n","    env._render()\n","    action = None \n","    if player == 1:\n","        action = int(input())\n","    else:\n","        state = torch.tensor([obs], dtype=torch.float).to(device)\n","        with torch.no_grad():\n","            p = F.softmax(model.forward(state), dim=1).cpu().numpy()\n","            valid_moves = (state.cpu().numpy().reshape(3,3,3).argmax(axis=2).reshape(-1) == 0)\n","            p = valid_moves*p\n","            action = p.argmax().item()        \n","    obs, _, done, exp = env._step(action)\n","    player = 1 - player\n","print(\"Commands:\\n{}|{}|{}\\n-----\\n{}|{}|{}\\n-----\\n{}|{}|{}\\n\\nBoard:\".format(*[x for x in range(0, 9)]))\n","env._render()\n","print(exp)\n","if \"tied\" in exp[\"reason\"]:\n","    print(\"A strange game. The only winning move is not to play.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CLdt8G-JdwMc"},"source":["# 課題 D\n","\n","〇×ゲームの勝率を少しでも良いので改善しなさい"]}]}