{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dataai-text-F-PyTorch-GAN.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"iClPGVu3SCIE"},"source":["---\n","> 中学生までは勉強嫌いで、何のために勉強しているのかほとんど理解していませんでした。  \n",">天野浩 85年に青色LEDの材料となるGaNの透明結晶製造に成功\n","---"]},{"cell_type":"markdown","source":["# GANとは"],"metadata":{"id":"cIO9pORO69Th"}},{"cell_type":"markdown","source":["Generative Adversarial Network（敵対的生成ネットワーク）と呼ばれるDNNモデルの一種\n","- 入力データの特徴を学習し、結果的に存在するデータの特徴をとらえた実在しないデータの生成することができる\n","- GANは、Generator（生成ネットワーク）とDiscriminator（識別ネットワーク）で構成され、互いに競い合わせながら学習を進める\n","  - 贋作者（Generator）と鑑定士（Discriminator）で競争させて学習させることから敵対的と呼ばれる\n","  - この競い合わせるというのが重要で、片側だけが先走って学習が進むと失敗する\n","  - 学習する際には、「少し難しい内容を次々と学ぶ」ことが重要で「いきなり高度な内容は学習できない」ということを意味しており、人間と同じである\n","  - 実際には、本物のデータと偽物のデータを交互に与えるなどして、判定させる\n"],"metadata":{"id":"WMFcLdZL7DiD"}},{"cell_type":"markdown","source":["## GANの特徴\n","\n","GANが使われるシーンは、例えば、\n","- 誰かに似せた**自然な**絵を自動で描かせたい\n","- よくある**偽物とわかりにくい**フェイク画像や動画、音声などを作らせたい\n","- **あたかもそこにありそうなもの**をないところから追加したい\n","- 逆にあって邪魔なものを**あたかもなかったかのように自然に**消し去りたい\n","\n","といった用途が思い当たる\n","\n","簡単に共通するのは、\n","- 人間が判断して違和感がなく自然であること\n","- 自由に生成できること\n","\n","であろう\n","\n","この魔法を実現するような内容から、AI関連で話題をさらうのは主にGAN応用であることもうなづける\n","\n","GANでは2つのモデルを競合させるように学習させる**巧妙**な手法である\n","\n","GANのその他の特徴として、次のような点を挙げることができる\n","- 教師なし学習が可能\n","- データのラベリングが不要\n","  - というか、そういう問題を扱うことが特異\n","- ラベリングが無いため学習が不安定になりやすい\n","  - 学習を安定させる工夫が必要で、様々提案されている"],"metadata":{"id":"zrscGJAO767L"}},{"cell_type":"markdown","metadata":{"id":"VENXB91VZYP8"},"source":["## DiscriminatorとGenerator\n","\n","DiscriminatorとGeneratorという2つのモデルを贋作者と鑑定士にたとえたが、この対立する目的を持つモデル同士を競わせる過程をAdversarial(敵対的) Processと呼ぶ\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/ganfig.png\" width=500>\n","\n","Generatorは一様分布や正規分布などからサンプリングしたノイズベクトル$z$をもとに、アップサンプリングして画像を生成する\n","- この辺りが何を言っているか？というのが最初はつかみにくいかもしれない\n","- やりたいことは、やりたいことは、先に述べた「自由に生成できること」であるため、この「自由に」を表現するには乱数がどうしても必要となる\n","  - つまり、乱数から何かが作れたら、乱数だから毎回違うものができ、これが「自由な生成」ということになる\n","\n","をアップサンプリングして画像とする．\n","\n","一方，Discriminatorは単純な分類問題を解くネットワークでGeneratorが生成した画像と本物の画像を分類する\n","\n","この２つのネットワークを交互に学習すれば、Generatorは本物のデータに近いデータを生成するようになる\n","\n","特に、Generatorの入力が乱数で、かつ、DiscriminatorはReal画像と、Generator画像(偽画像)を交互(もしくはランダムに)受け付けて、出力はその真偽のみという、入出力が極めてシンプルかつ謎めいた構成を持つ\n","\n","もちろん、これだけではうまく動作しない\n"]},{"cell_type":"markdown","metadata":{"id":"KhHWfJg3FkOA"},"source":["## DiscriminatorとGeneratorの目的\n","\n","Descriminatorの目的は明瞭で、予測値と実際の値が一致すればよく、より具体的にはデータを適切に分類する決定境界を見つけることが目標である\n","\n","しかしながら、Generatorは、乱数だけ入力され、それに対して勝手に生成した謎なデータを出力し、その出力が妥当なデータでなければならない\n","- つまり、Generatorは**正解となる出力値がない**状況で、妥当な出力を出す必要がある\n","- これでは、学習は進まず、何か目的・目標が必要となる\n","\n","Generatorは、**データに近いモデル分布を見つける**ことを目標とする\n","\n","- すなわち、Generatorは「今観測できているデータは、なんらかの確率分布に基づいて生成されている」という仮定に基づき、データを生成する確率分布そのものをモデル化しようと試みることであるといえる\n"]},{"cell_type":"markdown","metadata":{"id":"NL9VEVUyGzMG"},"source":["## GANの定式化\n","\n","定式化にあたり、先ほどの図における入力としてのDataset、乱数、判定結果を次のように定める\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/gan1.svg\" width=500>\n","\n","このように定めると、Datasetとしての入力の確率変数$x$に対して、Real dataとしてDiscriminatorに入力される$p_d(x)$と、Fake dataとして入力されるデータ分布と$p_g(x)$で与えられるモデル分布の2つの確率分布の「距離」を近づけることを目的とするといえる\n","\n","もちろんであるが、Generatorは明確に$x$の入力を持っておらず、その分布は明示的に与えられていない\n","- よって、例えば生成結果とデータ分布との尤度を直接計算するなどして、生成結果とデータ分布との近さを測るなどということはできない\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZrrpvX8_bAe1"},"source":["### Descriminatorの定式化\n","\n","では、実際にデータ分布$p_d(x)$を求めるとはどういうことかをみてみよう\n","\n","GANでは直接尤度を測る代わりに、データ分布とモデル分布の密度比$r(x)$を考える\n","$$\n","r(x) = \\frac{p_d(x)}{p_g(x)}\n","$$\n","\n","ここで、データ分布あるいはモデル分布から生成されたラベル付きのデータ集合$\\{(x1,y1),⋯,(xN,yN)\\}$を考え、データ分布により生成されたデータのラベルをy=1、モデル分布により生成されたデータのラベルをy=0とすると、それぞれの分布は次のように表される\n","\n","$$\n","p_d(x) = p(x|y = 1)\\\\\n","p_g(x) = p(x|y = 0)\n","$$\n","\n","この時、密度比$r(x)$はベイズの式により次のように表すことができる\n","\n","$$\n","\\begin{align}\n","r(x) &= \\frac{p(x|y = 1)}{p(x|y = 0)}\\\\\n","&= \\frac{p(y = 1| x)p(x)}{p(y=1)}\\cdot\\frac{p(y=0)}{p(y=0|x)p(x)}\n","\\end{align}\n","$$\n","\n","ここで、$\\pi = p(y=1)$とすると、\n","$$\n","\\begin{align}\n","r(x) &= \\frac{p(y = 1|x)}{p(y = 0| x)}\\cdot\\frac{1-\\pi}{\\pi}\n","\\end{align}\n","$$\n","\n","となる\n","\n","$\\pi$は実際のデータ数の比で近似できる\n","\n","ラベルはy=0かy=1のみであるため、$p(y=1∣x)$を推定できれば、密度比$r(x)$が求まる\n","- そこで、$p(y=1∣x)$を近似する分布を、例えばNNを用いて求めることを考えて、パラメータ$\\varphi$を用いて$q_\\varphi(y=1∣x)$とする\n","\n","これを式で書くと\n","\n","$$\n","p(y = 1| x) \\approx q_\\varphi(y = 1| x)\n","$$\n","となる\n","\n","$q_\\varphi(y = 1| x)$を見出すモデルをDescriminatorと呼び、$D(\\phi; x)$と表す\n","\n","本来密度比を考える問題が、分類問題と同じ確率的分類器の最適化問題に置き換わった\n","\n","この最適化に用いる誤差関数$U(D)$は、例えば、交差エントロピーを想定すれば、\n","$$\n","U(D) = -E_{p(x,y)}[y \\ln D(\\phi; x) + (1-y) \\ln (1-D(\\phi; x))]\n","$$\n","として平均を考えればよい\n","\n","これを変形すると、\n","$$\n","\\begin{align}\n","U(D) &= -E_{p(x,y)}[y \\ln D(\\phi; x) + (1-y) \\ln (1-D(\\phi; x))]\\\\\n","&= -E_{p(x|y)p(y)}[y \\ln D(\\phi; x) + (1-y) \\ln (1-D(\\phi; x))]\\\\\n","&= -E_{p(x|y=1)p(y=1)}[\\ln D(\\phi; x)] + E_{p(x|y=0)p(y=0})[ \\ln (1-D(\\phi; x))]\\\\\n","&= \\pi \\cdot E_{p_d(x)}[\\ln D(\\phi;x)]+(1-\\pi)\\cdot E_{p_g(x)}[\\ln (1-D(\\phi;x))]\n","\\end{align}\n","$$\n","\n","となる\n","\n","各ラベルのデータを与えるとき、データが丁度同数づつ混ざっていれば、y=0およびy=1となるラベルのデータ数が等しい場合$\\pi = \\frac{1}{2}$となることから、Descriminatorの目的関数$V(D)$は、\n","\n","$$\n","V(D) = E_{p_d(x)}[\\ln D(\\phi;x)]+E_{p_g(x)}[\\ln (1-D(\\phi;x))]\n","$$\n","\n","となり、これが最大となるように訓練することになる"]},{"cell_type":"markdown","metadata":{"id":"WqIHPKw4_jX-"},"source":["### Generatorの定式化\n","\n","潜在変数$z$を仮定すると、\n","\n","$$\n","p_g(x) = \\int{p(x|z)p(z)}dz\n","$$\n","\n","となる\n","\n","先ほどと同様に、$p(x|z)$を近似する分布として$q_\\theta(x|z)$を導入すると\n","\n","$$\n","p(x|z) \\approx q_\\theta(x|z)\n","$$\n","\n","となり、この$q_\\theta(x|z)$を推定するモデルとGeneratorと呼び$G(\\theta;z)$と表す\n","\n","さて、Generatorの目的関数を求めるにあたり、Descriminator $D(\\varphi;x)$について最適なDescriminator $D^*(\\varphi;x)$が得られたとすると、\n","\n","$$\n","V(D^*, G) = E_{p_d(x)}[\\ln D^*(x)]+E_{p(z)}[\\ln (1-D^*(G(\\theta;z))]\n","$$\n","\n","となる\n","\n","ここで、Generatorは、この$V(D^*, G)$を最小化することが目的であることに注意する\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oGm-CpZxK45T"},"source":["### GAN全体の定式化\n","\n","さて、実際に用いる目的関数は次の通りとなる\n","\n","Discriminatorは、Generator$G(\\theta;z)$を固定したうえで、\n","$$\n","\\mathop{\\rm max}\\limits_{\\phi}\n","E_{p_d(x)}[\\ln D(\\phi;x)]+E_{p(z)}[\\ln (1-D(\\phi;G(\\theta;z))]\n","$$\n","を計算する\n","\n","Generatorは、Descriminator$D(\\phi;x)$を固定したうえで、\n","$$\n","\\mathop{\\rm min}\\limits_{\\theta}\n","E_{p(z)}[\\ln (1-D(\\phi;G(\\theta;z))]\n","$$\n","\n","を計算する\n","\n","DとGは包含関係にあり、このことがいわゆる2つのネットワークを互いに競わせるように学習するという意味である\n","\n","より簡潔には、\n","$$\n","\\mathop{\\rm min}\\limits_{G}\\mathop{\\rm max}\\limits_{D} V(D, G)\n","$$\n","と表すことができ、学習が進むと、生成器$G(\\theta;z)$が生成するデータは、実際のデータに近くなる\n"]},{"cell_type":"markdown","metadata":{"id":"II-vUZE2Bh7T"},"source":["## GANの評価指標\n","\n","GANは教師なし学習であり、教師あり学習で用いられるAccuracy, F1 scoreといった評価指標がない。妥協案として、次の2つがしばしば利用される。"]},{"cell_type":"markdown","metadata":{"id":"FWA_MR5MU-tz"},"source":["### Frechet Inception Distance(FID)\n","\n","生成された画像の分布と元の画像の分布がどれだけ近いかを測る指標があればよいが、この近さをどのように表現するかが問題となる。そこで、人間を超える画像認識精度をもつようになった機械学習モデルを用い、画像を低次元の潜在空間で表現し、その空間で距離を測るというコンセプト。\n","\n","- 実際には、Inception V3と呼ばれるモデルで低次元な潜在空間表現、ここではPoolingの出力を用いてWasserstein-2距離を算出して利用する。次の式で求める。なお、m,cは埋め込み空間上での平均ベクトルおよび共分散行列である。添字wは生成画像を意味し、何もついていないものは実画像を意味する。距離を表すため、値は小さいほど実画像に近いことを意味し、Generator性能がより優れていることを示す。\n","$$\n","||m-m_w||^2_2+T_r(C+C_w-2(CC_2)^{1/2})\n","$$"]},{"cell_type":"markdown","metadata":{"id":"o_ZMtMCPVA5l"},"source":["## Perceptual Path Length(PPL)\n","\n","人間の感覚、つまり視覚・知覚的に潜在空間上で画像が滑らかに変化するかを表す指標です。FIDと同様に学習済みモデルにおける潜在空間での距離を利用する。\n","\n","- 画像を生成する種となる潜在空間上で、画像の変化は『知覚的』に短距離で変化しているか」を表す指標。\n","  - モーフィングのようにずれることなくダイレクトかつまっすぐに変化すれば小さな値をとるため、潜在空間がどれだけ適切に構築されているかを評価できる。\n","\n","- 例えば画像認識モデルであるVGGを使用し、その上での特徴量ベクトルの距離を用いる。解析的に求めることができないため、多くの画像を用意し、実際に距離を求めて平均値を算出することでPPLを得る。\n","\n","- この値が小さいほど潜在空間が知覚的に滑らかであることを意味する。"]},{"cell_type":"markdown","metadata":{"id":"ZsS1Nm5SaxvE"},"source":["## GANの実際\n","\n","MNISTを用いて、MNISTっぽいデータを生み出すGANを構成する\n","- 似た動作を行うAutoEncoderとも比較するとよい\n","\n","今回は正確にはGeneratorとDiscriminatorに畳み込みニューラルネットを利用しており、**DCGAN(Deep Convolutional Generative Adversarial Networks)**と呼ばれる\n","\n","今回の実装におけるDiscriminatorとGeneratorは次の通りである\n","\n","**Discriminator**\n","\n","一般的な畳み込みニューラルネットワークを利用している\n","- ただし、MaxPoolingを使わずにstride=2として画像サイズを半分にする\n","\n","**Generator**\n","入力$z$は62次元の乱数ベクトル、これをシードとして画像を生成する\n","\n","最初に全結合網を用いてサイズを拡大する\n","- 6272次元まで拡張し7x7x128のテンソルに変換\n","- ConvTranspose2Dでチャネルを減らしつつ、画像サイズをMNISTの28x28ピクセルまで拡張する\n","- 出力は1チャンネル28x28ピクセルの画像となる\n","\n"]},{"cell_type":"code","metadata":{"id":"XFx9UiDIZsFI"},"source":["import os\n","import pickle\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image\n","import matplotlib.pyplot as plt\n","device = torch.device(cuda if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jvDCbNymUIkB"},"source":["ハイパーパラメータは次の通り"]},{"cell_type":"code","metadata":{"id":"_jO3eCJ-cIbN"},"source":["# hyperparameters\n","batch_size = 128\n","lr = 0.0002\n","z_dim = 62\n","num_epochs = 25\n","sample_num = 16\n","log_dir = './logs'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MXFu4OjAccVH"},"source":["MNISTデータの読み込みとDataLoaderの設定\n","\n","今回は、訓練用とテスト用に分ける必要はない"]},{"cell_type":"code","metadata":{"id":"C5W-2yYwcLkd"},"source":["transform = transforms.ToTensor()\n","dataset = datasets.MNIST('data/mnist', train=True, download=True, transform=transform)\n","data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F2zy8KR0UNQd"},"source":["## GeneratorとDiscriminatorの定義\n","\n","GeneratorとDiscriminatorの定義は次のようになる\n","- 同様に似たようなデータを作り出す方法としてAutoEncoderを学んだ\n","- AEでは基本的にEncoderとDecoderは対称形とするが、GANでは目的が異なるため異なる構造になる\n","\n"]},{"cell_type":"code","metadata":{"id":"XCD_udZqcEGk"},"source":["class Generator(nn.Module):\n","  def __init__(self):\n","    super(Generator, self).__init__()\n","    self.fc = nn.Sequential(\n","      nn.Linear(62, 1024),\n","      nn.BatchNorm1d(1024),\n","      nn.ReLU(),\n","      nn.Linear(1024, 128 * 7 * 7),\n","      nn.BatchNorm1d(128 * 7 * 7),\n","      nn.ReLU(),\n","    )\n","    self.deconv = nn.Sequential(\n","      nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n","      nn.BatchNorm2d(64),\n","      nn.ReLU(),\n","      nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n","      nn.Sigmoid(),\n","    )\n","    initialize_weights(self)\n","  def forward(self, input):\n","    x = self.fc(input)\n","    x = x.view(-1, 128, 7, 7)\n","    x = self.deconv(x)\n","    return x\n","\n","class Discriminator(nn.Module):\n","  def __init__(self):\n","    super(Discriminator, self).__init__()\n","    self.conv = nn.Sequential(\n","      nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),\n","      nn.LeakyReLU(0.2),\n","      nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n","      nn.BatchNorm2d(128),\n","      nn.LeakyReLU(0.2),\n","    )\n","    self.fc = nn.Sequential(\n","      nn.Linear(128 * 7 * 7, 1024),\n","      nn.BatchNorm1d(1024),\n","      nn.LeakyReLU(0.2),\n","      nn.Linear(1024, 1),\n","      nn.Sigmoid(),\n","    )\n","    initialize_weights(self)\n","  def forward(self, input):\n","    x = self.conv(input)\n","    x = x.view(-1, 128 * 7 * 7)\n","    x = self.fc(x)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KVnABtknUXCO"},"source":["### ウェイトの初期化\n","\n","これまであまり問題としなかったが、GANでは学習を安定的に進めるために様々な工夫がなされている。その一つがウェイトの初期化である\n","- ただし、現時点で必須の方法というわけではない\n","- 一方で、パラメタ初期化の違いで学習結果に違いが出る可能性もあり、ここでまとめて扱う\n","\n","まず、PyTorchのパラメーター初期化について簡単に説明する\n","\n","これまでは、Noneとしてゼロ初期化のみ議論してきた\n","\n","パラメータ(重み)は`nn.Linear()`などとしたときに設計され(インスタンス化した時に実態ができる)、その値はインスタンス化した後weightメソッドを使うことでを見ることができる\n","```\n","linear = nn.Linear(5, 2)\n","linear.weight\n","```\n","また、逆にweightに対してnn.init の中のメソッドを適用すれば初期化できる\n","\n","この初期化においては、次のような分布や定数の利用が想定でけいるので、例を示す\n","\n","- 正規分布:`normal_(weight, mean, std)`\n","```\n","linear = nn.Linear(5, 2)\n","nn.init.normal_(linear.weight, 0.0, 1.0)\n","```\n","- 一様分布:`uniform_(weight, a, b)`\n","```\n","linear = nn.Linear(5, 2)\n","nn.init.normal_(linear.weight, 0.0, 1.0)\n","```\n","\n","- 定数:`constant_(weight, c)`\n","```\n","linear = nn.Linear(5, 2)\n","nn.init.constant_(linear.weight, 1.0)\n","```\n","\n","- Xavierの初期値:`xavier_normal_(weight, gain=1)`\n","```\n","linear = nn.Linear(5, 2)\n","nn.init.xavier_normal_(linear.weight)\n","```\n","\n","- Heの初期値\n","```\n","kaiming_normal_(weight, a=0, mode='fan_in', nonlinearity='leaky_relu')\n","linear = nn.Linear(5, 2)\n","nn.init.kaiming_normal_(linear.weight)\n","```\n"]},{"cell_type":"markdown","source":["### Discriminatorのウェイトの初期化について、\n","\n","ここでは、全て正規分布で初期化しており、専用の関数(メソッド)を定義している\n","- Discriminatorクラスから呼び出される"],"metadata":{"id":"zOSOM5vb-AAG"}},{"cell_type":"code","metadata":{"id":"z6zIAKB7cF6q"},"source":["def initialize_weights(model):\n","  for m in model.modules():\n","    if isinstance(m, nn.Conv2d):\n","      m.weight.data.normal_(0, 0.02)\n","      m.bias.data.zero_()\n","    elif isinstance(m, nn.ConvTranspose2d):\n","      m.weight.data.normal_(0, 0.02)\n","      m.bias.data.zero_()\n","    elif isinstance(m, nn.Linear):\n","      m.weight.data.normal_(0, 0.02)\n","      m.bias.data.zero_()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ejT17b9rbcpL"},"source":["### ネットワークのインスタンス化とオプティマイザの指定\n","- 今回はAdamで、ハイパーパラメタは微妙にチューニングしている\n","- また、ロス関数は真偽のみを議論するためバイナリクロスエントロピーを用いる"]},{"cell_type":"code","metadata":{"id":"C1LSNCaQcJ2z"},"source":["G = Generator().to(device)\n","D = Discriminator().to(device)\n","G_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n","D_optimizer = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n","criterion = nn.BCELoss()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8V69LC1Ug9mb"},"source":["## 実際の処理内容\n","\n","コードを見ると実際何をしているかがよくわかる"]},{"cell_type":"code","metadata":{"id":"2t1TTYFScOeh"},"source":["def train(D, G, criterion, D_optimizer, G_optimizer, data_loader):\n","  # 訓練モードへ\n","  D.train()\n","  G.train()\n","  # 本物ラベルは1\n","  y_real = Variable(torch.ones(batch_size, 1))\n","  # 偽物ラベルは0\n","  y_fake = Variable(torch.zeros(batch_size, 1))\n","  y_real = y_real.to(device)\n","  y_fake = y_fake.to(device)\n","  D_running_loss = 0\n","  G_running_loss = 0\n","  for batch_idx, (real_images, _) in enumerate(data_loader):\n","    # 一番最後のデータがバッチサイズに満たない場合は無視してエラーを避ける\n","    if real_images.size()[0] != batch_size:\n","      break\n","    # 潜在変数としての入力(変な言い方だが)を乱数で初期化\n","    z = torch.rand((batch_size, z_dim))\n","    real_images, z = real_images.to(device), z.to(device)\n","    # Discriminatorの勾配の初期化\n","    D_optimizer.zero_grad()\n","    # Discriminatorは実画像データを1=Trueと認識するほどよい\n","    D_real = D(real_images)\n","    D_real_loss = criterion(D_real, y_real)\n","    # DiscriminatorはGeneratorが生成した偽画像を0=Falseと認識するほどよい\n","    # fake_imagesをDiscriminatorが学習しないようにdetach()する\n","    fake_images = G(z)\n","    D_fake = D(fake_images.detach())\n","    D_fake_loss = criterion(D_fake, y_fake)\n","    # 2つのlossの和を最小化する\n","    D_loss = D_real_loss + D_fake_loss\n","    D_loss.backward()\n","    D_optimizer.step()  # Dだけ更新(Gのパラメータは更新しない)\n","    D_running_loss += D_loss.data.item()\n","    # Generatorの更新\n","    G_optimizer.zero_grad()\n","    # GeneratorにとってGeneratorが生成した画像の認識結果は1（本物）に近いほどよい\n","    fake_images = G(z)\n","    D_fake = D(fake_images)\n","    G_loss = criterion(D_fake, y_real)\n","    G_loss.backward()\n","    G_optimizer.step()\n","    G_running_loss += G_loss.data.item()\n","  D_running_loss /= len(data_loader)\n","  G_running_loss /= len(data_loader)\n","  return D_running_loss, G_running_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cRlmjcdVhPwf"},"source":["### 途中結果の確認\n","\n","学習途中で、お試しに画像を保存するため、適当な$z$から画像を生成させる"]},{"cell_type":"code","metadata":{"id":"jBWd7wk7cQNs"},"source":["def generate(epoch, G, log_dir='logs'):\n","  G.eval()\n","  if not os.path.exists(log_dir):\n","    os.makedirs(log_dir)\n","  # 生成のもとになる乱数を生成\n","  sample_z = torch.rand((64, z_dim))\n","  sample_z = sample_z.to(device)\n","  # Generatorでサンプル生成\n","  samples = G(sample_z).data.cpu()\n","  save_image(samples, os.path.join(log_dir, 'epoch_%03d.png' % (epoch)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ipsoLldwh7Hj"},"source":["## 学習\n","\n","実際にGANを学習させる\n","- 10分程度必要"]},{"cell_type":"code","metadata":{"id":"CbCuUP3ocRwO"},"source":["history = {}\n","history['D_loss'] = []\n","history['G_loss'] = []\n","for epoch in range(num_epochs):\n","  D_loss, G_loss = train(D, G, criterion, D_optimizer, G_optimizer, data_loader)\n","  print('epoch %d, D_loss: %.4f G_loss: %.4f' % (epoch + 1, D_loss, G_loss))\n","  history['D_loss'].append(D_loss)\n","  history['G_loss'].append(G_loss)    \n","  # 特定のエポックでGeneratorから画像を生成してモデルも保存\n","  if (epoch+1)%5 == 0:\n","    generate(epoch + 1, G, log_dir)\n","    torch.save(G.state_dict(), os.path.join(log_dir, 'G_%03d.pth' % (epoch + 1)))\n","    torch.save(D.state_dict(), os.path.join(log_dir, 'D_%03d.pth' % (epoch + 1)))\n","# 学習履歴を保存\n","with open(os.path.join(log_dir, 'history.pkl'), 'wb') as f:\n","  pickle.dump(history, f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hCu4ethDg7od"},"source":["## 結果の表示\n","\n","DiscriminatorとGeneratorのロス曲線は次の通り\n","\n","まだまだサチュレーションには至っておらず、より正確な画像を生成でそうである\n","\n","Discriminatorのロスが減少し、Generatorのロスが増大していることから、設計通りである\n","\n","また、学習の初期段階で不安定になることも知られており、最初ダメだからと言ってあきらめない方が良い\n","- このあたりがGANの難しいところ\n"]},{"cell_type":"code","metadata":{"id":"qYmL8SMEcTV3"},"source":["with open(os.path.join(log_dir, 'history.pkl'), 'rb') as f:\n","    history = pickle.load(f)\n","D_loss, G_loss = history['D_loss'], history['G_loss']\n","plt.plot(D_loss, label='D_loss')\n","plt.plot(G_loss, label='G_loss')\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.legend()\n","plt.grid()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wjQwsXnvr6ft"},"source":["徐々にきれいなMNISTを生成するようになっているのがわかるだろう"]},{"cell_type":"code","metadata":{"id":"mJcGt6zpjXx2"},"source":["from IPython.display import Image, display_png\n","for i in list([5, 10, 15, 20, 25]):\n","    fname = 'logs/epoch_{:03}.png'.format(i)\n","    print(20*'-')\n","    print(fname)\n","    display_png(Image(fname))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2-GkmxF87-r2"},"source":["余力がある人は、さらに計算パワーが必要なCelebAを試してみると良い\n","- 後述するが、実行は各自に任せる"]},{"cell_type":"markdown","metadata":{"id":"LvbqksL8kfWv"},"source":["## いろいろなGAN\n","\n","様々なGANが存在し、最も基本な形は、適当なベクトルから学習した画像を生成する構成である\n","\n","以下、その代表例をしめす\n","\n","- pix2pix  \n","画像の対応のペアを入力とし、それがペアとして正しいかどうかを判定して汎用的な画像の変換を行うGAN\n","  - 通常のGANはベクトルを入力に一枚の画像を生成、判定するが、pix2pixは画像を入力とし、また条件として出力画像を作成し、入力画像と出力画像のペアについて真贋を判定する\n","  - 画像を条件に画像を生成するためConditionalGANの一種である\n","\n","- pix2pixHD  \n","2048×1024の高解像画像を生成するGAN\n","  - Generatorを2段構造とし、1024×512を生成してから、解像度上げる別のGeneratorを用いる\n","\n","- CycleGAN  \n","例えばりんごとオレンジの画像など2つの画像変換するように学習したGAN\n","  - 学習時はこの場合りんごとオレンジの画像群のみあればよい\n","    - りんごと同じ姿勢、似たような形のオレンジの画像がひつようとなるわけではない\n","  - ネットワークがループ構造を持ち、オレンジの画像からりんごの画像を生成し、そのりんごの画像を再度オレンジ画像に戻したときの精度が高くなるように学習させる\n","  - 普通の馬がシマウマになったりできる\n","\n","- StarGAN  \n","CycleGANは2つの変換専用に構成するが、複数に対応する\n","  - 複数のドメイン間を学習するようにAとBのCycleGAN、BとCのCycleGANとせず、1つのGANで複数のGAN間を往来できるようにする\n","\n","- DCGAN  \n","  - CNNで構成するGAN\n","  - 既に役割は終えている\n","\n","- PGGAN(Progressive Growing of GANs)  \n","  - 高解像度の学習を段階的に行うように学習過程を構成することで、1024×1024という高解像度画像を生成する\n","\n","- ACGAN  \n","Discriminatorが真偽判定だけでなく、クラス判定もするように学習させる\n","  - 効果として、DCGANよりも綺麗な画像生成が可能となる\n","\n","- SAGAN  \n","Spectrum NormalizationやSelf Attention機構を導入することで現時点で最も高画質な画像生成を行う\n","  - 局所的な情報だけでなくSelf Attentionで大域的な視野をもって生成する\n","\n","- ConditionalGAN  \n","生成して欲しい画像のラベルも同時に入力に与えることで、指定した画像を生成する条件付き画像生成手法\n","  - ラベルも画像化するため、one-hotで正解は完全に白の画像、不正解は完全に黒の画像で構成し、チャネル数を拡張する\n","\n","- InfoGAN  \n","相互情報量を評価関数に導入し、画像の何らかの特徴と意味の取れた関係を持つように学習する\n","  - Gの入力およびDの出力に潜在空間表現を与え、Dはその潜在空間が何かも判定する\n","  - この潜在空間には何か意味を与えるなどしなくとも、例えば、回転に対応した要素や線の太さに対応した要素などが現れる\n","\n","- StackGAN  \n","pix2pixHDと同様、StackGANは2段のGANで構成されており、文章から画像を生成する段と、その画像を高精度にする段で構成される\n","  - 高精度に文章から画像を生成できる\n","\n","- AnoGAN  \n","GANはサンプルデータから生成モデルを学習するが、本物の正常入力画像は、それを生成するGeneratorの入力$z$は存在する、もしくは既知の値になるが、その周辺の$z$を使って画像生成し、その生成画像と入力画像との差があった部分を異常個所として判定する\n","  - 異常個所を色分けして表示できる\n","\n","- WGAN  \n","  - GANの損失関数にJSダイバージェンスを使わず、Wasserstein距離とすることで、学習を高速化及び安定させる\n","  - こちらが既に主流\n","\n","- StyleGAN\n","  - 様々あるGANの中でも著名なモデルが2018年に発表されたStyleGANである\n","  - Progressive Growingを用いた高解像画像生成\n","  - AdaINを用いて各層に画像のStyleを取り込む\n","  - Style Mixingによる潜在空間の自由度拡大\n"]},{"cell_type":"markdown","metadata":{"id":"woxTTB70FSbW"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"Q3e6MzRIVsuk"},"source":["### Progresive Growing\n","\n","Progressive-Growing GANで提案された高解像度画像の生成手法\n","- 低解像画像の生成から始めて徐々に高解像用のGenerator,Discriminatorを追加することで、最終的に高解像度画像を生成する手法\n","\n","下図で、最初に4x4の画像生成から始め、徐々に解像度を上げて最終的には1024x1024の高解像度画像を生成する\n","\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/ProgressiveGrowing.png\" width=500>\n"]},{"cell_type":"markdown","metadata":{"id":"24PdJPnRX3o0"},"source":["# pix2pix"]},{"cell_type":"markdown","metadata":{"id":"2vNM8QP6Ssvd"},"source":["## pix2pixを用いた画像生成\n","\n","ここでは、pix2pixを用いた白黒画像のカラーを行う\n","\n","その学習は次の通りであり、GANであるためGeneratorとDiscriminatorの2つのネットワークを利用する\n","\n","- あるカラー画像$p$に対して、グレーススケール変換$g$により$g(p)$を生成\n","- グレースケール画像からカラー画像を生成するGeneratorを$G$とする。つまりFake画像は$G(g(p))$として与えられる。\n","  - $G(g(p))$と$p$を例えばMSEなどで評価しても$G$を生成できるが、このようにして生成したGは、$p$にしか適用できない変換となる\n","    - ここで構成したいのは未知の絵に対してもカラー画像にすることができる$G$\n","- Discriminatorを$D$とすると、$D$は一般的な2値ラベルの分類器で、$G(g(p))$と元画像の2つについて、真贋すなわち、$G(g(p))$か$p$かを判別する。\n","  - 従って、$D$は、普通に$G(g(p))$や$p$を入力してそれぞれのラベルを学習させる、一般的な学習を行う\n","- $G$の目標は、とにかく自分の画像、つまり$G(g(p))$を選んでもらう確率を上げること\n","  - なお一般的には、$G(g(p))$と$p$は$1:1$で混入する\n","  - 乱数など用いず交互に導入することが多い\n","\n","このようにして、GとDの両方を学習させていく\n"]},{"cell_type":"markdown","metadata":{"id":"teGKWCBEYhRY"},"source":["## U-net\n","ここで、Generatorとしてセマンティックセグメンテーションにも用いられるU-netを利用する\n","\n","U-netは、AutoEncoderと同様、Encoder-Decoderを行うネットワークであり、エンコードの結果、今回は潜在空間$z$を256次元としている\n","\n","さらに、U-netの特徴は、Image Transferと同様の考え方になるが、画像の特徴を細部から全体特徴について各層がそれぞれ保持していると考えられることから、EncoderとDecoderをおおよそ同一の構造とし、そのパラメータを再利すると、Encoderにより抽出された元画像がもつ情報を効率的にDecoderに提供できる\n","\n","ここで、入力画像はグレースケール、出力画像はカラーであるため、完全に同一ではない\n","\n","要するに、パラメータを対象となる層でも利用しやすくする\n","  - これにはいくつ可能方法があるが、まず最初に行うのが、Encoderにおいて、一つ前の層の情報と、Encoder側の対象となる層の情報の2つの情報を混ぜるという作業であり、これには単純に入力数を増やして2つの入力を用いるという工夫がなされる\n","  - この時、この例では完全に同じ情報を利用しているが、何かしら別の層を介してもよい\n","  - これでは過学習になりそうだが、Batch Normalizationを用いてこれを回避している\n","  - 2つの入力を結合するには、torch.catを用いる\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Wax7FVvQcHYc"},"source":["## Patch GAN\n","\n","Descriminatorは、Generatorよりも小さな画像を入力する\n","\n","- 1枚の$G(g(p))$について真贋を得るとすると、それなりに大規模なネットワークが必要となる。また、真贋の鑑定に画像全体を見る必要性はあまりなく、部分的にみて不都合なところを見つけ出せばよい\n","- そこで、$G(g(p))$や$p$を分割し、その小さな画像について、それぞれで真贋を判定する\n","  - これをpatch GANと呼ぶ\n","- GANのDiscremenatorでは一般的であるがLeakey ReLUを用いる\n","- さらにInstance Normalizationを利用する\n","  - Bach Normalizationは、ミニバッチの各演算間で分布が揃うように値を移動するという処理を施す\n","  - 各イテレーションの平均の平均、各イテレーションでの分散の平均を求め、これをもとに分布の移動と拡大を行う\n","インスタンスノームは1つのデータの1つのチャネルに対して正規化する方法で、GANではしばしば用いられる\n","  - 特にInstance Normalizationでなければならないかは実験が必要\n","\n"]},{"cell_type":"code","metadata":{"id":"ws3MhnGaHVtz"},"source":["import os\n","import glob\n","import pickle\n","import torch\n","import torch.nn.functional as F\n","import torchvision\n","import torch.utils.data as data\n","import torchvision.transforms as transforms\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from torch import nn\n","from skimage import io\n","import datetime\n","\n","usedevice = \"cuda\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jMDBVE1LggoF"},"source":["データを読み込むが相当にサイズが大きく、ダウンロードとデータの展開だけで2分程度必要である\n","\n","ファイルサイズが大きいと、Google Driveは直リンクダウンロードを拒否するので、pythonでゴリ押しする\n","\n","- なお、boxを利用するとどうやらwgetでもできるようになった様子なので、wgetで取得するようにする\n","- いざというときのために、ゴリ押しルーチンはそのままにしておく"]},{"cell_type":"code","metadata":{"id":"UaIt7tJzxLuC"},"source":["import requests\n","\n","def download_file_from_google_drive(id, destination):\n","    URL = \"https://docs.google.com/uc?export=download\"\n","\n","    session = requests.Session()\n","\n","    response = session.get(URL, params = { 'id' : id }, stream = True)\n","    token = get_confirm_token(response)\n","\n","    if token:\n","        params = { 'id' : id, 'confirm' : token }\n","        response = session.get(URL, params = params, stream = True)\n","\n","    save_response_content(response, destination)    \n","\n","def get_confirm_token(response):\n","    for key, value in response.cookies.items():\n","        if key.startswith('download_warning'):\n","            return value\n","\n","    return None\n","\n","def save_response_content(response, destination):\n","    CHUNK_SIZE = 32768\n","\n","    with open(destination, \"wb\") as f:\n","        for chunk in response.iter_content(CHUNK_SIZE):\n","            if chunk: # filter out keep-alive new chunks\n","                f.write(chunk)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zoZJrKEea4qS"},"source":["import os\n","if not os.path.exists('cocog2c.tgz'):\n","  #file_id = '1OhWZKb1EcIMWTKJb8h6iRoTDx9mIQLFX'\n","  #destination = 'cocog2c.tgz'\n","  #download_file_from_google_drive(file_id, destination)\n","  !wget https://keio.box.com/shared/static/7ogtorp8uhgjjbtltyfpamkgp6kwb1si -O cocog2c.tgz\n","  !tar xzf colog2c.tgz\n","trainfiledir = \"coco/train/*\"\n","testfiledir = \"coco/test/*\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mie6bUWnyrR-"},"source":["サイズを確認する\n","- `-rw-r--r-- 1 root root 1627290651 MON DAY HH:MM cocog2c.tgz` となるはずである"]},{"cell_type":"code","metadata":{"id":"LI2RXCOyfuqS"},"source":["!ls -laF cocog2c.tgz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YESNeUsFgct7"},"source":["Generatorは[batch_size, 3, 128, 128]で、\n","チャネル数はRGBの3、128$\\times$128の画像となる\n","\n","Discriminatorは[batch_size, 1, 4, 4]で、4$\\times$4の各場所について真贋を判定し、チャネル数は1の出力となる\n","\n"]},{"cell_type":"code","metadata":{"id":"DhLPHbJ4LIgj"},"source":["class Generator(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2)\n","    self.bn1 = nn.BatchNorm2d(32)\n","\n","    self.av2 = nn.AvgPool2d(kernel_size=4)\n","    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n","    self.bn2 = nn.BatchNorm2d(64)\n","\n","    self.av3 = nn.AvgPool2d(kernel_size=2)\n","    self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n","    self.bn3 = nn.BatchNorm2d(128)\n","\n","    self.av4 = nn.AvgPool2d(kernel_size=2)\n","    self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n","    self.bn4 = nn.BatchNorm2d(256)\n","\n","    self.av5 = nn.AvgPool2d(kernel_size=2)\n","    self.conv5 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","    self.bn5 = nn.BatchNorm2d(256)\n","\n","    self.un6 = nn.UpsamplingNearest2d(scale_factor=2)\n","    self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","    self.bn6 = nn.BatchNorm2d(256)\n","\n","    #conv7にはconv6の出力とconv4の出力を流す, input channelが2倍\n","    self.un7 = nn.UpsamplingNearest2d(scale_factor=2)\n","    self.conv7 = nn.Conv2d(256 * 2, 128, kernel_size=3, stride=1, padding=1)\n","    self.bn7 = nn.BatchNorm2d(128)\n","\n","    #conv8にはconv7の出力とconv3の出力を流す, input channelが2倍\n","    self.un8 = nn.UpsamplingNearest2d(scale_factor=2)\n","    self.conv8 = nn.Conv2d(128 * 2, 64, kernel_size=3, stride=1, padding=1)\n","    self.bn8 = nn.BatchNorm2d(64)\n","\n","    #conv9にはconv8の出力とconv2の出力を流す, input channelが2倍\n","    self.un9 = nn.UpsamplingNearest2d(scale_factor=4)\n","    self.conv9 = nn.Conv2d(64 * 2, 32, kernel_size=3, stride=1, padding=1)\n","    self.bn9 = nn.BatchNorm2d(32)\n","\n","    self.conv10 = nn.Conv2d(32 * 2, 3, kernel_size=5, stride=1, padding=2)\n","    self.tanh = nn.Tanh()\n","\n","  def forward(self, x):\n","    #x1-x4はtorch.catする必要があるので,残しておく\n","    x1 = F.relu(self.bn1(self.conv1(x)), inplace=True)\n","    x2 = F.relu(self.bn2(self.conv2(self.av2(x1))), inplace=True)\n","    x3 = F.relu(self.bn3(self.conv3(self.av3(x2))), inplace=True)\n","    x4 = F.relu(self.bn4(self.conv4(self.av4(x3))), inplace=True)\n","    x = F.relu(self.bn5(self.conv5(self.av5(x4))), inplace=True)\n","    x = F.relu(self.bn6(self.conv6(self.un6(x))), inplace=True)\n","    x = torch.cat([x, x4], dim=1)\n","    x = F.relu(self.bn7(self.conv7(self.un7(x))), inplace=True)\n","    x = torch.cat([x, x3], dim=1)\n","    x = F.relu(self.bn8(self.conv8(self.un8(x))), inplace=True)\n","    x = torch.cat([x, x2], dim=1)\n","    x = F.relu(self.bn9(self.conv9(self.un9(x))), inplace=True)\n","    x = torch.cat([x, x1], dim=1)\n","    x = self.tanh(self.conv10(x))\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gtk9Irc0LOAy"},"source":["class Discriminator(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2)\n","    self.in1 = nn.InstanceNorm2d(16)\n","\n","    self.av2 = nn.AvgPool2d(kernel_size=2)\n","    self.conv2_1 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n","    self.in2_1 = nn.InstanceNorm2d(32)\n","    self.conv2_2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n","    self.in2_2 = nn.InstanceNorm2d(32)\n","\n","    self.av3 = nn.AvgPool2d(kernel_size=2)\n","    self.conv3_1 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n","    self.in3_1 = nn.InstanceNorm2d(64)\n","    self.conv3_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n","    self.in3_2 = nn.InstanceNorm2d(64)\n","\n","    self.av4 = nn.AvgPool2d(kernel_size=2)\n","    self.conv4_1 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n","    self.in4_1 = nn.InstanceNorm2d(128)\n","    self.conv4_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n","    self.in4_2 = nn.InstanceNorm2d(128)\n","\n","    self.av5 = nn.AvgPool2d(kernel_size=2)\n","    self.conv5_1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n","    self.in5_1 = nn.InstanceNorm2d(256)\n","    self.conv5_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","    self.in5_2 = nn.InstanceNorm2d(256)\n","\n","    self.av6 = nn.AvgPool2d(kernel_size=2)\n","    self.conv6 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n","    self.in6 = nn.InstanceNorm2d(512)\n","\n","    self.conv7 = nn.Conv2d(512, 1, kernel_size=1)\n","\n","  def forward(self, x):      \n","    x = F.leaky_relu(self.in1(self.conv1(x)), 0.2, inplace=True)\n","    x = F.leaky_relu(self.in2_1(self.conv2_1(self.av2(x))), 0.2, inplace=True)\n","    x = F.leaky_relu(self.in2_2(self.conv2_2(x)), 0.2, inplace=True)\n","    x = F.leaky_relu(self.in3_1(self.conv3_1(self.av3(x))), 0.2, inplace=True)\n","    x = F.leaky_relu(self.in3_2(self.conv3_2(x)), 0.2, inplace=True)\n","    x = F.leaky_relu(self.in4_1(self.conv4_1(self.av4(x))), 0.2, inplace=True)\n","    x = F.leaky_relu(self.in4_2(self.conv4_2(x)), 0.2, inplace=True)\n","    x = F.leaky_relu(self.in5_1(self.conv5_1(self.av5(x))), 0.2, inplace=True)\n","    x = F.leaky_relu(self.in5_2(self.conv5_2(x)), 0.2, inplace=True)\n","    x = F.leaky_relu(self.in6(self.conv6(self.av6(x))), 0.2, inplace=True)\n","    x = self.conv7(x)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XhLjtf7egEME"},"source":["DataLoaderは、PyTorchのDatasetクラスを継承して機能拡張している\n","- カラー画像を取得する\n","- データ拡張を行う\n","- これをそのまま、真贋の真のデータとする\n","\n","ここには含まれていないが、併せてグレースケール変換を行う"]},{"cell_type":"code","metadata":{"id":"wYk0_XOtLUCt"},"source":["class DataAugment():\n","  # データ拡張\n","  def __init__(self, resize):\n","    self.data_transform = transforms.Compose([\n","      transforms.RandomResizedCrop(resize, scale=(0.9, 1.0)),\n","      transforms.RandomHorizontalFlip(),\n","      transforms.RandomVerticalFlip()])\n","  def __call__(self, img):\n","    return self.data_transform(img)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LL-lEFWNlHH9"},"source":["もう一つのDataLoaderは、単にデータの正規化を行う"]},{"cell_type":"code","metadata":{"id":"odQPsSgeLVa1"},"source":["class ImgTransform():\n","  # データの正規化\n","  def __init__(self, resize, mean, std):\n","    self.data_transform = transforms.Compose([\n","      transforms.Resize(resize),\n","      transforms.ToTensor(),\n","      transforms.Normalize(mean, std)])\n","  def __call__(self, img):\n","    return self.data_transform(img)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h4Alv3CNlhUJ"},"source":["グレースケール変換を行うクラス"]},{"cell_type":"code","metadata":{"id":"s1KTKB94LZT7"},"source":["class MonoColorDataset(data.Dataset):\n","  def __init__(self, file_list, transform_tensor, augment=None):\n","    self.file_list = file_list\n","    self.augment = augment     #PIL to PIL\n","    self.transform_tensor = transform_tensor  #PIL to Tensor\n","\n","  def __len__(self):\n","    return len(self.file_list)\n","\n","  def __getitem__(self, index):\n","    #index番号のファイルパスを取得\n","    img_path = self.file_list[index]\n","    img = Image.open(img_path)\n","    img = img.convert(\"RGB\")\n","    if self.augment is not None:\n","      img = self.augment(img)\n","    #モノクロ画像用のコピー\n","    img_gray = img.copy()\n","    #カラー画像をモノクロ画像に変換\n","    img_gray = transforms.functional.to_grayscale(img_gray, num_output_channels=3)\n","    #PILをtensorに変換\n","    img = self.transform_tensor(img)\n","    img_gray = self.transform_tensor(img_gray)\n","    return img, img_gray"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nqssBB_uh0jL"},"source":["テスト用のデータローダ"]},{"cell_type":"code","metadata":{"id":"t7QZ2i5FLbHO"},"source":["def load_train_dataloader(file_path, batch_size):\n","  size = (128,128)             #画像の1辺のサイズ\n","  mean = (0.5, 0.5, 0.5) #画像の正規化した際のチャンネル毎の平均値\n","  std = (0.5, 0.5, 0.5)  #画像の正規化した際のチャンネル毎の標準偏差\n","\n","  #データセット\n","  train_dataset = MonoColorDataset(file_path_train, \n","    transform_tensor=ImgTransform(size, mean, std), \n","    augment=DataAugment(size))\n","  #データローダー\n","  train_dataloader = data.DataLoader(train_dataset,\n","    batch_size=batch_size,\n","    shuffle=True)\n","  return train_dataloader"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oW2wloAgiJix"},"source":["PyTorchのtensor表現された画像をタイル状に描画する\n","- nrowでタイルの1辺の数を決定できる"]},{"cell_type":"code","metadata":{"id":"eo6EfS2d6pgg"},"source":["def mat_grid_imgs(inum, imgs, nrow, save_path = None):\n","  imgs = torchvision.utils.make_grid(\n","    imgs[0:(nrow**2), :, :, :], nrow=nrow, padding=5)\n","  imgs = imgs.numpy().transpose([1,2,0])\n","  imgs -= np.min(imgs)   #最小値を0\n","  imgs /= np.max(imgs)   #最大値を1\n","\n","  plt.imshow(imgs)\n","  plt.xticks([])\n","  plt.yticks([])\n","  plt.show()\n","\n","  if save_path is not None:\n","    io.imsave(save_path, imgs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dYchMPkmiY5n"},"source":["テスト画像をロードして、グレースケール画像とフェイク画像をタイル状に描画する"]},{"cell_type":"code","metadata":{"id":"2TyyGS2gLgYP"},"source":["def evaluate_test(file_path_test, model_G, device=usedevice, nrow=4):\n","  # test画像をロード,gray画像とfake画像をタイル状に描画\n","  model_G = model_G.to(device)\n","  size = (128,128)\n","  mean = (0.5, 0.5, 0.5)\n","  std = (0.5, 0.5, 0.5)\n","  test_dataset = MonoColorDataset(file_path_test, \n","    transform_tensor=ImgTransform(size, mean, std), \n","    augment=None)\n","  test_dataloader = data.DataLoader(test_dataset,\n","     batch_size=nrow**2,\n","     shuffle=False)\n","  #データローダーごとに画像を描画\n","  for i, (img, img_gray) in enumerate(test_dataloader):\n","    mat_grid_imgs(i, img_gray, nrow=nrow)\n","    img = img.to(device)\n","    img_gray = img_gray.to(device)\n","    #img_grayからGeneratorを用いて,FakeのRGB画像\n","    img_fake = model_G(img_gray)\n","    img_fake = img_fake.to(\"cpu\")\n","    img_fake = img_fake.detach()\n","    mat_grid_imgs(i, img_fake, nrow=nrow)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zMvgtS5diepr"},"source":["学習前に、一番最初の状態を確認する\n","\n","ここでは、ノイズのかかった画像になる\n","- 通常の画像を扱うGANでは砂嵐画像から学習を開始するが、U-netの結合によりなんとなく形は残るようになる"]},{"cell_type":"code","metadata":{"id":"XE_tksfF-iRM"},"source":["g = Generator()\n","file_path_test = glob.glob(testfiledir)\n","evaluate_test(file_path_test, g)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GUKu47YGi0gN"},"source":["画像をえり好みしてグレースケールに変換する\n","\n","- 初めからグレースケールであったり、あまり色の変化がない画像は学習の役に立たないので採用しないようにしている\n","\n","- えり好みした画像を専用の場所に保存する\n","\n","この変換と保存作業はかなり重く、ここだけで4分程度必要である"]},{"cell_type":"code","metadata":{"id":"iFBQ57dcxC6L"},"source":["from skimage import io, color, transform\n","\n","def color_mono(image, threshold=150):\n","  #入力画像がカラーか否かを判別する(thresholdでカラーぐらいの閾値を与える)\n","  image_size = image.shape[0] * image.shape[1]\n","  #チャネル0と1、0と2、1と2について差分を求める\n","  diff = np.abs(np.sum(image[:,:, 0] - image[:,:, 1])) / image_size\n","  diff += np.abs(np.sum(image[:,:, 0] - image[:,:, 2])) / image_size\n","  diff += np.abs(np.sum(image[:,:, 1] - image[:,:, 2])) / image_size\n","  if diff > threshold:\n","    return \"color\"\n","  else:\n","    return \"mono\"\n","\n","def bright_check(image, ave_thres = 0.15, std_thres = 0.1):\n","  try:\n","    #白黒に変換する\n","    image = color.rgb2gray(image)\n","    #明るすぎる画像、暗すぎる画像、明るさに差がない画像を除く\n","    if image.shape[0] < 144:\n","      return False    \n","    if np.average(image) > (1.-ave_thres): #明るすぎる画像\n","      return False\n","    if np.average(image) < ave_thres: #暗すぎる画像\n","      return False\n","    if np.std(image) < std_thres: #明るさに差がない画像\n","      return False\n","    return True\n","  except:\n","    return False\n","\n","paths = glob.glob(trainfiledir)\n","numpics = 0\n","maxpics = 9990\n","for i, path in enumerate(paths):\n","  image = io.imread(path)\n","  save_name = \"./trans/mscoco_\" + str(i) +\".png\"\n","  x = image.shape[0] #xピクセル数\n","  y = image.shape[1] #yピクセル数\n","  try:\n","    #xとyの内、短い方の1/2\n","    clip_half = min(x, y)/2\n","    #画像を正方形で切り出し\n","    image = image[int(x/2 -clip_half): int(x/2 + clip_half),\n","      int(y/2 -clip_half): int(y/2 + clip_half), :]\n","    if color_mono(image) == \"color\":\n","      if bright_check(image):\n","        image = transform.resize(image, (144, 144, 3),\n","                                 anti_aliasing = True)\n","        image = np.uint8(image*255)\n","        io.imsave(save_name, image)\n","        numpics += 1\n","        if numpics > maxpics:\n","          break\n","  except:\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sAdav6vljMee"},"source":["学習をそれぞれ行う\n","\n","ロス計算において、true_labelsおよびfalse_labelsは、4$\\times$4のブロック毎に判定していることに注意する\n"]},{"cell_type":"code","metadata":{"id":"BLgVQt6VxPAq"},"source":["def train(model_G, model_D, epoch, epoch_plus):\n","  device = usedevice\n","  batch_size = 100\n","  tstart = datetime.datetime.now()\n","\n","  model_G = model_G.to(device)\n","  model_D = model_D.to(device)\n","\n","  params_G = torch.optim.Adam(model_G.parameters(),\n","    lr=0.0002, betas=(0.5, 0.999))\n","  params_D = torch.optim.Adam(model_D.parameters(),\n","    lr=0.0002, betas=(0.5, 0.999))\n","  #loss計算のためのラベル\n","  true_labels = torch.ones(batch_size, 1, 4, 4).to(device)    #True\n","  false_labels = torch.zeros(batch_size, 1, 4, 4).to(device)  #False\n","  #loss_function\n","  bce_loss = nn.BCEWithLogitsLoss()\n","  mae_loss = nn.L1Loss()\n","  log_loss_G_sum, log_loss_G_bce, log_loss_G_mae = list(), list(), list()\n","  log_loss_D = list()\n","\n","  for i in range(epoch):\n","    #ロスを記録\n","    loss_G_sum, loss_G_bce, loss_G_mae = list(), list(), list()\n","    loss_D = list()\n","\n","    train_dataloader = load_train_dataloader(file_path_train, batch_size)\n","\n","    for real_color, input_gray in train_dataloader:\n","      batch_len = len(real_color)\n","      real_color = real_color.to(device)\n","      input_gray = input_gray.to(device)\n","      #Generatorの訓練\n","      fake_color = model_G(input_gray) #偽カラー画像生成\n","      fake_color_tensor = fake_color.detach()\n","      #偽画像のロスを計算\n","      LAMBD = 100.0 # BCEとMAEの係数\n","      #fake画像を識別器に入れたときの出力\n","      out = model_D(fake_color)\n","      #Dの出力に対するロス\n","      loss_G_bce_tmp = bce_loss(out, true_labels[:batch_len])\n","      #Gの出力に対するロス\n","      loss_G_mae_tmp = LAMBD * mae_loss(fake_color, real_color)\n","      loss_G_sum_tmp = loss_G_bce_tmp + loss_G_mae_tmp\n","\n","      loss_G_bce.append(loss_G_bce_tmp.item())\n","      loss_G_mae.append(loss_G_mae_tmp.item())\n","      loss_G_sum.append(loss_G_sum_tmp.item())\n","\n","      params_D.zero_grad()\n","      params_G.zero_grad()\n","      loss_G_sum_tmp.backward()\n","      params_G.step()\n","\n","      #Discriminatorの訓練\n","      real_out = model_D(real_color)\n","      fake_out = model_D(fake_color_tensor)\n","\n","      #ロスの計算\n","      loss_D_real = bce_loss(real_out, true_labels[:batch_len])\n","      loss_D_fake = bce_loss(fake_out, false_labels[:batch_len])\n","\n","      loss_D_tmp = loss_D_real + loss_D_fake\n","      loss_D.append(loss_D_tmp.item())\n","\n","      params_D.zero_grad()\n","      params_G.zero_grad()\n","      loss_D_tmp.backward()\n","      params_D.step()\n","\n","    i = i + epoch_plus\n","    telapsed = datetime.datetime.now() - tstart\n","    print(i, \"loss_G\", np.mean(loss_G_sum), \"loss_D\", np.mean(loss_D), \" time:\", telapsed)\n","    log_loss_G_sum.append(np.mean(loss_G_sum))\n","    log_loss_G_bce.append(np.mean(loss_G_bce))\n","    log_loss_G_mae.append(np.mean(loss_G_mae))\n","    log_loss_D.append(np.mean(loss_D))\n","\n","    file_path_test = glob.glob(testfiledir)\n","    evaluate_test(file_path_test, model_G, device)\n","\n","  return model_G, model_D, [log_loss_G_sum, log_loss_G_bce, log_loss_G_mae, log_loss_D]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HsxCzFcdjjtE"},"source":["学習させつつ、実際に白黒画像をカラー化する"]},{"cell_type":"code","metadata":{"id":"k9_46WX8xUN_"},"source":["file_path_train = glob.glob(testfiledir)\n","model_G = Generator()\n","model_D = Discriminator()\n","model_G, model_D, logs = train(model_G, model_D, 40, 0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5RUXx5xExnRv"},"source":["# VAEとGAN\n","\n","VAEとGANを単純に画質で比較すれば、VAEはぼやっとした画像となり、GANはくっきりした画像になる\n","- VAEの画像には詳細が描かれない\n","\n","VAEは、潜在空間の中に連続的に画像が存在する事が損失関数の形から要請されている\n","- 結果として潜在空間の近い点同士の画像は似ていなくてはならない\n","- 2つの画像が似ているというのは、行列としての画像の同じ要素の値が近いということを意味する\n","- つまり、模様のような一つ一つの画像固有の部分を細かく学習することは苦手である\n","\n","学習という面でGANには問題が沢山ある\n","- 2つのモデルを学習するときに、損失関数が学習度合の絶対値の指標にならない\n","- モデルが複雑になるため、学習コストが破綻的に必要となる\n","- これらを解決するGANの改良が次々と提案されている\n","\n","一方で、VAEも負けていない\n","- 最新の研究でVQ-VAE2が提案され、GAN最強といわれるBigGANに劣らない性能をより少ないコストの計算で達成するとしており、BigGANを超えたとさえも言われている"]},{"cell_type":"markdown","metadata":{"id":"SkoIrdqvfEH0"},"source":["# 課題\n","\n","Pix2pixによるカラー化において、自分で準備、もしくは作成した白黒画像を実際にカラー化しなさい\n","- 本内容で得られたコード、学習結果を利用してよい\n","- 解像度は問わない"]}]}