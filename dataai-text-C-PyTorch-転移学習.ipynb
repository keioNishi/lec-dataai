{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dataai-text-C-PyTorch-転移学習.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"iClPGVu3SCIE"},"source":["---\n",">「『優れた芸術家はまねをし、偉大な芸術家は盗む』とピカソは言った。\n","だからすごいと思ってきたさまざまなアイデアをいつも盗んできた。」\\\n",">スティーブ・ジョブズ\n","---"]},{"cell_type":"markdown","metadata":{"id":"V4Vk2qzyem_b"},"source":["# 転移学習\n","\n","一般にデモで凄い！と思わせるようなAIアプリはともかく、シンプルなVGGやResNetといった画像認識でさえ、膨大なデータと計算量が必要である\n","- フルスクラッチ（ランダム重み）から学習できるひとは一握り\n","\n","では、あきらめるのか？というと、それに対する一つの答えが**転移学習(Transfer Learning)**である\n","- DLにおけるエコ、地球にやさしいDL\n","\n","転移学習とは**大規模データで学習済みのモデルを別のタスクに転移、つまり応用する技術全般を指す**"]},{"cell_type":"markdown","metadata":{"id":"7ps7NFPgem_d"},"source":["## 一般的な転移学習の分類"]},{"cell_type":"markdown","source":["転移学習でできることは、\n","- 利用したデータセットを流用・転用する\n","- 利用したモデル（ネットワーク構成）を流用・転用する\n","- 利用したパラメータを流用・転用する\n","\n","のいずれかであり、技術的に学ぶべきところは、\n","\n","- オリジナルモデルの一部を利用する（一部を変更する）にはどうすればよいか\n","- オリジナルモデルのパラメータの一部を利用する（一部を変更する）にはどうすればよいか\n","という点を学ぶことができればよい。"],"metadata":{"id":"pvEtyBpc8MP-"}},{"cell_type":"markdown","metadata":{"id":"gFVU91yLTW8F"},"source":["### 問題設定に基づく分類\n","\n","**帰納的転移学習(Inductive Transfer Learning)**\n","\n","元の特徴量(ソースドメイン)と転移先の特徴量(ターゲットドメイン)が同じか否かに関わらず、元の変換(特に条件付確率モデル)(ソースタスク)と転移先の変換(ターゲットタスク)が異なる\n","\n","- 帰納的転移学習はターゲットドメインにラベルが存在する場合に定義可能\n","- ソースにラベルがある場合は**機能的転移学習**、ない場合は**自己教示学習**と呼ぶ\n","\n","**教師なし転移学習(Unsupervised Transfer Learning)**\n","\n","ソースタスクとターゲットタスクが異なり、教師なし学習であるクラスタリングや次元削減をターゲットドメインで解く手法\n","- ソースドメインとターゲットドメインの両方にラベルがない\n","\n","**トランスダクティブ転移学習(Trunsductive Transfer Learning)**\n","\n","ソースタスクとターゲットタスクが同一であるが、ドメインが異なる場合の手法\n","\n","- ソースドメインにおけるラベル付きデータの殆どが利用可能であるが、ターゲットドメインのラベル付きデータが利用できない場合を指す"]},{"cell_type":"markdown","metadata":{"id":"OW9aCqPzTcDG"},"source":["### アプローチに基づく分類\n","\n","何を転移するかによる分類\n","\n","**インスタンス転移**\n","- ソースドメインのインスタンス、つまりサンプルや データセットの特定部分について再度重み付けすることで、ターゲットドメインの学習に再利用する\n","\n","**特徴表現転移**\n","- ソースドメインとターゲットドメインの差や、分類・回帰モデルの誤差を軽減する都合の良い特徴表現を発見し利用する\n","\n","**パラメータ転移**\n","- ソースモデルとターゲットモデルそれぞれのハイパーパラメータが同一であるという前提のもと、そのパラメータや事前分布を発見し利用する\n","\n","**関係性のある知識の転移**\n","- ソースモデルにおける結果としてのデータ間の関係性をターゲットドメインに転移し利用する\n"]},{"cell_type":"markdown","metadata":{"id":"2N_KYVQ4em_e"},"source":["## ディープラーニングにおける転移学習の分類\n","\n","**ネットワークベース転移学習**\n","\n","ソースドメインのネットワーク(モデル)を再利用する手法で、DLにおける転移学習といえば、一般にこの手法を指す\n","\n","- **事前学習済みネットワークベース転移学習**\n","\n"," 最終層で最終出力が得られるが、その層まではそこに至る特徴を保存しているといえ、この特徴を再利用するために最終層付近のみ重みを再学習する、もしくは、ネットワークを組み替える\n","\n","- **事前学習済ネットワークの微調整(ファインチューニング)**\n","\n"," 単にファインチューニングと呼ばれることの多い手法で、学習済みネットワークの重みを初期値として、ターゲットドメインでモデル全体の重みを再学習する、もしくは、ネットワークの一部を組み替えて全体を再学習させる\n","  - 違いは、固定するパラメタがあるかないか\n","\n","なお、この違いについては、文献などにより様々存在しており、画一的な見解がなく、例えば次のような分類も存在する\n","\n","- 転移学習\n","  学習済みモデルの重みは更新せず、このモデルに新たな層を追加、この追加した層のみ学習させ重みを更新する\n","\n","- ファインチューニング\n","  学習済みモデルの一部の重みを更新せず、主に後段の層の重みを更新しつつ、追加した層も学習により重みを更新する\n","\n","この分類は「層を追加する」という観点で異なるが、いずれの場合も、「ファインチューニングの方が更新対象範囲が広い」という観点で類似している\n","\n","\n","**インスタンスベース転移学習**\n","\n","ソースドメインのインスタンス、つまりサンプルやデータセットの特定部分について再重み付けすることで、ターゲットドメインの学習に再利用する\n","\n","- 先に示したインスタンス転移を行うこと\n","\n","**地図ベース転移学習**\n","\n","ソースドメインとターゲットドメインのインスタンスを新しいデータ空間にマッピングして利用すること\n","\n","**敵対ベース転移学習**\n","\n","GANを活用しソースドメインとターゲットドメインの両方に適用可能で転移可能な表現を見つけ出して利用すること\n","- ソースドメインとターゲットドメインそれぞれから特徴量を抽出し、GANのDescriminator(識別ネットワーク)でどちらのドメインに属する特徴量かを判別させる\n","  - 判定精度が低ければ、両ドメインの特徴量の差が小さく、転送性が良いと判断する\n","  - 判別性能が高い場合は、特徴量の差が大きく、転送性が低いと判断する\n","\n","なお、以下の関連用語についてもここで纏めて奥\n","\n","**蒸留(Distillation)**\n","\n","大容量かつ深いモデルで学んだ知識を蒸留、すなわち縮約し、小さく軽量なモデルの学習に活用すること\n","\n","**マルチタスク学習**\n","\n","ソースとターゲットを区別せず、共有層を含む複数のタスクを同時に学習させること"]},{"cell_type":"markdown","metadata":{"id":"hzwkgbIvem_f"},"source":["## 転移学習のメリット・デメリット\n","\n","**メリット**\n","\n","- ある領域(ドメイン)で学習したモデルを別の領域に適用するため、サンプル数が限定されている場合でも比較的高精度なモデルを構築できる\n","\n","  - 高品質なデータを大量に取得することは、コスト的・時間的に難しい場合が多い\n","  - 大量かつ高品質なデータによって学習した質の高いモデル・知識領域を転移させることで、限定的なデータであっても高精度なモデルを構築できる可能性がある\n","\n","- モデルを短時間で構成\n","\n","  - 事前学習済みネットワークを利用する場合は、0から学習する必要がないため学習時間・コストを短縮できる\n","\n","- シミュレーター環境で訓練したモデルを現実に適応させる\n","\n","- これらの背景にはすでに学んだが、DNNにおいては、タスク共通の主要な特徴があり、これを共通化できるという特徴を利用している\n","  - 特に画像認識などの領域ではスタイルトランスファーのように、大きくとらえる・細かくとらえるといった認識範囲や粒度の特徴をうまく活用できる可能性がある\n","\n","**デメリット**\n","\n","- 転移が必ず精度改善などよい結果を生むとは限らない\n","  - **負の転移(negative transfer)**と呼ばれる状況が発生しうる\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"udlnqFaFem_g"},"source":["# 転移学習の実際\n","\n","ここでは、代表的な事前学習済みネットワークベース転移学習について学ぶ\n","\n","ファインチューニング(全パラメタを再学習)と、一般的な転移学習(入れ替えた層だけ学習)の二つを試す\n","\n","PyTorchでは、Alexnet、VGG、ResNet、SqueezeNet、Inception v3などの代表的なネットワークが利用できる。\n","\n","ここでは、ImageNetで学習した1000クラスの分類モデルを用いて、ウルトラマンの中でも、ウルトラマンと帰ってきたウルトラマンほどではないが、それなりに難しい分類を行う。"]},{"cell_type":"code","metadata":{"id":"tHIYP6Jfem_g"},"source":["cuda = \"cuda:0\"\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.autograd import Variable\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import os\n","import time\n","import copy\n","import numpy as np\n","import matplotlib.pyplot as plt\n","device = torch.device(cuda if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eyfzgrSHbgbw"},"source":["今回分類するのは、ウルトラマンタロウとウルトラマンレオである。\n","\n","ウルトラマンタロウとウルトラマンレオを分類するモデルを学習するための**少数の**画像を入手する\n","\n","知らない人は目視でも見分けるのが難しいといわれるが、角に注目すると、横に伸びるのがレオ、縦に曲がってとがっているのがタロウ、特に太郎は真ん中の角も目立つ。その他、胸のパターンが違うなど、よく見るとかなり違う。\n","\n"]},{"cell_type":"code","metadata":{"id":"ZZcYc6Flem_h"},"source":["if not os.path.exists('ultra.tar.gz'):\n","  #!wget \"https://drive.google.com/uc?export=download&id=1Oo-YhK2FTuqKMAAWkjwcVFjSJL9sXB6p\" -O ultra.tar.gz\n","  !wget https://keio.box.com/shared/static/smgue95s7l3b5z1augf34p3ecqrm0qqw -O ultra.tar.gz\n","  !tar xzf ultra.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"37pA2-uYem_i"},"source":["画像認識では、ネットワークの前段の方で、人間のニューロンが備えるようなある固定の形状に特異的に反応するニューロンクラスタが生成されているのではないかという仮説があり、このことの実験的解析・証明も行われた\n","\n","- 画像からタロウとレオを分類するモデルを学習する\n","- レオは237枚、タロウはどちらかというと人気が高いので集まりやすく289枚ある。なお、手作業で仕訳けているので多少のミスが混入している可能性がある。\n","\n","フルスクラッチで学習するには不十分な枚数であるが、転移学習であれば十分可能な枚数である\n","\n","今回の目標は、\n","- 学習済みImageNetを転移学習する\n","- ImageNet自体も蟻とハチの分類が可能であるが、転移学習により、その分類精度を向上させる\n","  - 今回はタロウとレオしか最後に判定しない(他が判定できないようにする)\n","\n","なお、PyTorchのチュートリアルには、ハチと蟻の分類があり、これに関して多くの参考例が存在する\n","- この場合は、オリジナルの1000のクラスにハチと蟻が既に含まれているため、若干議論が必要になる\n","- 本来は1000のクラスに**含まれない**データを扱うべき\n","- 含まれているが、精度が向上すればよいという考えは次の疑問に対して答える必要がある\n","  - 1000のクラスがありました\n","  - ハチと蟻はそのまま正解するでしょう\n","  - ハチと蟻しかないのであるから、ハチと蟻以外の残り998クラスに分類された絵に対して、各クラスの画像がハチと蟻、どちらに近いかだけ与えてしまえば、精度は向上するというクイックハックが想定できる\n","  - それよりも向上した、ということが示されなければ、**転移学習のネットワーク組み換えにより本当によくなったといえないのではないか？**\n","\n","今回はウルトラマンなので、このような疑問を挟む余地はない\n"]},{"cell_type":"markdown","metadata":{"id":"MrXW2CvEem_i"},"source":["## 展開フォルダから画像をロード\n","\n","PyTorchのImageFolderを用いてデータをロードする\n","- 画像データはPIL形式で読み込む必要がある\n","  - 画像フォルダからデータをPIL形式で読み込むにはtorchvision.datasets.ImageFolderを利用する\n","  - 既に習得済みのtransform機能がImageFolderにも存在し、データ拡張を行う変換関数群を指定できる\n","  - クラス名のサブフォルダ(train/ants, train/bees)を作成しておくと自動的にクラス(ants, bees)を割り付けることができる\n","    - ラベルはフォルダの順番に0, 1, 2, ...と割り当てられる\n","\n","この関数は全画像データをまとめてメモリにロードするわけではないため、大量かつ巨大な画像ファイルがあっても問題ない\n","\n","まずは、試しにデータをよみだし、データ数を確認する"]},{"cell_type":"code","metadata":{"id":"LtKBM47Mem_j"},"source":["image_dataset = datasets.ImageFolder(root=\"ultra\")\n","image, label = image_dataset[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WccNz6N1em_j"},"source":["一番最初の画像と自動的に振られたラベル(ハチは0)を確認する\n","\n","**ウルトラマンが嫌いな人には本当に申し訳ない**\n","- 正解しているのか不正解なのかの判別ができないかもしれない。"]},{"cell_type":"code","metadata":{"id":"PtZGB5Lmem_j"},"source":["plt.figure()\n","plt.axis(\"off\")\n","plt.imshow(image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-O6EL5iAem_k"},"source":["## データ拡張\n","\n","PyTorchにはさまざまなデータ拡張機能があるが、今回以下の機能を利用する\n","- 既に紹介済みの機能も改めて紹介する"]},{"cell_type":"markdown","metadata":{"id":"HM5bVrPSem_k"},"source":["### RandomResizedCrop\n","\n","PIL画像をランダムなサイズとアスペクト比にクロップする\n","- 実行ボタンを押すと、毎回異なる部位が現れる"]},{"cell_type":"code","metadata":{"id":"RnGoRaTYem_k"},"source":["t = transforms.RandomResizedCrop(224)\n","trans_image = t(image)\n","plt.figure()\n","plt.axis(\"off\")\n","plt.imshow(trans_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bH-IeZ68em_l"},"source":["### RandomHorizontalFlip\n","\n","与えられたPIL画像を0.5の確率でランダムに水平反転させる"]},{"cell_type":"code","metadata":{"id":"QJAuwVyHem_l"},"source":["t = transforms.RandomHorizontalFlip()\n","trans_image = t(image)\n","plt.figure()\n","plt.axis(\"off\")\n","plt.imshow(trans_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"93Smzm2fem_l"},"source":["### Resize\n","\n","PIL画像を指定されたサイズにリサイズする"]},{"cell_type":"code","metadata":{"id":"hfSV3jKfem_l"},"source":["t = transforms.Resize((int(torch.rand(1).item()*200+50), int(torch.rand(1).item()*200+50)))\n","trans_image = t(image)\n","trans_image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3cUpoIZAem_l"},"source":["### CenterCrop\n","\n","PIL画像を中央でトリミングする\n","- 何度も押して動作を確認すると良い"]},{"cell_type":"code","metadata":{"id":"ir14sgnNem_m"},"source":["t = transforms.CenterCrop(int(torch.rand(1).item()*200+50))\n","trans_image = t(image)\n","trans_image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wClrIVfpem_n"},"source":["## データ変換関数の作成\n","以上を全部用いたデータ変換関数を準備する\n","- 既に述べたが、**データをロードする度に変換される**\n","\n","- 訓練時と評価時ではデータ変換関数を変更している\n","  - 訓練時は汎化性能が上がるように RandomResizedCrop や RandomHorizontalFlip などデータ拡張する変換を用いる\n","  - 評価時はランダム性は入れずに入力画像のサイズがネットワークに合うようにサイズを変形する\n","  - ImageNetでは大きめ(256x256)にリサイズした後、中心部分の224x224を切り出すという手法を用いているため、それに従う\n","- Normalize()でImageNetの訓練データの平均と分散を用いて入力画像の画素値を平均0、分散1に正規化する\n","  - ImageNetにおいて、転移学習など、学習済のパラメタを使うときは、この変換を施す\n","  - 新たに加える画像の画素分布になるべく引っ張られないようにする\n","\n","変換関数などについて、trainとevalで参照できるようにしている\n","- `data_transforms['train']()`:訓練画像用変換関数\n","- `data_transforms['val']()`: 評価画像用変換関数\n","  - ダウンロードしたデータセットが'train'と'val'という2つのフォルダに画像を保存しているため、これに倣っている\n","\n","image_datasets: 画像データセット\n","- フォルダから画像をImageFolderで読み込んで画像データセットを作成する\n","- ImageFolderの第2引数にデータ変換用の関数を指定する\n"]},{"cell_type":"markdown","metadata":{"id":"oX5EFT8wem_n"},"source":["## ハイパーパラメタ\n","\n","ここでは、バッチサイズとエポック数だけ指定する\n","\n","簡単に変更できるので試してみると良い"]},{"cell_type":"code","metadata":{"id":"bItBkR3bem_n"},"source":["batch_size = 64\n","num_epochs = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"atq-KT1Lem_n"},"source":["data_transforms = {\n","  'train': transforms.Compose([\n","    transforms.RandomResizedCrop(224),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","  ]),\n","  'val': transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","  ]),\n","}\n","\n","image_datasets = {x: datasets.ImageFolder(\"ultra\", data_transforms[x])\n","                  for x in ['train', 'val']}\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x],\n","                  batch_size=batch_size, shuffle=True, num_workers=4)\n","                  for x in ['train', 'val']}\n","dataset_sizes = {x: len(image_datasets[x])\n","                  for x in ['train', 'val']}\n","class_names = image_datasets['train'].classes\n","acc_list = {x: [] for x in ['train', 'val']}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"86aTEj7aem_n"},"source":["## 訓練画像の可視化\n","\n","- データ変換においてToTensor()でPyTorchのテンソル形式に変換されている\n","  - これまでもそうであったが、このままでは描画できない\n","  - そのでテンソルをnumpy()でndarrayに変換しなおしてから描画する\n","- 画素値を正則化しているため、逆演算(標準偏差を積算し平均を加算)し元に戻す\n","\n","既におなじみと思われるが、`images.size()`として、バッチサイズ、チャネル(RGB)、画素x、画素y、さらにクラスのサイズが表示される\n","- ここでは、バッチサイズ数の画像が並ぶ\n","- また、次の'classes.size()として、バッチサイズ分のラベル値が表示される\n"]},{"cell_type":"code","metadata":{"id":"LmPShkrjem_n"},"source":["def imshow(images, title=None, size=10):\n","  images = images.numpy().transpose((1, 2, 0))\n","  mean = np.array([0.485, 0.456, 0.406])\n","  std = np.array([0.229, 0.224, 0.225])\n","  images = std * images + mean\n","  images = np.clip(images, 0, 1)\n","  plt.figure(figsize=(size,size))\n","  plt.imshow(images)\n","  plt.axis(\"off\")\n","#  if title is not None:\n","#    plt.title(title)\n","images, classes = next(iter(dataloaders['train']))\n","print(images.size(), classes.size())\n","images = torchvision.utils.make_grid(images)\n","imshow(images, title=[class_names[x] for x in classes])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YuetF3ztem_o"},"source":["一応確認すると、画像の分類に対応する1次元テンソルが表示される"]},{"cell_type":"code","metadata":{"id":"XurHxma5em_o"},"source":["classes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2no_dAgQem_o"},"source":["## 訓練用関数定義\n","\n","- 各エポックは訓練`trainとバリデーションデータに対する評価`val`を行う\n","  - 一般にそれぞれ別に記述するが、共通部分が多いためまとめた記述となっている\n","- PyTorchのloss関数はデフォルト(size_average=True)では、ミニバッチのサンプルあたりの平均lossを返す\n","  - 実際その値を用いて逆伝播が計算される\n","  - running_loss はミニバッチの平均lossをミニバッチのサンプル数倍し、これを全部足し集めた後、全サンプル数で割ることでサンプル当たりの平均ロスとしている\n","    - これまで通り、ミニバッチ毎や、エポック毎といった評価でも問題ない\n","\n","これまでとそれほど変わらないが、バリデーションデータでよい精度のモデルができるたびに自動的そのモデルを保存し、最終的に最高精度をたたき出すモデルを返す\n"]},{"cell_type":"code","metadata":{"id":"A58KddGBem_p"},"source":["def train_model(model, criterion, optimizer, scheduler, num_epochs=num_epochs):\n","  since = time.time()\n","  epoch_loss = 0.0\n","  epoch_acc = 0.0\n","  best_model_wts = copy.deepcopy(model.state_dict())\n","  best_acc = 0.0\n","  for epoch in range(num_epochs):\n","    print('Epoch {}/{}'.format(epoch, num_epochs - 1))  \n","    # 各エポックで訓練+バリデーションを実行\n","    for phase in ['train', 'val']:\n","      if phase == 'train':\n","        scheduler.step()\n","        model.train()\n","      else:\n","        model.eval()\n","      running_loss = 0.0\n","      running_corrects = 0\n","      for data in dataloaders[phase]:\n","        inputs, labels = data\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","        # 順伝播\n","        if phase == 'train':\n","          outputs = model(inputs)\n","        else:\n","          with torch.no_grad():\n","            outputs = model(inputs)\n","        _, preds = torch.max(outputs.data, 1) #_は無視、データを捨てる\n","        loss = criterion(outputs, labels)\n","        if phase == 'train':\n","          loss.backward()\n","          optimizer.step()\n","        running_loss += loss.data.item() * inputs.size(0)\n","        running_corrects += torch.sum(preds == labels.data)\n","      epoch_loss = running_loss/dataset_sizes[phase]\n","      epoch_acc = running_corrects.item()/dataset_sizes[phase]\n","      print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","      # よい精度のモデルを自動的に保存する、よくあるテクニック\n","      acc_list[phase].append(epoch_acc)\n","      if phase == 'val' and epoch_acc > best_acc:\n","        best_acc = epoch_acc\n","        best_model_wts = copy.deepcopy(model.state_dict())\n","  time_elapsed = time.time() - since\n","  print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed / 60, time_elapsed % 60))\n","  print('Best val acc: {:.4f}'.format(best_acc))\n","  model.load_state_dict(best_model_wts) #もっともよいモデルを読み直す\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kLI18Tevem_p"},"source":["## 学習済みモデルの読み込みとFine-tuning\n","\n","学習済みの大規模なネットワークとしてResNetを選択する\n","- 出力層部分のみ2クラス分類になるように置き換えて、重みを固定せずに新規データで全層を再チューニングする方針を選択する\n","\n","学習済みのResNet18をロードする"]},{"cell_type":"code","metadata":{"id":"aReD8k7-em_p"},"source":["model_ft = models.resnet18(pretrained=True)\n","model_ft"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wTfvjdBMem_q"},"source":["本格的なResNetの構造を見ると流石に巨大である\n","\n","最終層の`fc`は出力がImageNetの1000クラス分類であるため、1000の出力がある\n","  - この部分を2クラスに置き換えればアリとハチの分類に利用できる\n","\n","次のようにして置き換える\n","- 置き換え方は直観的にわかる通り、モデルのメソッドfcを指定しなおすだけ\n","\n","ネットワークを表示させて、最後が置き換わっていることを確認する"]},{"cell_type":"code","metadata":{"id":"fbLCtMTlem_q"},"source":["num_features = model_ft.fc.in_features\n","# fc層を置き換える\n","model_ft.fc = nn.Linear(num_features, 2)\n","model_ft"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VI4EIOdMem_q"},"source":["model_ft = model_ft.to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n","\n","# 7エポックごとに学習率を0.1倍する\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n","model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=num_epochs)\n","torch.save(model_ft.state_dict(), 'model_ft.pkl')\n","acc_list_ft = acc_list['val'].copy()\n","acc_list['val'] = []"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RMQ-DLv5em_q"},"source":["## 学習済パラメタを固定\n","\n","先ほどは重みを固定せずにResNet18の全レイヤの重みを更新対象にしていた\n","- その結果をメモしておこう\n","\n","ここでは、全体をFine-tuningせず、最終層だけ重みを調整する\n","\n","次のようにする\n","- `require_grad = False` として重みを固定する\n","- `optimizer`に更新対象のパラメータのみ渡す\n","  - `model_conv.parameters()`といった具合に固定パラメータを含めるとエラーとなる\n","\n","先ほどよりも速く学習が進む\n","- GPUの威力で、それなりに速く求めてしまう……\n","- backwardの勾配計算を最終段のみ計算すればよいため\n","- 但し、lossを計算する必要があることから、forwardはすべて計算する"]},{"cell_type":"code","metadata":{"id":"Tmrr5ASdem_q"},"source":["# 訓練済みResNet18をロード\n","model_conv = torchvision.models.resnet18(pretrained=True)\n","# 全パラメータを固定\n","for param in model_conv.parameters():\n","  param.requires_grad = False\n","# 最後のfc層を置き換える(requires_grad=Trueでありパラメータ更新の対象)\n","num_features = model_conv.fc.in_features\n","model_conv.fc = nn.Linear(num_features, 2)\n","model_conv = model_conv.to(device)\n","criterion = nn.CrossEntropyLoss()\n","# Optimizerの第1引数には更新対象のfc層のパラメータのみ指定\n","optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7n2F18hHem_q"},"source":["model_conv = train_model(model_conv, criterion, optimizer_conv,\n","                       exp_lr_scheduler, num_epochs=num_epochs)\n","acc_list_tr = acc_list['val'].copy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sk4PcTefem_r"},"source":["先程よりもよくなったであろうか？\n","\n","- バッチサイズを変える\n","- 最適化手法を変える\n","\n","などして、どちらがよくなるか、調べてみると良いであろう\n","\n","なお、今回は10エポック程度でも十分であったといえるが、このように学習速度も速い"]},{"cell_type":"code","metadata":{"id":"zLLTQ609em_r"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","plt.figure()\n","plt.plot(range(num_epochs), acc_list_ft, label='FT Loss')\n","plt.plot(range(num_epochs), acc_list_tr, label='TR Loss')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.grid()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UnAM4DJXem_r"},"source":["## 分類結果の可視化"]},{"cell_type":"code","metadata":{"id":"CR7wIyfTem_r"},"source":["model_ft.load_state_dict(torch.load('model_ft.pkl', map_location=lambda storage, loc: storage))\n","def visualize_model(model, num_images=6):\n","  images_so_far = 0\n","  fig = plt.figure()\n","  model.eval()\n","  for i, data in enumerate(dataloaders['val']):\n","    inputs, labels = data\n","    inputs = inputs.cuda().to(device)\n","    inputs.requires_grad = False\n","    labels = labels.cuda().to(device)\n","    labels.requires_grad = False\n","    with torch.no_grad():\n","      outputs = model(inputs)\n","    _, preds = torch.max(outputs.data, 1)\n","    for j in range(inputs.size()[0]):\n","      images_so_far += 1\n","      plt.subplot(num_images // 2, 2, images_so_far)\n","      plt.axis('off')\n","      plt.title('predicted: {}'.format(class_names[preds[j]]))\n","      imshow(inputs.cpu().data[j], size=4)\n","      plt.show()\n","      if images_so_far == num_images:\n","        return\n","visualize_model(model_ft)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W8FnNt3oem_r"},"source":["学習をGPUマシン行い、可視化や分析をCPUで行うことはよくある\n","\n","最初にCPU用とGPU用、両方のモデルを保存するという方法があるが、上記のようにmap_locationを記述することで変更できる\n","\n","その他の変更ルールは次を参考のこと\n","\n","- CPU - > CPUやGPU - > GPU\\\n","`torch.load('p.pth'）`\n","\n","- CPU - > GPU1\\\n","`torch.load('p.pth', map_location = lambda storage, loc: storage.cuda(1))`\n","\n","- GPU1 - > GPU0\\\n","`torch.load('p.pth', map_location = {'CUDA:1':'CUDA:0'})`\n","\n","- GPU - > CPU\\\n","`torch.load('p.pth', map_location = lambda storage, loc storage)`"]},{"cell_type":"markdown","metadata":{"id":"B8I8lP1cem_r"},"source":["新しいレイヤを作成して代入するだけなのですごく簡単にできる\n","- 結果的に2クラス分類である\n"]},{"cell_type":"markdown","metadata":{"id":"wuyMjsJ5eJ0T"},"source":["# 演習課題\n","\n","次のどちらかの課題に取り組みなさい\n","\n","- 2クラス分類であるから、出力層のユニット数を1にして活性化関数をsigmoid、Loss関数をbinary cross entropyにしてもよいだろうか、実際に実装して確認しなさい\n","\n","- イノシシと黒豚、月とスッポン、鳶と鷹など、似ていて非なるモノを用いて同様に分類しなさい\n","  - データセットは自分で作成すること\n","  - 判別率が65%を超えれば何でもい\n","  - 一般に似て非なるものを扱うこと"]}]}