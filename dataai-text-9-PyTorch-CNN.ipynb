{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dataai-text-9-PyTorch-CNN.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"oQetb06aZwML"},"source":["#@title Data-AI（必ず自分の名前・学籍番号を入力すること） { run: \"auto\", display-mode: \"form\" }\n","\n","import urllib.request as ur\n","import urllib.parse as up\n","Name = '\\u591A\\u7530 \\u5DEB\\u5973\\u7F8E' #@param {type:\"string\"}\n","EName = 'Tata Mikomi' #@param {type:\"string\"}\n","StudentID = '87654321' #@param {type:\"string\"}\n","Addrp = !cat /sys/class/net/eth0/address\n","Addr = Addrp[0]\n","url = 'https://class.west.sd.keio.ac.jp/classroll.php'\n","params = {'class':'dataai','name':Name,'ename':EName,'id':StudentID,'addr':Addr,\n","           'page':'dataai-text-9','token':'27652099'}\n","data = up.urlencode(params).encode('utf-8')\n","#headers = {'itmes','application/x-www-form-urlencoded'}\n","req = ur.Request(url, data=data)\n","res = ur.urlopen(req)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wz9sZYV_mPPk"},"source":["---\n","> 畳み込むの意味は、折りたたんで中に入れること、心の中に深く留めること、工学的には移動しながら共有部分の和を求めていくこと\n","> \n","> 畳み込み演算に意味ができることの本質は線形で時不変であること、受けた影響の総和が求まるということ\n",">\n","> RC回路に方形波を入力する時その出力を、RC回路のインパルス応答曲線と、方形波との積分を、方形波を移動させながら求めて応答を得るのが畳み込み積分\n",">\n","> 源画像データとフィルタ行列とのアダマール積を移動しながら求めるのが畳み込み層\n",">\n","> この授業も教師曲線と学生曲線の畳み込み値が大きくなるようにしたい\n",">\n","> それ以前に線形で時不変であることが難しいのだが\n","---"]},{"cell_type":"markdown","metadata":{"id":"dKyntKyzCmyb"},"source":["# 畳み込み層とプーリング層"]},{"cell_type":"markdown","metadata":{"id":"1_0GQiZNW_9V"},"source":["## 畳み込み層について\n","\n","畳み込みニューラルネットワーク、Convolutional Neural Network (CNN)については既に説明済みである\n","\n","特に画像に用いるが、画像など2次元データから特徴を効率よく集めることができる仕組みといえる\n","\n","CNNについての本質的な理解は、マルチメディアデザインの授業を受けた学生は画像フィルタリングの動作原理を理解していれば掴みやすいであろう\n","- 画像とオペレータのアダマール積により、エッジ抽出や線分抽出などの画像フィルタが記述できる\n","- オペレータを工夫すれば、他にも様々な特徴が抽出できることはマルチメディアデザインの授業でも紹介した通りである\n","\n","なお、後で確認できるが、画像に対して全結合網で次元削減しながら学習させるのと、CNNを使うのでは明確にCNNの方が性能がよくなる\n","\n","なお、次の図のように複数のフィルタを同時に用いることが多い\n","- この図において情報の次元数であるチャネル数の移り変わりについてまとめる\n","- 画像の画素の位置(x,y)の2次元の情報のチャネルに加えてRGBの色情報チャネルを含む3チャネルあるが、同数の3枚のフィルタが必要となる\n","- フィルタと畳み込み演算を行ったRGB各チャネルのデータを合計して1チャネルのデータとする\n","- バイアスを与えて活性化関数による演算を施す\n","- 結果としてフィルタ数分のチャネルの生成データが得られる\n","  - 画像としてみることもできるが、画像と呼べるかどうかは微妙であるため生成データとここでは表現している\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/cnnch.png\" width=\"60%\">\n","\n","畳み込み層の処理は、2次元だけでなく、1次元や3次元にも拡張でき、PyTorchはネイティブでconv1d, conv2d, conv3dを備える\n","- conv4dなどもGitHubなどで利用できる\n","\n","オプションは次の通り\n","\n","| | |\n","|:--|:--|\n","|in_channels (int)| 入力チャネル数、最初の層なら通常RGBで3 |\n","|out_channels (int) | 出力チャネル数でフィルタ数 |\n","|kernel_size (int or tuple) |各フィルタカーネルのサイズ|\n","|stride (int=1 or tuple, optional)|ストライド|\n","|padding (int=0 or tuple, optional)|パディング幅|\n","|padding_mode (string='zero', optional)|多次元テンソルに対するパディング層の考慮に関する内容で指定<br>することはないであろう<br>なおpadというパディング専用の柔軟な関数も準備されている|\n","|dilation (int=1 or tuple, optional)|計算したカーネルの値を並べる際のストライド|\n","|groups (int=1, optional)| 入力から出力へのカーネルの分割ブロック数で並列化の指定<br>一般にgroup convolutionとは異なる複数のカーネルを一つの入力に<br>適用することであるが、その文脈では定義されていない|\n","|bias (bool=1, optional)|True=1であれば、学習可能なバイアスを追加する|\n"]},{"cell_type":"markdown","metadata":{"id":"PQ3rndD7CrtP"},"source":["## プーリング層について\n","\n","こちらも既に説明済みであるが、ある領域の最大や平均といった値を取得してさらにデータサイズを縮約する\n","- 一般的には最大プーリングを利用する\n","- 平均プーリングは2016年に提案された画像物体認識DNNであるLeNetで利用されたが、普通利用しない\n","- ややこしいが、ResNetでは全平均を計算するglobal average poolingが用いられる \n","\n","画像では一般的にmax poolingが用いられ、最も特徴的な値のみを次に伝えるという効果が得られる\n","\n","pooling層なしにCNNを重ねてもよいのではないか？という考えもあると思うが、計算コストを削減する意味でCNNとpoolingを交互に重ねるような構成が多くみられる\n","- どのみち特徴を抽出するという明確な目的においては、特徴がある、つまり値の大きいところを選び出せばよいと考えられる\n","- poolingの方が演算コストが小さいことから、抽出するという処理であればpoolingを用いることで学習に必要な計算コストを削減できる\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fD70YILzKJi2"},"source":["## ストライドについて\n","\n","これも既に説明済みであるが、一般に畳み込み層ではストライドは1となり画像サイズに大きな差が出ない\n","\n","プーリングではプーリング用のフィルタサイズに合わせてストライドが設定される\n","- つまり、プーリングの各演算範囲が重ならないようにずらす\n","- 最大値が欲しいのであるから、重なってはいけない\n","  - 重なると同じ最大値が複数回結果に表れてしまい、特徴が重複・強調される\n"]},{"cell_type":"markdown","metadata":{"id":"vUIH4Bp8LNtP"},"source":["## 出力データのサイズについて\n","\n","次のように変数を定める\n","- 入力画像高さ,幅: $I_h$, $I_w$\n","- フィルタ高さ,幅: $F_h$, $F_w$\n","- パディング幅: $D$\n","- ストライド幅: $S$\n","\n","すると、出力画像の高さ$O_h$と幅$O_w$は次のように表される\n","$$\n","O_h = \\frac{I_h-F_h+2D}{S}+1\\\\\n","O_w = \\frac{I_w-F_w+2D}{S}+1\n","$$\n","\n","例:畳み込み層\n","- 入力画像: 32$\\times$32、RGB 3チャネル\n","- フィルタ: 5$\\times$5, フィルタ数6\n","- パディング: なし\n","- ストライド: 1\n","\n","このとき、出力データは、\n","$(32-5+2\\times 0)/1+1=28$となる\n","\n","すなわち、6チャネルの28$\\times$28のデータとなる\n","\n","例:プーリング層\n","上記データを入力とする\n","- 入力画像: 28$\\times$28、6チャネル\n","- 領域サイズ: 2$\\times$2\n","\n","このとき、出力データは、\n","$20/2=14$となる\n","\n","すなわち、6チャネルの14$\\times$14のデータが得られる\n","\n","全体をまとめると次の通りとなる\n","\n","| | | | |\n","|:-:|:-:|:-:|:-:|\n","| |**CONV**|**POOL**|**FC**|\n","|図|<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/table-conv.png\" width=\"150\">|<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/table-pool.png\" width=\"150\">|<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/table-fc.png\" width=\"150\">|\n","|入力サイズ|$I \\times I \\times C$|$I \\times I \\times C$|$N_{\\text{in}}$|\n","|出力サイズ|$O \\times O \\times K$|$O \\times O \\times C$|$N_{\\text{out}}$|\n","|パラメータの数|$(F \\times F \\times C + 1) \\cdot K$|$0$|$(N_{\\text{in}} + 1 ) \\times N_{\\text{out}}$|\n","|備考|•フィルタ毎にバイアスがある<br>•一般に$S<F$<br>•一般に$K=2C$|•プール操作はチャネル毎<br>•一般に$S+F$|•ニューロン毎にバイアス1つ<br>•ニューロン数に構造的制約なし|\n"]},{"cell_type":"markdown","metadata":{"id":"hGkPZzkmNDDo"},"source":["## 畳み込み層でよく利用される活性化関数\n","\n","ReLUが一般的に用いられるが、次のような活性化関数が利用される\n","- 悩むぐらいならReLUで十分\n","\n","| | | |\n","|:-:|:-:|:-:|\n","|ReLU|Leaky ReLU|ELU|\n","|$g(z)=\\max(0,z)$|$g(z)=\\max(\\epsilon z,z)$<br>ただし$\\epsilon\\ll1$|$g(z)=\\max(\\alpha(e^z-1),z)$<br>ただし$\\alpha\\ll1$|\n","|<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/figrelu.png\" width=\"150\">|<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/figleaky-relu.png\" width=\"150\">|<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/figelu.png\" width=\"150\">|\n","|•生物学的に解釈可能な構成かつ非線形|•ReLUの負値不感問題に対処|•どこでも微分可能|\n"]},{"cell_type":"markdown","metadata":{"id":"Ni3cyNWeO4XR"},"source":["# データ拡張(data augmentation)による汎化性能向上\n","\n","データ拡張とは、汎化性能を向上するためにデータを水増しすることである\n","\n","- 例えば、訓練データのサンプル数が少ない場合などにおいては、過学習が進みやすく汎化性を失う\n","\n","- その単純な対策の一つが、**データを増やす**ということ、これがデータ拡張の意味である"]},{"cell_type":"markdown","metadata":{"id":"ogMdiuSnymL_"},"source":["## 改めて過学習とはどういう状態か\n","\n","既に何度か扱っているので、ここでは一般的な説明とするが、欲しいモデルとは異なる、あるデータだけに特化したモデルが構築された状態のこと\n","\n","学習曲線、検証曲線、ロス曲線において、学習データのロスとテストデータのロスの間が開いている状態を過学習状態と呼ぶ\n","\n","- ただし、学習があまり進んでいない状態で開いている場合は、モデル構築が不十分で、未学習(アンダーフィッティング)というべき状態であり、過学習(オーバーフィッティング)とは言わない\n","\n","- 学習がある程度進んだ状態で呼ぶ\n","\n","- 学習が比較的早く進むadamを用いて比較的簡単なモデルを多めのエポック学習させてみるとよい\n","\n","重要なので、改めてまとめる\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/losscurve.png\" width=\"60%\">\n"]},{"cell_type":"markdown","metadata":{"id":"0ZS_xceeypHO"},"source":["## 実際のデータ拡張\n","\n","画像データでは次のような方針がとられる\n","- 画像を様々な方向に回転させる\n","- 画像を拡大・縮小する\n","- 画像の平行移動\n","- 画像の上下・左右の反転\n","- 画像の一部を削除する(隠す)"]},{"cell_type":"markdown","metadata":{"id":"fX42gAqeE87N"},"source":["# ドロップアウト(dropout)\n","\n","具体的に何をするかは説明済みであるので、その用い方のテクニックについて述べる\n","\n","- 出力層以外の通常ノードを一定確率でランダムに消去する\n","- 実装が容易で過学習を抑制でき、またその効果が大きい\n","- バッチごとに消去する結合を変更する\n","\n","また、\n","\n","- ドロップアウト率$p$に基づきノード単位でドロップアウトするかどうかを決定する\n","\n","- CNNでは用いてはいけない\n","\n","  - 重要な特徴を捕まえられなくなる\n","  - そもそもCNNはものすごい勢いで情報圧縮・次元圧縮している\n","\n","- 解釈としては、ランダムフォレストで利用したアンサンブル学習と同じ効果が期待できる\n","\n","  - Dropoutによりスリム化されたネットワークを組み合わせると解釈できる\n","\n","Dropoutしても安定して学習できるようになってしまった、そういう技術ができてしまったということ\n","\n","近年では、画像処理応用が旺盛で、CNNではdropboutを用いないため、あまり利用されていない\n","\n","- Dropoutさせた方が学習で最適解に至る速度は低下する\n","\n","- ならば、ちゃんと監視して過学習に至る前に学習をやめればよいという意見もある"]},{"cell_type":"markdown","metadata":{"id":"4ZEtNfbZG3SR"},"source":["# バッチノーマライゼーション(Batch Normalization)\n","\n","Batch Normalization(以下BN)はシンプルでありながら、\n","\n","> 特にGAN等安定化しにくいネットワークが利用可能になったのは、Batch NormalizationとAdamによる功績が大きく、なければノイズしか出てこない\n","\n","といわれており、また、今後VAEについても学ぶが、\n","\n","> VAEでも、うまくいかなかった事例が、Batch Normalizationを入れるといきなり学習が成功した\n","\n","という事例も報告されている\n","\n","BNはもともと、勾配消失・爆発を防ぐための手法として提案され、そのための対策として、\n","- 活性化関数をReLUなどに変更\n","- ネットワークの重みの初期値を事前学習しておく\n","- 学習係数を下げる(遅くなる)\n","- ネットワークの自由度を下げる\n","  - Dropoutもこの手法の一つ\n","\n","といった対応が取られてきたが、これに対する新たな対応手法である"]},{"cell_type":"markdown","metadata":{"id":"fa10xw3pytak"},"source":["### バッチノーマライゼーションの処理内容\n","\n","BNとは、各ユニットの出力をminibatchごとにnormalizeした新たな値で置き直すことで、内部の変数の分布(内部共変量シフト)が大きく変わるのを防ぎ、学習を進め、過学習を抑える手法である\n","- 要するに、ネットワークの途中で伝播している値を無理やり正規化する手法\n","- 特に**白色化(whitening)**と呼ばれる手法では、平均0、標準偏差1に途中変換すればニューラルネットワークの収束が速くなることが知られている\n","- ネットワークの層の中に$\\boldsymbol{H}$という行列を配置する\n","  - 各行がminibatchの1つのデータ、各列がそれぞれのactivationとなるような値をとる行列\n","  - 例えばbatch sizeが128, hidden unitの数を256とすると、  $\\boldsymbol{H}$ は128×256の行列\n","\n","正規化のため、次のように$\\boldsymbol{H}'$ と置き直すのがbatch normalizationの仕組み\n","\n","$$\n","\\boldsymbol{H}' = \\frac{\\boldsymbol{H}-\\boldsymbol{\\mu}}{\\boldsymbol{\\sigma}}, \n","$$\n","\n","- $\\boldsymbol{\\mu}$と$\\boldsymbol{\\sigma}$ は、その層での各ユニットの平均および標準偏差のベクトル\n","\n","訓練時の$\\boldsymbol{\\mu}$と$\\boldsymbol{\\sigma}$ は次の式を用いて算出する\n","\n","$$\n","\\boldsymbol{\\mu} = \\frac{1}{m} \\sum_i \\boldsymbol{H}_{i,:}, \\\\\n","\\boldsymbol{\\sigma} = \\sqrt{\\delta + \\frac{1}{m} \\sum_{i}(\\boldsymbol{H}-\\boldsymbol{\\mu})^2_i}\n","$$\n","\n","- $\\delta$ は標準偏差が0にならないようにする十分小さな値\n","\n","推定時は、minibatchはないため、訓練時に利用した値の移動平均などを$\\boldsymbol{\\mu}$と$\\boldsymbol{\\sigma}$に適用する"]},{"cell_type":"markdown","metadata":{"id":"ZDMSNtcvyy80"},"source":["### バッチノーマライゼーションの種類\n","\n","処理方法は同一であるが、1度に正規化する範囲の違いから次のような種類がある\n","- バッチノーマライゼーション：ミニバッチの同一箇所(BachNormalization演算が割り当てられている同じ層)の間で正規化する\n","- レイヤーノーマライゼーション：異なるレイヤ(チャネル)全体で正規化する手法で、RNNやTransformerで用いられる\n","- グループノーマライゼーション：異なるいくつかのレイヤ(チャネル)で正規化する手法で、画像認識で用いられることがある\n","- インスタンスノーマライゼーション：1つのレイヤチャネルだけで正規化する手法で、StyleGANなどのGANで用いられる"]},{"cell_type":"markdown","metadata":{"id":"LENQpIP7y2Mh"},"source":["### なぜバッチノーマライゼーションが有効なのか\n","\n","まず、誤ってはいますが、現論文に記載されており、皆さんが信じた理由が、内部共変量シフト(= ICS, Internal Covariance Shift)の解消です\n","- ニューラルネットの層を過ぎる毎にその層のアウトプットの分布が変わる(これをICSと呼ぶ)が、このようにニューラルネットの中を通るデータの分布が頻繁に変化すると学習が困難になるというのは感覚的に理解できるであろう\n","- であれば、バッチノームはこのICSを正規化により解決し、精度向上に寄与すると考えられる\n","\n","しかしながら、実際には調べたところBNではICSは解決されないため、この主張は誤っているであろうというのが現在の見解です\n","- ですが、この考え方には大きなヒントがあります\n","- ある重みを更新する際に、本来その前の各層の重みがどのように更新されるのかを考慮する必要がある\n","- BNを用いることで、その前の全体の層の関係を表現た値を利用できる\n","というのが、現在の解釈となる"]},{"cell_type":"markdown","metadata":{"id":"22JpWNNcy5rA"},"source":["### BN利用のメリット\n","- 学習係数を大きくすることができる\n","  - 学習係数を増加させるとパラメタの大きさが飛躍的に増え、勾配消失や勾配爆発につながるため大きくできなかった\n","  - 途中で正規化すればパラメタが飛躍的に大きくなることを防ぐことができる\n","\n","- 正則化効果がある\n","  - L2正則化の必要性がほぼなくなる\n","  - Dropoutの必要性がほぼなくなる\n","    - 近年ではDropoutがあまり利用されていないが、これはBNの影響によるところが大きい\n","    - Dropoutは学習速度が低下するが、BNは逆に向上するので利用価値が薄れてしまった\n","- ネットワークの重みの初期値の影響を受けにくい\n","\n","性能で悩んだら、まずはBNの利用を検討するべきである\n","\n","PyTorchでは、画像の場合`BatchNorm2d`が提供されている"]},{"cell_type":"markdown","metadata":{"id":"r700Fih6XEEB"},"source":["# MNISTのCNNによる分類\n","\n","今まで何度も取り扱ってきたMNISTを改めてCNNを用いて評価する\n","\n","プログラムも理解が進んでいると思うので、より行儀のよいスタイルにする"]},{"cell_type":"markdown","metadata":{"id":"xvq4OockXMVn"},"source":["GPU対応の確認を行う\n","- PyTorchはGPUモードを明示的に指定する"]},{"cell_type":"code","metadata":{"id":"n0I0PIEjXFoF"},"source":["cuda = \"cuda:0\"\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","# ハイパーパラメータ \n","num_epochs = 10\n","batch_size = 100\n","learning_rate = 0.001\n","# GPU存在のチェック\n","device = torch.device(cuda if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Yn6-SWBXP_8"},"source":["MNISTを読み込むDataSetとDataLoaderを作成する"]},{"cell_type":"code","metadata":{"id":"cQ4p5FLoXND-"},"source":["# MNISTデータセット\n","train_dataset = dsets.MNIST(root='mydata', \n","                            train=True, \n","                            transform=transforms.ToTensor(),\n","                            download=True)\n","test_dataset = dsets.MNIST(root='mydata', \n","                           train=False, \n","                           transform=transforms.ToTensor())\n","# DatasetLoader\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n","                                           batch_size=batch_size, \n","                                           shuffle=True)\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n","                                          batch_size=batch_size, \n","                                          shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t7oQZqmgXUQh"},"source":["## モデルの作成\n","\n","- CNNはブロック単位で処理した方がよいのでブロック（Conv+BN+ReLU+Pooling）ごとにまとめて Sequential を使うとわかりやすくなる\n","- Conv2d や BatchNorm2d は出力のユニットサイズをきちんと指定する\n","- サイズを自分で計算するのが面倒ならば、モデルの途中結果サイズを`print(out.size())`や`print(out.shape)`として出力して確認するとよい\n","  - ここではそのようにして調べた\n","- ネットワークの指定は画像1枚に対して記述しているように見えるが、実際にはバッチ数分並列して動作するため、推定結果もバッチ数分出力されることに注意する\n","  - つまり、画像一枚に対するノード数が、さらにバッチ数分一挙に出力される\n","\n","このサイズの求め方は面倒なので、printしてみて納得したら、`out.size(0)`のように指定すると若干？お手軽である\n","- この場合`print(out.shape)`では`torch.Size([100, 32, 7, 7])`と表示される\n","- 先頭の100はバッチサイズで、これが`out.size(0)`である\n","  - ネットワークは内部では100個並列で作成され、推定結果も100個並列して出てくる\n","  - それが全て配列になって保存される、一つの画像当たり$32\\times 7\\times 7$の数のノードの出力が得られる\n","- 最終的には100個の並列した推定結果が欲しいので、`torch.Size([100,1568])`となってほしい\n","  - そこで、ノードの個数がわかるのならば、バッチサイズを未確定として`view(-1,1568)`としてもよいし、バッチサイズがわかるのであればノード数を未確定として`view(100,-1)`としてもよい\n","  - バッチサイズは、`torch.Size([100, 32, 7, 7])`なので、`out.size(0)`で得ることができ、`view(out.size(0), -1)`が成立する\n","\n","後半のCIFAR-10では畳み込み層の出力サイズを求める計算式を示して計算してサイズを得るため、こちらのノードサイズを未確定とする例と違いバッチサイズを未確定としている\n","- どちらも同じであるが、`out.size(0)`を用いる方がお手軽である\n","- とはいえ、結局ネットワークの定義で、`nn.Linear(7 * 7 * 32, 10)`とする必要があるため、ノード数の情報は必要となることから、お手軽さは限定的"]},{"cell_type":"code","metadata":{"id":"z_ZJpo72XRsl"},"source":["class CNN(nn.Module):\n","  def __init__(self):\n","    super(CNN, self).__init__()\n","    self.layer1 = nn.Sequential(\n","      nn.Conv2d(1, 16, kernel_size=5, padding=2),\n","      nn.BatchNorm2d(16),\n","      nn.ReLU(),\n","      nn.MaxPool2d(2))\n","    self.layer2 = nn.Sequential(\n","      nn.Conv2d(16, 32, kernel_size=5, padding=2),\n","      nn.BatchNorm2d(32),\n","      nn.ReLU(),\n","      nn.MaxPool2d(2))\n","    self.fc = nn.Linear(7 * 7 * 32, 10)\n","  def forward(self, x):\n","    out = self.layer1(x)\n","    out = self.layer2(out)\n","    out = out.view(out.size(0), -1)\n","    out = self.fc(out)\n","    return out\n","model = CNN().to(device)\n","model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mu5kgxivXbN4"},"source":["# テスト\n","images, labels = iter(train_loader).next()\n","print(images.size())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4qD7DSn5X0uU"},"source":["今回のモデルのように、Batch NormalizationやDropoutなど学習時と推論時で挙動が変わるレイヤを使う場合はモデルのモードに注意して、きちんと切り替えること\n","- model.train() で訓練モード\n","- model.eval() で評価モード\n","  - 切り替えなくてもエラーにはならないがそもそも結果に影響するなどの弊害が発生する\n","- torch.no_grad()も忘れずに\n","  - こちらは忘れてもよいが、特にGPUでは処理速度やメモリ効率の影響が大きくなる\n","また、GPUモードで動かすときはテンソルを下のように to(device) でGPUに送る必要がある"]},{"cell_type":"markdown","metadata":{"id":"f1RNR4O-XhQ6"},"source":["モデルオブジェクトを生成する\n","\n","GPUモードで動かすには\n","\n","- モデルを to(device) でGPUに転送する\n","- テンソルデータも to(device) でGPUに転送する\n","\n","の2つだけ実装すればよい"]},{"cell_type":"code","metadata":{"id":"AjzzBQ6HX7a2"},"source":["images = images.to(device)\n","labels = labels.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qi3zKTDHXetG"},"source":["model = CNN().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MCu334FjXwsu"},"source":["## 訓練\n","\n","今後テンプレ的に使えるよう、ここでは精度も計算するが、精度を求めるのは若干工夫がいる\n","- **一気にまとめて調べ上げる方法**で普通によく見る記述\n","- まずモデルの出力が全検証のそれぞれのテンソルを持つ\n","- そこで、各テンソルの最大値を集めれば、それが予測ラベル\n","  - `predicted = outputs.max(1, keepdim=True)[1]`\n","- view_asはtensorとtensorを比較する時などで形を揃えたい時に便利な関数で、型をsize()で調べてviewで変換するなどとしなくても変換できる\n","  - viewは、直接、型を指定するが、view_asは他のテンソルの型に倣って変換してくれる\n","    - `labels.view_as(predicted)`でlabelテンソルをpredictedテンソルの形に変換\n","    - eqで複数を同時に比較、それぞれでTrueかFalseを得る\n","    - そのうちのTrueの数をsumで数える\n","    - Trueは1で、Falseは0扱いなので、sumするとTrueを数えるのと同じ\n","    - item()でPythonの普通の数字として取り出す\\\n","    これを忘れると、`correct/total`の計算がtensorの割り算となりうまくいかない\n","    - これをcorrectに加える\n","\n","- 別の方法として、**一つの結果であれば**`correct += (outputs.argmax(1) == labels).sum().item()`とすることもできる\n","  - 中身は同じだがargmaxで最大値となる**場所**を取得してそれがラベルと一致すれば正解、これを合計して、pythonの数字として取り出している\n","  - ただし**一つの結果だけ**であり、**全結果まとめて計算しているわけではない**ことに注意する"]},{"cell_type":"code","metadata":{"id":"zWWGgy6_XyZi"},"source":["def train(train_loader):\n","  model.train()\n","  running_loss = 0\n","  for batch_idx, (images, labels) in enumerate(train_loader):\n","    images = images.to(device)\n","    labels = labels.to(device)\n","    optimizer.zero_grad()\n","    outputs = model(images)\n","    loss = criterion(outputs, labels)\n","    running_loss += loss.item()\n","    loss.backward()\n","    optimizer.step()\n","  train_loss = running_loss / len(train_loader) \n","  return train_loss\n","\n","def valid(test_loader):\n","  model.eval()\n","  running_loss = 0\n","  correct = 0\n","  total = 0\n","  with torch.no_grad():\n","    for batch_idx, (images, labels) in enumerate(test_loader):\n","      images = images.to(device)\n","      labels = labels.to(device)\n","      outputs = model(images)\n","      loss = criterion(outputs, labels)\n","      running_loss += loss.item()\n","      predicted = outputs.max(1, keepdim=True)[1]\n","      correct += predicted.eq(labels.view_as(predicted)).sum().item() # ここで精度を計算\n","      total += labels.size(0)\n","  val_loss = running_loss / len(test_loader)\n","  val_acc = correct / total\n","  return val_loss, val_acc\n","\n","loss_list = []\n","val_loss_list = []\n","val_acc_list = []\n","for epoch in range(num_epochs):\n","  loss = train(train_loader)\n","  val_loss, val_acc = valid(test_loader)\n","  print('epoch %d, loss: %.4f val_loss: %.4f val_acc: %.4f' % (epoch, loss, val_loss, val_acc))\n","  # logging\n","  loss_list.append(loss)\n","  val_loss_list.append(val_loss)\n","  val_acc_list.append(val_acc)\n","\n","# save the trained model\n","np.save('loss_list.npy', np.array(loss_list))\n","np.save('val_loss_list.npy', np.array(val_loss_list))\n","np.save('val_acc_list.npy', np.array(val_acc_list))\n","torch.save(model.state_dict(), 'cnn.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EX_BdFzTYA1o"},"source":["## 結果表示\n","\n","前回、多層パーセプトロンでの精度が85%くらいだったので99%程度出るCNNはかなり性能がよいことがわかる"]},{"cell_type":"code","metadata":{"id":"oUv21GWi9zkE"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","# plot learning curve\n","plt.figure()\n","plt.plot(range(num_epochs), loss_list, 'r-', label='train_loss')\n","plt.plot(range(num_epochs), val_loss_list, 'b-', label='test_loss')\n","plt.legend()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.grid()\n","\n","plt.figure()\n","plt.plot(range(num_epochs), val_acc_list, 'g-', label='val_acc')\n","plt.legend()\n","plt.xlabel('epoch')\n","plt.ylabel('acc')\n","plt.grid()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d_mPyya7YEHm"},"source":["# CIFAR-10のCNNによる分類\n"]},{"cell_type":"markdown","metadata":{"id":"M50POs56YF72"},"source":["次は同じことをCIFAR-10で行う\n","\n","初めに必要なライブラリをimportしておく\n","\n","必要な時にそれぞれ読み込むのが本来は正しいのかもしれないが、テンプレート化した方が使い勝手が良い\n","\n","なお、`from rotch.utils.data import DataLoader`としていたためDataLoaderで読み込めるが、ここでは、敢えてクラスを辿ってDataLoaderを指定する\n","\n","同様に、`from torchvision.datasets import CIFAR10`とすると`train_set = CIFER10...`とできる\n","\n","どちらでも好みでよい"]},{"cell_type":"code","metadata":{"id":"qQXTtUPQYHhP"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","device = torch.device(cuda if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LfR7zsxgEYQL"},"source":["ハイパーパラメータを宣言"]},{"cell_type":"code","metadata":{"id":"k__7hPEVCgBh"},"source":["num_epochs = 20\n","batch_size = 128 # GPUを使うなら大きくてもよい\n","classes = ('plane', 'car', 'bird', 'cat', 'deer',\n","           'dog', 'frog', 'horse', 'ship', 'truck')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ggaJIW0EcSL"},"source":["## CIFAR10データの確認\n","\n","本来は不要だが、CIFAR10データの中身を確認してみる\n","\n","- CIFAR-10はラベル付きの6万枚のカラー画像により構成されるデータセットで、画像サイズは$32 \\times 32$\n","- ラベルも少なく、10種類のみ\n","\n","普通にmathplotlibの機能を利用すると、次のような表示となる"]},{"cell_type":"code","metadata":{"id":"7yFCUpSZCkAy"},"source":["cifar10 = torchvision.datasets.CIFAR10(root=\"mydata\",\n","                       train=False,download=True,\n","                       transform=transforms.ToTensor())\n","plt.figure(figsize=(7,7))  # 画像の表示サイズ\n","chkdata = iter(torch.utils.data.DataLoader(cifar10, batch_size=4*4, shuffle=False))\n","chkimages, chklabels = chkdata.next()  # イテレータから最初のバッチを取り出す\n","for i in range(4*4):\n","  plt.subplot(4,4,i+1)\n","  plt.imshow(np.transpose(chkimages[i], (1, 2, 0)))  # チャンネルを一番後ろへ\n","  label = classes[chklabels[i]]\n","  plt.title(label)\n","  plt.tick_params(labelbottom=False, labelleft=False, bottom=False, left=False)  # ラベルとメモリを非表示に\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KLRlCHUDYOlx"},"source":["## データ拡張"]},{"cell_type":"markdown","metadata":{"id":"csvDF8GGzFKS"},"source":["### データの回転・反転・拡大縮小\n","\n","CIFAR-10の画像情報をランダムに回転、左右反転および拡大縮小する\n","\n","transforms.Composeにより配列で処理順序を指定する、つまり**順番が重要となる**\n","\n","- 例えば、`Normalize`と`RandomAffine`の順番を逆にするとエラーになる\n","  - `RandomAffine()`はtorch型ではなく、PIL.Imagesを返すため、`ToTensor`よりも,前に`RandomAffine`を施す必要がある\n","\n","  - これらの変換は**データを取得する度に実行される**ので、同じデータを取り出しても異なる回転、反転、拡大縮小の画像が得られることに注意する\n","  - appendして増やそうなどと考えなくともよい"]},{"cell_type":"markdown","metadata":{"id":"RQWrD1HRzImJ"},"source":["### PyTorchが備えるデータ拡張手法\n","\n","PyTorchには、データ拡張用に次の手法が準備されている\n","- RandomHorizontalFlip:  画像を確率pで左右反転する\n","- RandomAffine: 画像をランダムに回転/拡大縮小する\n","- RandomErasing: 画像をランダムな一部分にノイズを付加する\n","- RandomPerspective: 画像を確率pでランダムに透視変換、すなわち視角を変える\n","\n","などがある、他にも便利な、\n","- Resize: 画像のリサイズ\n","- CenterCrop: 画像の中心に移動する\n","\n","などの機能も備えている"]},{"cell_type":"markdown","metadata":{"id":"EHjOuNxuzLeP"},"source":["### データの標準化\n","\n","データ拡張とは関係ないが、CIFAR-10の画素情報をオリジナルの [0, 1] から[-1, 1] になるように標準化する\n","\n","- `transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))`について、一つ目の引数のタプルはRGBの各チャンネルの平均を表し，二つ目の引数のタプルは標準偏差を表す\n","  - つまり0から1のデータの平均値はおそらく0.5に近い値であると思われるので、平均を$mean=0.5$として、これを正規化後0にする\n","  - $z=\\frac{x-mean}{std}$として求められ、$std=0.5$とすれば、丁度取りうる値の範囲が1から2倍の2となる\n","   - これを全チャネルに対して処理する\n","\n","- 画像を表示させてから標準化させてもよいが、途中結果を表示する際に変な絵が出てきてびっくりということになりかねない\n","\n","  - ここでは、あえて先に標準化し、標準化した画像を表示させるようにしている"]},{"cell_type":"markdown","metadata":{"id":"sc8tPvpyzNqq"},"source":["### DataLoader\n","\n","num_workers を指定するとファイルの読み込みが並列化され、例えばCPUのコアが複数ある場合は一気に速くなる\n","\n","`batch_size`は、`train_loader`では`batch_size`を指定しているが、`test_loader`では`len(test_set)`として全て食べさせている"]},{"cell_type":"code","metadata":{"id":"wZ7PCby0YLZ3"},"source":["transform = transforms.Compose([\n","  transforms.RandomAffine([0,30], scale=(0.8, 1.2)), # 回転および拡大縮小\n","  transforms.RandomHorizontalFlip(p=0.5), # 左右反転\n","  transforms.ToTensor(),\n","  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # [0, 1] => [-1, 1]\n","])\n","train_set = torchvision.datasets.CIFAR10(root='mydata', train=True,\n","                          download=True, transform=transform)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n","                          shuffle=True, num_workers=4)\n","test_set = torchvision.datasets.CIFAR10(root='mydata', train=False,\n","                          download=True, transform=transform)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=len(test_set),\n","                          shuffle=False, num_workers=4)\n","print(\"訓練データの数:\", len(train_set))\n","print(\"検証データの数\", len(test_set))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aRHTKHI0tXGE"},"source":["torchvisionには、次のように複数の絵を纏めて一つの絵にする機能がある"]},{"cell_type":"code","metadata":{"id":"TFzJj8BqYVVv"},"source":["def imshow(img):\n","  # unnormalize [-1, 1] => [0, 1]\n","  img = img / 2 + 0.5\n","  img = img.numpy()\n","  # [c, h, w] => [h, w, c]\n","  plt.imshow(np.transpose(img, (1, 2, 0)))\n","images, labels = iter(test_loader).next()\n","images, labels = images[:16], labels[:16]\n","imshow(torchvision.utils.make_grid(images, nrow=4, padding=1))\n","plt.axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UuVpQAurYXVR"},"source":["PyTorchのチュートリアルに従い、ドロップアウト層を追加して別途CNNの簡単なモデルを定義する\n","\n","レイヤの内容をよく確認すること\n","- 2つめの畳み込み層もプーリング層を伴っていることを確認する\n","  - forwardでそのように定義されている\n","- 次の全結合層はそれなりにノード数を大きくとっている\n","  - ドロップアウト層では、なんと50%ものノードの計算をスキップする\n","  - この全結合層の数を増やすのも面白いであろう\n","    - 各自試してみると良い\n","- 1つめの畳み込み層(conv1)の出力サイズ\n","  - 画像サイズは32$\\times$32、フィルタサイズが5、パディングが0、ストライドが1なので、32-5+1=28\n","    - フィルタサイズはカーネルサイズとも呼ばれる\n","- 続くプーリングの出力サイズ\n","  - 2$\\times$2なので、画像サイズは28$\\div$2=14\n","- 続く畳み込み層(conv2)の出力サイズ\n","  - データサイズ14、フィルタサイズ5なので14-5+1=10\n","- 続く1次元化(view)では、フィルタ数16で各フィルタのサイズが10であるため、$16\\times 5\\times 5$の要素となる\n","- MNISTの例の通り、結局ミニバッチ数がわかっているので、`x.view(-1, 16 * 5 * 5)`という記述は、`x.view(x.size(0), -1)`でもよいが、いずれにせよノード数の情報は必要でそれほど得はしない\n","\n","シンプルな画像認識について一番最初にデモを示したが、それでも23層、その何倍も層数の多いモデルも普通に存在する\n","- もうこうなると、どういうトポロジがよいのか確認するのも大変\n","- 経験がものをいう困った世界\n","  - AIなんて経験なんだよという矛盾"]},{"cell_type":"code","metadata":{"id":"mf7a0xm7YZv8"},"source":["class DNN(nn.Module):\n","  def __init__(self):\n","    super(DNN, self).__init__()\n","    self.conv1 = nn.Conv2d(3, 6, kernel_size=5) # 畳み込み層(入力ch,フィルタ,そのサイズ)\n","    self.pool = nn.MaxPool2d(2, 2) # プーリング層(領域サイズ,ストライド)\n","    self.conv2 = nn.Conv2d(6, 16, kernel_size=5) # 2つめの畳み込み層\n","    self.fc1 = nn.Linear(16 * 5 * 5, 256) # 全結合層\n","    self.dropout = nn.Dropout(p=0.5) # ドロップアウト(ドロップアウト率)\n","    self.fc2 = nn.Linear(256, 10)\n","  def forward(self, x):\n","    x = self.pool(F.relu(self.conv1(x)))\n","    x = self.pool(F.relu(self.conv2(x)))\n","    x = x.view(-1, 16 * 5 * 5)\n","    x = F.relu(self.fc1(x))\n","    x = self.dropout(x)\n","    x = self.fc2(x)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HI_wR2qDYd6Y"},"source":["ReLUは nn.ReLU() で層として定義する場合もあるが、パラメータがないので F.relu() のように関数として使うこともできる\n","\n","モデルを確認する"]},{"cell_type":"code","metadata":{"id":"8ElkuGjdYeuW"},"source":["model = DNN().to(device)\n","model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fqXWYNruYikd"},"source":["## 学習\n","\n","モデルを訓練する\n","\n","- DataLoaderを用いてミニバッチ毎に訓練と評価を行う\n","- 評価時にミニバッチは使わずテストデータ全体を使って一度に誤差を計算する\n","\n","同様に精度も求めており、さらにモデルと結果の保存も行う\n","- **訓練に5分程度必要となるので気長に待つこと**\n","- 訓練の待ち時間を勉強時間に含めると簡単に文科省の規定時間は満たせるであろう"]},{"cell_type":"code","metadata":{"id":"aMqLDmUWYlBE"},"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","def train(train_loader):\n","  model.train()\n","  running_loss = 0\n","  for i, (images, labels) in enumerate(train_loader): # 一応回数も使えるように\n","    images, labels = images.to(device), labels.to(device)\n","    optimizer.zero_grad()\n","    outputs = model(images)\n","    loss = criterion(outputs, labels)\n","    running_loss += loss.item()\n","    loss.backward()\n","    optimizer.step()\n","  train_loss = running_loss / len(train_loader)\n","  return train_loss\n","\n","def valid(test_loader):\n","  model.eval() # 今回はDropoutを利用しているので必須\n","  running_loss = 0\n","  correct = 0\n","  total = 0\n","  with torch.no_grad(): # メモリの効率利用、optimizer.zero_grad()を呼び出す必要がなくなる\n","    for i, (images, labels) in enumerate(test_loader):\n","      images, labels = images.to(device), labels.to(device)\n","      outputs = model(images)\n","      loss = criterion(outputs, labels) # ロスの計算\n","      running_loss += loss.item()\n","      predicted = outputs.max(1, keepdim=True)[1] # 精度の計算\n","      correct += predicted.eq(labels.view_as(predicted)).sum().item() # 先の例で説明済み\n","      total += labels.size(0)\n","  val_loss = running_loss / len(test_loader)\n","  val_acc = correct / total\n","  return val_loss, val_acc\n","\n","loss_list = []\n","val_loss_list = []\n","val_acc_list = []\n","for epoch in range(num_epochs):\n","  loss = train(train_loader)\n","  val_loss, val_acc = valid(test_loader)\n","  print('epoch %d, loss: %.4f val_loss: %.4f val_acc: %.4f'\n","        % (epoch, loss, val_loss, val_acc))\n","  # logging\n","  loss_list.append(loss)\n","  val_loss_list.append(val_loss)\n","  val_acc_list.append(val_acc)\n","print('訓練終了')\n","# 結果の保存とモデルの保存\n","np.save('loss_list.npy', np.array(loss_list))\n","np.save('val_loss_list.npy', np.array(val_loss_list))\n","np.save('val_acc_list.npy', np.array(val_acc_list))\n","torch.save(model.state_dict(), 'cnn.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LqDFbOfoYn4r"},"source":["## 結果表示\n","\n","アルゴリズム自体は、MNISTと全く同じである\n","\n","今回もtestデータの方がロスが小さいが、これは前回説明した通り、ミニバッチを用いているため、そのtrainの平均としてのロスと、エポックでの学習が終了した状態でのtestのロスを比較しているためである\n","\n","また、データ拡張を用いた際も、testデータの方がロスが小さくなる傾向にある"]},{"cell_type":"code","metadata":{"id":"fGuSKXiMYqeY"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","# plot learning curve\n","plt.figure()\n","plt.plot(range(num_epochs), loss_list, 'r-', label='train_loss')\n","plt.plot(range(num_epochs), val_loss_list, 'b-', label='test_loss')\n","plt.legend()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.grid()\n","\n","plt.figure()\n","plt.plot(range(num_epochs), val_acc_list, 'g-', label='val_acc')\n","plt.legend()\n","plt.xlabel('epoch')\n","plt.ylabel('acc')\n","plt.grid()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aNAcYOyuv42o"},"source":["## 訓練済みモデルを用いた予測\n","\n","画像を入力しモデルが機能していることを確認する\n","\n","- 簡単なモデルであるため、精度は期待しないこと\n","- `shffle=True`としているため、このセルを実行する度にランダムに画像が選ばれ、評価される"]},{"cell_type":"code","metadata":{"id":"KFZYIjiOwBZ3"},"source":["dataiter = iter(torch.utils.data.DataLoader(cifar10, shuffle=True))\n","images, labels = dataiter.next()  # サンプルを1つだけ取り出す\n","\n","plt.imshow(np.transpose(images[0], (1, 2, 0)))  # チャンネルを一番後ろに\n","plt.tick_params(labelbottom=False, labelleft=False, bottom=False, left=False)  # ラベルとメモリを非表示に\n","plt.show()\n","\n","model.eval()  # 評価モード\n","imagee, labele = images.cuda(), labels.cuda()  # GPU対応\n","with torch.no_grad():\n","  labely = model(imagee)\n","print(\"正解:\", classes[labels[0]],\n","      \"予測結果:\", classes[labely.argmax().item()])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ook7Hd0byNiM"},"source":["# 課題9-1 (CNN)\n","\n","**(課題1) CIFAR-10のコードを改良して精度を向上させよう**\n","\n","レポート条件\n","- 上記の例の結果を添付する\n","- 改良し、上記よりも少ないエポック数で、上記よりも良い結果を得るモデルを作成して添付する\n","- 改良は、既習済みの内容であればどこを改良してもよい\n","  - データは指定したデータ以外使わないこと\n","  - ヒントはMNISTをちゃんと見るとすぐにわかる(じゃぁ、BNを使えと素直に言えばよいじゃん！ということになるが、皆さんの自由な発想は奪わないようにしたい)\n","- 評価部分などは一切触らないこと\n","\n","<font color='red'>他人と同じ内容、レポートになる可能性はゼロになるはずです。この課題については、Turnitinの剽窃チェックの結果も点数に反映されます。</font>\n","\n","**(課題2) CIFAR-10のコードについてデータ拡張を追加しよう**\n","\n","レポート条件\n","- 領域のランダム消去(tansforms.RandomErasing)を**追加で**用いて評価しよう\n","  - 消去確率は0.5とすること\n","  - 正しく画像が一部消去されていることも確認すること\n","- 課題1とは独立です\n","  - なので、結果がよくなることは期待していません\n","- その他の部分は修正せずに提出すること\n","\n","<font color='red'>ランダム消去なので結果は基本的に全員異なるはずです。少なくとも学習過程は違うはずですね。</font>\n","\n","以下、PyTorchのページにあるマニュアルの簡単な翻訳です\n","- 課題は利用すればよいので、オプションにこだわる必要はありません\n","\n","---\n","```torchvision.transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)```\n","\n","画像内の矩形領域をランダムに選択し、そのピクセルを消去します\n","\n","| | |\n","|:--|:--|\n","|p| ランダム消去操作が実行される確率|\n","|scale|入力画像に対する消去領域の割合の範囲|\n","|ratio| 消去領域のアスペクト比(縦横比)の範囲|\n","|value=0| 消去値を指定<br>単一のint数値の場合は全ピクセルを消去<br>3要素のタプルはRGBに対応する<br>'random' の文字列は各ピクセルをランダムな値で消去|\n","|inplace=False| 参照されたデータのみ矩形消去するか(False)、<br>元データそのものを矩形消去するか(True)を選択|\n","\n","サンプルコード\n","```\n","transform = transforms.Compose([\n","  transforms.RandomHorizontalFlip(),\n","  transforms.ToTensor(),\n","  transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n","  transforms.RandomErasing(),\n","  ])\n","```\n","---\n","\n","**課題1と課題2の二つのノートブックは、一つのノートブックに記載すること**\n","\n","その他の提出方法はこれまでと同様とする"]},{"cell_type":"markdown","metadata":{"id":"Tb5blkkWkeyE"},"source":["# CNNによるStyle Transfer\n","## 考え方\n","写真をピカソやゴッホのようなスタイルに変換できるスマホアプリ(Prisma)が話題となったことがあるが、このような技術をStyle Transferと呼ぶ\n","- 特にDNNを用いる高品質なStyle Transfer手法が2016年に出現、その後様々な改良がなされている\n","\n","ここでは、その基本となるGatysらの研究に基づき実装を行う\n","\n","まず、CNNを用いたVGG19を利用している\n","- VGG19は画像分類(物体認識)モデルであり、2014年のILSVRCコンペで優勝したモデル\n","- CNNを用いたSytle Transferは、CNNによる画像分類の仕組みをうまく利用している\n","\n","VGGなどCNNを用いる画像分類モデルでは、最初の層ほど細かな特徴を扱い、層を進むに従い、分類上重要となる情報が残り、その中で関係のない情報が欠落するような変換が行われる\n","- つまり、コンテンツとスタイルを分離できるのではないか？というのがこの手法の出発点\n","\n","この性質を利用して、コンテンツ(例えば街並みの写真、ここでは$\\boldsymbol{p}$)を保ったまま、スタイル(たとえばゴッホの絵など、ここでは$\\boldsymbol{a}$)を別の画像で獲得したスタイルと入れ替えることを考える\n","\n","スタイル変換にはVGGから全結合層を除いた部分の重みを利用する\n","\n","次の図はCNNの各層が画像をどのように表現しているかを示しており、下の街並みの写真にあるように、上位の層で入力画像を復元すると、元の画像と変化は少ないが、より深い層では、\n","入力画像の詳細な情報が欠落していることがわかる\n","\n","<img src='http://class.west.sd.keio.ac.jp/dataai/text/stcnn.png' width=600>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Gw7CUzH0xbtO"},"source":["## 仕組み\n","### 損失関数\n","コンテンツを保ちつつ、スタイルを他の画像のスタイルに近づけるため、**コンテンツの損失+スタイルの損失**を損失関数とし、これを最小化する\n","\n","#### コンテンツの損失\n","コンテンツの損失を抑えるため、コンテンツ画像のコンテンツ保持層の重みと、生成画像のコンテンツ重みを比較し、これらが等しくなるようにする\n","- 生成画像を$\\boldsymbol{x}$とする\n","- $F$および$P$の添え字が意味するのは複数の層$l$の$i$番目チャネルの位置$j$における活性度を表しており、隠れCNN層の出力値に相当する\n","- つまり、画像であり、深い層において同じ画像出力が得られるようにしている\n","  - 細かい画像の情報は、スタイル変換で失われても、大筋で元の画像を残していればよい、という発想\n","\n","$$\n","\\mathcal{L}_{content}(\\boldsymbol{p}, \\boldsymbol{x}, l)=\\frac{1}{2}\\sum_{i,j}(F^l_{ij}-P^l_{ij})^2\n","$$\n","\n","ここでは、コンテンツ重みは、conv4_2層を利用している\n","- コードでは、21番目の層のみ考慮するため、$l$は1つでよい\n","\n","#### スタイルの損失\n","次に、スタイルの損失は、特徴マップの相関とする\n","\n","まず、グラム行列$G^l$を次のように定義する\n","$$\n","G^l_{ij}=\\sum_k(F^l_{jk}-P^l_{jk})^2\n","$$\n","\n","スタイルを抜き出す画像のグラム行列$G$と、生成画像のグラム行列$A$を用いて次の値を求める\n","- 要するにこれらのグラム行列の違いを表現している\n","$$\n","E_l=\\frac{1}{4N^2_lM^2_l}\\sum_{i,j}(G^l_{ij}-A^l_{ij})^2\n","$$\n","コンテンツの損失同様、複数の層を考慮するため、最終的に次の式をスタイルの損失とする\n","$$\n","\\mathcal{L}_{style}(\\boldsymbol{a},\\boldsymbol{x})=\\sum_l^L\\boldsymbol{w}_lE_l\n","$$\n","ここで、$\\boldsymbol{a}$はスタイル画像を、$\\boldsymbol{w}_l$は第$l$層の重みを表し、ここでは、conv1_1, conv2_2, conv3_1, conv4_1, conv5_1層を用いる\n","- つまり、スタイル画像が持つCNNの重みにできるだけ近づけることで、そのスタイルを残そう、という発想\n","\n","#### 全体の損失\n","\n","$\\alpha$と$\\beta$をそれぞれの損失重み係数とすると、\n","$$\n","mathcal{L}_{content}(\\boldsymbol{p}, \\boldsymbol{a}, \\boldsymbol{x})=\n","\\alpha\\mathcal{L}_{content}(\\boldsymbol{p}, \\boldsymbol{x})+\\beta\\mathcal{L}_{style}(\\boldsymbol{a},\\boldsymbol{x})\n","$$\n","が求める損失となる\n","\n","<img src='http://class.west.sd.keio.ac.jp/dataai/text/stloss.png' width=600>\n","\n","## その他\n","\n","- VGGにおいて、MaxPoolingが用いられているが、AveragePoolingの方が画像変換には適しており、良い結果が得られることが知られている\n","  - ここでは学習済みモデルを用いているため、MaxPoolingである\n","- conv5_2を用いる方が良いという実装あり\n","  - とくに修正していない\n","- 変換過程がわかるように、動画を生成させている\n","  - 生成例は次の通りで、直接クリックしても再生できない場合は、ファイルを保存して再生するとよい\n","  - [こちら夜の都会バージョン](https://keio.box.com/shared/static/bhxd114ih0g4lvfycziryve573os2514)と[こちらゴッホ+凛尾君バージョン](https://keio.box.com/shared/static/jgnobefkgc9tbdox08tpk95dmwig9lk2)、[ラッセン+凛尾君バージョン](https://keio.box.com/shared/static/p7fqj84ha1haqannzbnlqzjq4qeelbfi)、[ゲルニカ+凛尾君バージョン](https://keio.box.com/shared/static/saeczpp7w0mcg6wg6b8nbdz4xjvvcewi)\n","\n","- やはりうまく生成させるためには、画像ごとにパラメタのチューニングが必要\n","  - 触りやすいのは、$\\alpha$と$\\beta$であろう\n","  - スタイルの適用具合の調整が行いやすい\n","生成例は次の通り\n","\n","論文から\n","\n","<img src='http://class.west.sd.keio.ac.jp/dataai/text/start.png' width=600>\n","\n","お試し例\n","\n","<img src='http://class.west.sd.keio.ac.jp/dataai/text/riost.png' width=600>\n"]},{"cell_type":"markdown","metadata":{"id":"iqcUKcIbxmOV"},"source":["## 実行コード"]},{"cell_type":"markdown","metadata":{"id":"JlxWO3P7TmWO"},"source":["Pillowを入れるが、Google Colaboratoryの場合不要"]},{"cell_type":"code","metadata":{"id":"oXxD6ptSFYFe"},"source":["!pip install Pillow"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HiuoCQZOUS1i"},"source":["いつもの初期化コード"]},{"cell_type":"code","metadata":{"id":"oFxZnrwDGCAV"},"source":["import torch\n","import torch.optim as optim\n","from torchvision import transforms, models\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import numpy as np\n","device = torch.device(cuda if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nysqCDHxUapR"},"source":["今回はVGGと呼ばれる著名な学習済み画像認識モデルを利用する\n","- PyTochはこのような著名な画像認識モデルを含む様々なモデルをはじめから準備している\n","- 学習済みモデルのパラメータを変更しないように、AutogradをOFFにしておく"]},{"cell_type":"code","metadata":{"id":"h_pTMgBHOuaR"},"source":["vgg = models.vgg19(pretrained=True).features\n","vgg.to(device)\n","for param in vgg.parameters():\n","  param.requires_grad_(False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"juxf7zXMUxLb"},"source":["画像を読み込むための関数の定義\n","- 画像のサイズを整え、正規化する\n","- PillowのImage.openは、BMP, EPS、GIF、ICO、JPEG、PNG、PPM、TIFF、WebP、XBMなどなど、様々なフォーマットに対応している\n","- なので、皆さんは結構どのような画像を持ってきてもうまく動くはず\n","- `transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)`は説明済"]},{"cell_type":"code","metadata":{"id":"LEQ-rSgPPmn3"},"source":["def load_image(img_path, max_size=400, shape=None):\n","  image = Image.open(img_path).convert('RGB')\n","  if max(image.size) > max_size:\n","    size = max_size\n","  else:\n","    size = max(image.size)\n","  if shape is not None:\n","    size = shape\n","  in_transform = transforms.Compose([\n","               transforms.Resize(size),\n","               transforms.ToTensor(),\n","               transforms.Normalize((0.5, 0.5, 0.5), \n","                                    (0.5, 0.5, 0.5))])\n","  image = in_transform(image).unsqueeze(0)   \n","  return image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"huC3eBr4_zAE"},"source":["以下に用いる画像をロードしているが、好きな画像を利用するにはファイルを上書きすればよい\n","- 左のcontentフォルダにデータが入るが、好きな画像に入れ替えてみよう\n","- jpgじゃなくてもメジャーなフォーマットであれば大丈夫"]},{"cell_type":"code","metadata":{"id":"vQizqjGd_NtR"},"source":["import os\n","if not os.path.exists('contents.jpg'):\n","  #!wget \"https://drive.google.com/uc?export=download&id=1OhdW3HJwVRUOwGTOMxhj2YP8bDwWjMkk\" -O contents.jpg\n","  !wget https://keio.box.com/shared/static/nla0l244kq5o30p5eh9iop0sk5jsl6ap -O contents.jpg\n","\n","if not os.path.exists('style.jpg'):\n","  #!wget \"https://drive.google.com/uc?export=download&id=1OhdW3HJwVRUOwGTOMxhj2YP8bDwWjMkk\" -O style.jpg\n","  !wget https://keio.box.com/shared/static/az9yr07d5j5b3agtj7i8v5iwbso7za8u -O style.jpg"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8kgXiVNLVQ9d"},"source":["左のフォルダにドラッグして自分の画像ファイルを入れて、次のファイル名を変更するとよい\n","- スタイルは基本変更しなくてもよいが、「画風が独特の絵画」などを入れると楽しめるであろう"]},{"cell_type":"code","metadata":{"id":"pqdgcVDjwgQl"},"source":["content = load_image('contents.jpg').to(device)\n","style = load_image('style.jpg').to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UynRZNejWb9l"},"source":["途中経過などで利用するため、**画像をメモリからコピーして取り出し**、正規化しているのでを元に戻してきちんとした画像にする関数を定義\n","\n","- np.arrayからPillow(PIL)への変換に、transpose(1,2,0)を行う\n","  - これは並び順の変換で(color, x, y)という順番であるため、(x, y, color)にする\n","- 平均0、標準偏差0.5の正規表現($-1<pixels<1$)で正規化されたデータを0から1の値にするため0.5倍して0.5を足す\n","- image.clipで値の範囲を0から1に限定する"]},{"cell_type":"code","metadata":{"id":"zn0bvjFUzmsM"},"source":["def im_convert(tensor):\n","  image = tensor.to(\"cpu\").clone().detach()\n","  image = image.numpy().squeeze() # 大きさが1の次元を纏めて削除\n","  image = image.transpose(1,2,0)\n","  image = image * np.array((0.5, 0.5, 0.5)) + np.array((0.5, 0.5, 0.5))\n","  image = image.clip(0, 1)\n","  return image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F_zrsEOvbdZD"},"source":["読み込んだ2つの画像を実際に表示してみる"]},{"cell_type":"code","metadata":{"id":"svb0OoXkz2NR"},"source":["fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n","ax1.imshow(im_convert(content))\n","ax1.axis(\"off\")\n","ax2.imshow(im_convert(style))\n","ax2.axis(\"off\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1bbHoxdbbgc4"},"source":["既に説明した通り、必要な層のパラメータを取得する関数"]},{"cell_type":"code","metadata":{"id":"nDAOB_5_0Qe0"},"source":["def get_features(image, model): \n","  layers = {'0': 'conv1_1',\n","            '5': 'conv2_1', \n","            '10': 'conv3_1', \n","            '19': 'conv4_1',\n","            '21': 'conv4_2',  # Content Extraction\n","            '28': 'conv5_1'}     \n","  features = {}\n","  for name, layer in model._modules.items():\n","    image = layer(image)\n","    if name in layers:\n","      features[layers[name]] = image        \n","  return features"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yJ7iKwbQXvMC"},"source":["コンテンツとスタイルそれぞれの必要な層のパラメータを取得"]},{"cell_type":"code","metadata":{"id":"x_ju0WNDznvD"},"source":["content_features = get_features(content, vgg)\n","style_features = get_features(style, vgg)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RSmfE-DBdMS9"},"source":["グラム行列($\\boldsymbol{A} \\cdot \\boldsymbol{A})$を求める\n","- 既に説明した通り損失を得るために必要"]},{"cell_type":"code","metadata":{"id":"8AOrNbUzzpRn"},"source":["def gram_matrix(tensor):\n","  _, d, h, w = tensor.size()\n","  tensor = tensor.view(d, h * w)\n","  gram = torch.mm(tensor, tensor.t())\n","  return gram"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T5EWYfUpd45E"},"source":["グラム行列の実際の計算"]},{"cell_type":"code","metadata":{"id":"N0B6o3oFzuaq"},"source":["style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dI7woWcbd9lZ"},"source":["各層の特徴をどの程度利用するかの比率表\n","- ここは各自で自由に修正しても面白いであろう"]},{"cell_type":"code","metadata":{"id":"G00b3ZNazvba"},"source":["style_weights = {'conv1_1': 1.,\n","                 'conv2_1': 0.75,\n","                 'conv3_1': 0.2,\n","                 'conv4_1': 0.2,\n","                 'conv5_1': 0.2}\n","content_weight = 1  # alpha\n","style_weight = 1e6  # beta 1e6がデフォルトだが1e4などでも破綻はしない"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WhSMkr8meG-c"},"source":["データを修正するが、オリジナルを取っておきたいのでクローニングする\n","- 違うメモリに画像データを格納する\n","- これをそのままNNにかけるのでAutogradをONにする"]},{"cell_type":"code","metadata":{"id":"ryfwn-ZbzwxN"},"source":["target = content.clone().requires_grad_(True).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JaGT-x_neUkh"},"source":["各種ハイパーパラメタ\n","- 特に修正する必要はないと思が、STEPはどの程度画像を触るかに影響するので、各自で調整するとよいであろう"]},{"cell_type":"code","metadata":{"id":"hP-aUTBhzyaD"},"source":["show_every = 300\n","optimizer = optim.Adam([target], lr=0.003)\n","steps = 2100\n","height, width, channels = im_convert(target).shape\n","image_array = np.empty(shape=(300, height, width, channels))\n","capture_frame = steps/300\n","counter = 0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NrCtn7AYeeyB"},"source":["実際の処理\n","- 既に説明した通りにロスを計算している\n","- ロスを自由に設計してもPyTorchは、.backwordで一発逆誤差伝播法の演算が可能であることがわかる\n","- 上記capture_frameで指定された変換動画を作成するための画像データ取得間隔毎に、画像をimage_arrayに蓄えている"]},{"cell_type":"code","metadata":{"id":"2HHQ2t366VUV"},"source":["for ii in range(1, steps+1):\n","  target_features = get_features(target, vgg) # VGGから特徴量を抜き出す\n","  content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2) # 損失関数\n","  style_loss = 0\n","  for layer in style_weights: # スタイルの抽出\n","    target_feature = target_features[layer]\n","    target_gram = gram_matrix(target_feature)\n","    style_gram = style_grams[layer]\n","    layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n","    _, d, h, w = target_feature.shape\n","    style_loss += layer_style_loss / (d * h * w)\n","  total_loss = content_weight * content_loss + style_weight * style_loss\n","  optimizer.zero_grad()\n","  total_loss.backward()\n","  optimizer.step()\n","  if  ii % show_every == 0:\n","    print('Total loss: ', total_loss.item())\n","    print('Iteration: ', ii)\n","    plt.imshow(im_convert(target))\n","    plt.axis(\"off\")\n","    plt.show()\n","  if ii % capture_frame == 0:\n","    image_array[counter] = im_convert(target)\n","    counter = counter + 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yp1rK8Exfcm4"},"source":["最終結果の表示"]},{"cell_type":"code","metadata":{"id":"cK1_Kq36CmwZ"},"source":["fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 10))\n","ax1.imshow(im_convert(content))\n","ax1.axis('off')\n","ax2.imshow(im_convert(style))\n","ax2.axis('off')\n","ax3.imshow(im_convert(target))\n","ax3.axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FuWgpSIOffjb"},"source":["途中結果(image_array)を用いて変換動画(output.mp4)を作成"]},{"cell_type":"code","metadata":{"id":"RtWzCQFyYyL6"},"source":["import cv2 \n","frame_height, frame_width, _ = im_convert(target).shape\n","vid = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'XVID'), 30, (frame_width, frame_height))\n","for i in range(0, 300):\n","  img = image_array[i]\n","  img = img*255\n","  img = np.array(img, dtype = np.uint8)\n","  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","  vid.write(img)\n","vid.release()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KU4qwy6Yf8UG"},"source":["結果をブラウザからダウンロードさせる\n","- こんなこともできるよ、ということ\n","  - Google Colaboratoryで実行しているときのみ、ファイルをダウンロードできるようにしている"]},{"cell_type":"code","metadata":{"id":"wRL5tLUhfJ5L"},"source":["import sys\n","moduleList = sys.modules\n","if 'google.colab' in moduleList:\n","  from google.colab import files\n","  files.download('output.mp4')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AnRB6tb_gBuQ"},"source":["# 課題9-2 (StyleTransfer)\n","\n","なんでもよいので、自分の好きな画像とスタイル画像を用いて、変換してみよう！\n","\n","## レポート条件\n","\n","レポートにコンテンツ画像、スタイル画像、変換後画像をそれぞれ添付しなさい\n","  - スタイル画像は上記のままでも問題ありません\n","\n","<font color='red'>こちらも、もちろん皆さん違う写真であることを期待しています。</font>"]}]}