{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dataai-text-F-PyTorch-自然言語処理-2.ipynb","private_outputs":true,"provenance":[{"file_id":"1F4ifdHNnrAUsDv2pj0EXDImHsM_x_E9M","timestamp":1628197853609}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Ymc95iQm0xTt"},"source":["#@title Data-AI（必ず自分の名前・学籍番号を入力すること） { run: \"auto\", display-mode: \"form\" }\n","\n","import urllib.request as ur\n","import urllib.parse as up\n","Name = '\\u6771 \\u8A00\\u8449' #@param {type:\"string\"}\n","EName = 'Azuma Kotoha' #@param {type:\"string\"}\n","StudentID = '87654321' #@param {type:\"string\"}\n","Addrp = !cat /sys/class/net/eth0/address\n","Addr = Addrp[0]\n","url = 'https://class.west.sd.keio.ac.jp/classroll.php'\n","params = {'class':'dataai','name':Name,'ename':EName,'id':StudentID,'addr':Addr,\n","           'page':'dataai-text-F-2','token':'35672359'}\n","data = up.urlencode(params).encode('utf-8')\n","#headers = {'itmes','application/x-www-form-urlencoded'}\n","req = ur.Request(url, data=data)\n","res = ur.urlopen(req)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gU-Arpc9zElH"},"source":["---\n",">「少なくとも2つの言語を理解しない限り、1つの言語を理解することはない」\n"," \\\n",">（ジェフリー・ウィリアムズ）\n","---"]},{"cell_type":"markdown","metadata":{"id":"bMVPaUY5-hVV"},"source":["# 自然言語処理（Natural Language Processing）"]},{"cell_type":"markdown","metadata":{"id":"TDiUUzuZYdik"},"source":["# 簡単な実装\n","\n","ここでは、Transfomerを用いたBERTについて扱う\n","- 日本語は厄介なので、まずは英語を扱う\n","- Pytorch-Transformersを用いて学習済みBERTモデルによる簡単な事例を扱う"]},{"cell_type":"markdown","metadata":{"id":"Zd4rO-jl1Xe4"},"source":["## Transformer\n","- 2017年に導入されたディープラーニングモデルの一種\n","  - 主に自然言語処理で利用されている\n","- RNNと同様自然言語などの時系列データ処理向けに設計されているが、再帰や畳み込みは利用していない\n","- Attention層のみで構築されている(後述)\n","- 翻訳やテキスト要約などの各種タスクに利用可能\n","- 並列化が容易で訓練時間を削減できる\n","- 「Attention is All You Need」という論文で著名になった"]},{"cell_type":"markdown","metadata":{"id":"_ku4J5yu4iy0"},"source":["### Transformerの構造\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/transformer.png\" width=300>\n","\n","Seq2Seq同様EncoderとDecoderで構成\n","\n","### Encoderの構造\n","1. Embedding層により入力文章をベクトルに圧縮、つまり分散表現に変換する\n","1. Positional Encoder層で文章内のどこにあるかという位置情報を加える\n","1. Multi-Head Attention層(後述)\n","1. normalization(正規化)によりデータの偏りを削減する\n","  - batch normalizationではなくlayer normalizationが行なわれる\n","1. Feed Forward層との組み合わせて処理され、実際のモデルでは6回繰り返される\n","  - 出力されたベクトルはDecoderに渡される\n","  - 特にPositionwise fully connected feed-forward networkと呼ばれる\n","\n","- Multi-Head Attention層とFeed Forward層の組み合わせが6回繰り返される\n","\n","以上で、Encoderが構成される\n","\n","### Decoderの構造\n","\n","1. Embedding層により入力文章をベクトルに圧縮(分散表現を獲得)\n","1. Positional Encoder層で位置情報を追加\n","1. Masked Multi-Head Attention層、先ほどと同様であるがAttention内のsoftmax関数を通す直前の値にマスキングが適用されている\n","  - 特定のkeyに対して、Attention weightを0にすることで入力した単語の先読みによる「カンニング」を防ぐ\n","  - 入力に予測すべき結果が入らないようにする\n","1. normalization（正規化）などで先ほどと同様\n","1. Multi-Head Attention層（Encoderの出力を入力として使用）\n","1. normalization（正規化）など\n","1. Positionwise fully connected feed-forward network(先ほどと同じ)\n","1. normalization（正規化）など\n","- 以上を6回繰り返す\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZCbAQSWygRj8"},"source":["### Transformerの構成要素\n","\n","- Attention\n","  -「文章中のどの単語に注目すればよいかを表すスコア」のこと\n","  - Query、Key、Valueの3つのベクトルで求める\n","    - Query: Inputのうち「検索をかけたいもの」\n","    - Key: 検索対象とQueryの近さ、どれだけ似ているかを測る\n","    - Value: Keyに基づき、適切なValueを出力する\n","  - Self-Attention\n","    - 下図でInputとMemoryが同一のAttention\n","      - 文法の構造や、単語同士の関係性などを獲得するのに使用される\n","  - SourceTarget-Attention\n","    - 下図でInputとMemoryが異なるAttention\n","      - TransformerではDecoderで使用される\n","  - Multi-Head Attention\n","    - Attentionを複数並列して並べたもの(後述)\n","  - Masked Multi-Head Attention\n","    - Multi-Head Attentionにマスクをつけたもの\n","    - 特定の key に対してAttention weight を0にする\n","    - TransformerではDecoderで使われる\n","    - 入力した単語が先読みを防ぐために 情報をマスクで遮断する、言わば「カンニング」を防ぐ\n","  - Attentionは可視化できる\n","    - すでに示したが、attentionは可視化でき、どの単語に注目しているかを知ることができる\n","- Position-wise Fully-connected Feedforward Network\n","  - 2層からなる全結合ニューラルネットワーク\n","  - 単語の位置ごとに個別の順伝播ネットワークとなる\n","    - これにより他単語との影響関係を排除することができる\n","  - パラメータは全てのネットワークで共通\n","$FNN(x) = LeRU(xW_1+b_1)\\cdot W_2+b_2$\n","- Positional Encoding ($PE$)\n","  - 「単語の位置」の情報をベクトルに加える\n","  - $pos$は位置を表し、$2i$および$2i+1$はEmbeddingの何番目の次元か、$d_{model}$が次元数を示す  \n","偶数番目：$PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})$  \n","機数番目：$ PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$"]},{"cell_type":"markdown","metadata":{"id":"Qqqr2QeuiwaR"},"source":["<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/attention2.png\" width=600>\n","\n","- 丸角(緑)がベクトル(テンソル)、四角角(青)が処理を表す\n","- InputとMemoryはそれぞれ異なる埋め込みベクトルを表し、例えば2つの異なる文章を表す\n","- Inputについて全結合層で各単語のQueryを作成する\n","- Memoryについても同様に全結合層でKeyを作成しQueryとの内積をとって関連度合い見る\n","  - 同じ向きを向いていれば掛け算となる\n","  - 垂直である、つまり関連しなければ0\n","  - この値を関連度(logit)とする\n","- logitにSoftmaxを適用して0から1の間に調整して出力、この結果が Attention weightとなる\n","  - メモリのどの単語に注意を払うかの重みづけ\n","  - QueryとKeyの関連が大きいとAttention weightが大きくなる\n","    - 正しくMemoryの単語に注意を向けるように,keyが正しくAttentionに向けられるように学習される\n","- Memoryから全結合層を経て、Memoryの各単語に対する埋め込みベクトルであるValueを算出する\n","  - ValueとAttenthion weightとの内積を求める\n","    - Attention weightに従ってValueを選択することを意味する\n","- 最後に全結合層を挟んで出力を得る\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SUEwYu73qKFn"},"source":["### InputとMemory\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/input-memory.png\" width=400>\n","\n","各文章は分かち書きされIDで表現された後、Embeddingにより埋め込みベクトルに変換される\n","\n","### Attention Weightの算出\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/attention-weight.png\" width=600>\n","\n","QueryとKeyの内積を算出してInputとMemoryの各単語の関連度であるlogitを算出、Softmaxを用いてAttention weightとする\n","- Memoryのどの単語に注意を払うかの重み付け\n","\n","例えば、Inputのスポーツという単語に対して、Memoryの「野球」 「が」 「得意」の各単語について正しく注意を向けるように学習する\n","- ここでは野球が高い値になるようになる\n","\n","### valueとの内積\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/value-naiseki.png\" width=600>\n","\n","この内積は、value、ここでは「野球」「が」「得意」の各単語のValueとAttention weightを掛け合わせて総和を計算することになる\n","\n","最も注目するべきvalueの値が算出されているといえるが、他の単語との関連性も考慮した値となっている"]},{"cell_type":"markdown","metadata":{"id":"ooi7J-rSvL7e"},"source":["### Multi-Head Attention\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/mhattention.jpg\" width=200>\n","\n","- Attentionを並列させた構造を持つ\n","- それぞれのAttentionをHeadと呼ぶ\n","- 「Attention Is All You Need」ではMulti-Head化により性能が向上するとされている\n","  - アンサンブル学習に近い\n","  - Dropoutも毎回異なるネットワークを使っており、アンサンブルに通じるところがある"]},{"cell_type":"markdown","metadata":{"id":"1vnTzJPH9ouJ"},"source":["## BERT\n","\n","BERT(Birdirectional Encoder Representation from Transformers)の略で、2018年の後半にGoogleが発表\n","- 自然言語処理のための新たなディープラーニングモデル\n","- Transformerをベースとしている\n","- 様々な自然言語処理タスクでファインチューニングが可能\n","  - 訓練済みモデルを与えられたタスクに合わせて調整可能- 従来の自然言語処理タスクよりも汎用性が高い\n","\n","「BERT: Pre-training of Deep Bidirectional Transformers for  Language Understanding」という論文で登場\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9ncw_gtK-nEi"},"source":["### BERTにおける学習\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/bert.png\" width=600>\n","\n","- 左側が事前学習モデル\n","  - ラベルのない、つまり正解のない文章のペアを渡す(Unlabeled Sentence A and B Pair)\n","    - Maskedとあるように文章の一部が隠されている\n","  - 各種出力がある\n","    - NSP = Next Sentence Predictionの略で次の文章を予測する時に使う\n","    - Mask LM = 文章の穴埋め補間に使う\n","   - BERTの学習には多大な計算コストが伴うため、事前学習モデルを利用することが多い\n","- Fine-Tuningは計算コストが比較的小さいので比較的手軽に実行可能\n","  - SQuADタスク\n","    - 「Stanford Question Answering Dataset」の略\n","    - 言語処理の精度を測るベンチマークであり約10万個の質問(Question)と回答(Paragraph)とのペアで構成される\n","    - このデータセットを用いてファイチューニングすることが行なわれている\n","    - 後述するNext Sequence Predictionにも関連するが、この質問と回答のセットを正しく選ぶことができるかどうかという問いに対し、BERTは人間よりも正答率を上げることができる\n","    - 画像分野では人間よりも正確に画像から病巣を見つけるなどできるようになったが、文章処理においても人間よりも正確に判断ができるようになったことで話題となった\n","  - NERタスク\n","    - 固有表現抽出で、文字列中に現れる（人名、組織名、日付などのような）様々な種類の固有表現に目印をつける\n","  - MNLIタスク\n","    - 自然言語推論のコーパス\n","\n","- 事前学習\n","  - Transformerが、文章から文脈を双方向(Bidirectional)に学習\n","    - Masked Language ModelおよびNext Sentence Predictionによる双方向学習を用いる\n","- ファインチューニング\n","  - 事前学習により得られたパラメータを初期値として、ラベル付き(正解付き)のデータでファインチューニングを行う\n","\n","日本語のBERT事前学習モデルが存在する\n","- 京都大学 黒橋・褚・村脇研究室\n","- http://nlp.ist.i.kyoto-u.ac.jp/index.php?BERT\n","- とにかくでかい\n"]},{"cell_type":"markdown","metadata":{"id":"Q2awMOEKaFtY"},"source":["### BERTのモデル\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/bert2.png\" width=300>\n","\n","BERTに埋め込みベクトルEn(単語の分散表現)を入力する\n","- N個の単語が並んでいる文章とする\n","- TrmつまりTransformerに入力する\n","- 文章の単語の関連が、前後両方のリンクを伴うため、Bidirectional(双方向)とネーミングされている\n"]},{"cell_type":"markdown","metadata":{"id":"hGpDEGB0bZlj"},"source":["### BERTの入力\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/bert3.png\" width=600>\n","\n","BERTの入力は文章の埋め込みベクトルである\n","- まず「my dog is cute. He likes play ##ing .」という文章を入力することを考える\n","  - 実際には文章の開始を意味する[CLS]、文章の区切りを表す[SEP]=セパレータを用いて表現される\n","- 各単語をEmbeddingする、つまり分散表現としての埋め込みベクトルに変換する\n","  - Segmentつまり属する文章を表現する\n","  - つまり、$E_A$の文章と$E_B$の文章の2つの文章のどちらに各単語が属するかを表す\n","- Position Embeddingsを加える\n","  - 文章のどの位置にあるかを表すベクトルである\n","  - ここでは、0から10の11単語存在することを表している\n"]},{"cell_type":"markdown","metadata":{"id":"3_nHmTTkd0he"},"source":["### BERTの学習\n","\n","- 事前学習\n","  - Transformerが、文章から文脈を双方向(Bidirectional)に学習する\n","  - ここではMasked Language ModelおよびNext Sentence Predictionによる双方向学習を行う\n","\n","- ファインチューニング\n","  - 事前学習により得られたパラメータを初期値として、ラベル付きのデータで ファインチューニングを行う\n"]},{"cell_type":"markdown","metadata":{"id":"fZrT09k_A5dc"},"source":["### Masked Language Model\n","\n","文章から特定の単語を15％ランダムに選び、[MASK]トークンに置き換える\n","- 例: my dog is hairy → my dog is [MASK]\n","- [MASK]の単語を、前後の文脈から予測する\n"]},{"cell_type":"markdown","metadata":{"id":"kdDrSZggBJ8T"},"source":["### Next Sentence Prediction\n","- 2つの文章に関係があるかどうかを判定する\n","  - 後ろの文章を50%の確率で無関係な文章に置き換える\n","  - 後ろの文章が意味的に適切であればIsNext、そうでなければNotNextと判定\n","- 例：[CLS] the man went to [MASK] store [SEP] / he bought a gallon [MASK] milk [SEP]\n","  - CLSは文章開始、SEPは文章の区切りや終わり\n","  - 判定：IsNext、つまり2つの文章は自然な流れである\n","- 例：[CLS] the man went to [MASK] store [SEP] / penguin [MASK] are flight #less birds  [SEP]\n","  - 判定：NotNext、つまり2つの文章は不自然な流れである\n"]},{"cell_type":"markdown","metadata":{"id":"JlmFSWjcEaRS"},"source":["## Transformers\n","\n","Transformers自然言語処理ライブラリでBERTが利用できる\n","  - 米国のHugging Face社が提供\n","  - 分類、情報抽出、質問回答、要約、翻訳、テキスト生成などのに利用できる事前学習モデルを100以上の言語で提供、話題のGPT-2もある\n","  - 最先端の自然言語処理技術が簡単に使用可能\n","  - PyTorchに対応\n","  - https://huggingface.co/transformers/\n","\n","構成クラス\n","- model classes\n","  - 事前学習済みのパラメータを持つモデルのクラス\n","- configuration classes\n","  - ハイパーパラメータとして中間層のニューロンの数や層の構造などモデルの設定を行うためのクラス\n","tokenizer classes\n","  - 語彙の保持、形態素解析などに関連するクラス\n"]},{"cell_type":"markdown","metadata":{"id":"UsiATWvZG1vH"},"source":["### TransformersにおけるBERTモデル\n","- BertForPreTraining\n","これを継承して以下のモデルがある\n","- BertModel 特定タスクを想定しない汎用BERT\n","- BertForMaskedLM 単語の一部にマスクをかけてそれを予測\n","- BertForNextSentencePrediction ある文書の次の文章が適切か判別\n","- BertForSequenceClassification 文章を分類\n","- BertForMultipleChoice 文章を分類\n","- BertForTokenClassification 単語の分類\n","- BertForQuestionAnswering 質問と答えのペアを扱う"]},{"cell_type":"markdown","metadata":{"id":"KcHOX9LyZc2g"},"source":["# PyTorch-Transformersの基本的な使い方\n"]},{"cell_type":"markdown","metadata":{"id":"L_Ozfz3NhltP"},"source":["## ライブラリのインストール\n","PyTorch-Transformersおよび必要なライブラリをインストールする"]},{"cell_type":"code","metadata":{"id":"Y_mDYVlb-sqi"},"source":["!pip install folium\n","!pip install urllib3\n","!pip install pytorch-transformers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nMfvi2uVkItT"},"source":["## PyTorch-Transformersのモデル\n","PyTorch-Transformersには、様々な訓練済みのモデルを扱うクラスが用意されている\n","\n","ここでは、文章の一部をMaskする問題である`BertForMaskedLM`のモデルを設定する\n","- 詳細はこちらhttps://huggingface.co/transformers/model_doc/bert.html#bertformaskedlm  \n","\n","`BertForMaskedLM`はベースとなるモデルであり、`PreTrainedModel`を継承している\n","- 詳細はこちらhttps://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel  \n","\n","さらに、`BertForMaskedLM`は`nn.Module`クラスも継承しており、今までと同様にPyTorchのモデルとして利用できる！"]},{"cell_type":"code","metadata":{"id":"E9Hv5L2HGDmI"},"source":["import torch\n","from pytorch_transformers import BertForMaskedLM\n","\n","msk_model = BertForMaskedLM.from_pretrained('bert-base-uncased')  # 訓練済みパラメータの読み込み\n","print(msk_model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SpM7nLUtKD6c"},"source":["中身をみてみよう\n","- BertForMaskedLMとある\n","- BertModelの記述があり、word_embeddingsという単語をベクトルに変換する(分散表現化)\n","- BertEncoderで入力の特徴を抽出する\n","  - この中に0から11の12のBertレイヤがある(でかい)\n","  - Attention(特徴量を抽出)、Intermediate(特徴量の拡張)、Output(整形)がある\n","- BertPooler 出力をタスクに合わせて調整する\n","ここまでは全てのタスクで共通\n","- BertOnlyMLMHead 今回のタスクに特化している部分\n","  - 最終的に単語の数である30522クラスに分類する問題であるとわかる\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/pytorch-transbert.png\" width=600>\n","\n","- BertEmbeddings\n","  - 入力を埋め込みベクトルに変換\n","  - word_embeddings, position_embeddings, token_type_embeddings、これら3つの埋め込みベクトルを足し合わせる\n","\n","- BertAttention\n","  - 入力を埋め込みベクトルに変換\n","  - word_embeddings, position_embeddings, token_type_embeddings、これら3つの埋め込みベクトルを足し合わせる\n","  - https://git.io/JkniK で確認するとよい(github.com専用のURL短縮サービスを利用)\n","BertAttention\n","  - BertSelfAttention、BertSelfOutputによりSelf-Attentionを実装\n","  - Maskの実装\n","  - 以下を利用して確認するとよい  \n","https://git.io/JknTg （BertAttention）  \n","https://git.io/JknTi （BertSelfAttention）  \n","https://git.io/JknXl （BertSelfOutput）\n","\n","- BertIntermediateおよびBertOutput\n","  - Positionwise fully connected feed-forward networkを実装\n","  - 以下を利用して確認するとよい  \n","https://git.io/Jkn1u （BertIntermediate）  \n","https://git.io/Jkn1r （BertOutput）\n","\n","- BertPooler\n","  - 最初のトークン（単語）を取得し全結合層、活性化関数で処理\n","  - Next Sentence Predictionなどで使用\n","  - https://git.io/JknUw で確認するとよい\n"]},{"cell_type":"markdown","metadata":{"id":"n3V7quRwp1jn"},"source":["さらに文章を分類するモデルである`BertForSequenceClassification`を設定する\n","- 詳細はこちらhttps://huggingface.co/transformers/model_doc/bert.**html**#bertforsequenceclassification  "]},{"cell_type":"code","metadata":{"id":"7VBwLQZBJyEh"},"source":["from pytorch_transformers import BertForSequenceClassification\n","\n","sc_model = BertForSequenceClassification.from_pretrained('bert-base-uncased')  # 訓練済みパラメータの読み込み\n","print(sc_model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ahWRDafvLy9I"},"source":["中身を見てみよう\n","\n","- 最初は先ほどと全く同じである\n","- 違いは最後のdropoutと全結合層となる\n","- `out_features=2`とあるため、文章を2クラスに分類する問題であることが分かる\n"]},{"cell_type":"markdown","metadata":{"id":"rMlz4gJQucPB"},"source":["# BERTの設定\n","`BertConfig`クラスによりモデルの設定が可能である  "]},{"cell_type":"code","metadata":{"id":"F54bOxW6uGBX"},"source":["from pytorch_transformers import BertConfig\n","\n","config = BertConfig.from_pretrained(\"bert-base-uncased\")\n","print(config) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hxzTPzizQDT3"},"source":["以上が初期設定である\n","- `hidden_size` 隠れ層のノードの数\n","- `vocab_size` 単語数\n","- `type_vocab_size` セグメントの数、2なので2つの文章を扱う\n","- `intermediate_size` BERT intermediate layerのノード数"]},{"cell_type":"markdown","metadata":{"id":"nMQKVuZyv-sC"},"source":["## Tokenizer\n","`BertTokenizer`クラスを使って、訓練済みのデータに基づく形態素解析が可能となる"]},{"cell_type":"code","metadata":{"id":"zspnwnNHxIEz"},"source":["from pytorch_transformers import BertTokenizer\n","\n","text = \"I have a pen. I have an apple.\"\n","\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","words = tokenizer.tokenize(text)\n","print(words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4seFzZGTQwfa"},"source":["では、BertForMaskedLM(文章におけるMASKされた単語の予測)とBertForNextSentencePrediction(ある文章の、次の文章が適切かどうかの判定)を実際に行う\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Gb5gVyYF2vjL"},"source":["## 文章の一部の予測\n","文章における一部の単語をMASKし、それをBERTのモデルを使って予測する"]},{"cell_type":"code","metadata":{"id":"G6oFZnS21Mqs"},"source":["text = \"[CLS] I played baseball with my friends at school yesterday [SEP]\"\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","words = tokenizer.tokenize(text)\n","print(words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nMd8sN_FRxyN"},"source":["英語の文章であると、どうも答えがつまらない"]},{"cell_type":"markdown","metadata":{"id":"t3Y32Tl55dSl"},"source":["文章の一部をMASKする\n","- MASKする場所は、様々に変更してみるとよいだろう"]},{"cell_type":"code","metadata":{"id":"0_H50V7b5RM0"},"source":["msk_idx = 3  # ここでは3番目の単号であるbaseballをマスクする\n","words[msk_idx] = \"[MASK]\"  # baseballを[MASK]に置き換える\n","print(words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xBXgAn9s528_"},"source":["単語を対応するインデックスに変換する"]},{"cell_type":"code","metadata":{"id":"zm4qbMPW56-w"},"source":["word_ids = tokenizer.convert_tokens_to_ids(words)  # 単語をインデックスに変換\n","word_tensor = torch.tensor([word_ids])  # テンソルに変換\n","print(word_tensor)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y76O88877cB_"},"source":["BERTのモデルを使って予測する\n","- from_pretrainedで学習済みモデルを利用する\n","- 学習は行わないため、evalを指定する"]},{"cell_type":"code","metadata":{"id":"M2NWREc77gQC"},"source":["msk_model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n","msk_model.cuda()  # GPU対応\n","msk_model.eval()\n","\n","x = word_tensor.cuda()  # GPU対応\n","y = msk_model(x)  # 予測(順伝搬)\n","result = y[0]  # tuppleなので要素を取り出す\n","print(result.size())  # 結果の形状"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zJ267bsDTmdV"},"source":["resultの形状を確認する\n","- 文章が11個の単語で構成されており、30522個の単語それぞれの適切度合いが含まれていることがわかる\n","\n","ここから、値の大きい5個を取り出す\n","- 結果がバッチ対応であるため0とする\n","- 最も大きな値そのものが`_`の部分に入るが、利用しないためその値を捨てられる"]},{"cell_type":"code","metadata":{"id":"oOF2sKnUTiuA"},"source":["_, max_ids = torch.topk(result[0][msk_idx], k=5)  # 最も大きい5つの値\n","result_words = tokenizer.convert_ids_to_tokens(max_ids.tolist())  # インデックスを単語に変換\n","print(result_words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IreVvSumSsCQ"},"source":["きちんと「play」できるものが選択されている\n","- pianoなど楽器はtheが伴うので含まれていない！流石！"]},{"cell_type":"markdown","metadata":{"id":"2Su6QTCAAgFS"},"source":["## 文章が連続しているかどうかの判定\n","BERTのモデルを使って、2つの文章が連続しているかどうかを判定(確率表示)する関数`show_continuity`で2つの文章の連続性を判定する"]},{"cell_type":"code","metadata":{"id":"FC0nihWMAtgG"},"source":["from pytorch_transformers import BertForNextSentencePrediction\n","\n","def show_continuity(text, seg_ids):\n","    words = tokenizer.tokenize(text)\n","    word_ids = tokenizer.convert_tokens_to_ids(words)  # 単語をインデックスに変換\n","    word_tensor = torch.tensor([word_ids])  # 文章そのもの、テンソルに変換\n","    seg_tensor = torch.tensor([seg_ids])  # どこで前と後ろの文章が区切られるか、同様にテンソルに変換\n","\n","    nsp_model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n","    nsp_model.cuda()  # GPU対応\n","    nsp_model.eval()\n","\n","    x = word_tensor.cuda()  # 文章そのもの、GPU対応\n","    s = seg_tensor.cuda()  # 分割場所、GPU対応\n","\n","    y = nsp_model(x, s)  # 予測\n","    result = torch.softmax(y[0], dim=1)  # 確率表現に変換\n","    print(result)  # Softmaxで確率になっているのでそれを表示\n","    print(str(result[0][0].item()*100) + \"%の確率で連続しています。\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wWogb8nFIQMg"},"source":["`show_continuity`関数に、自然につながる2つの文章を与えると、当然ながら高い値が出る"]},{"cell_type":"code","metadata":{"id":"OaUmof1yF_rD"},"source":["text = \"[CLS] What is baseball ? [SEP] It is a game of hitting the ball with the bat [SEP]\"\n","seg_ids = [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ,1, 1]  # 0:前の文章の単語、1:後の文章の単語\n","show_continuity(text, seg_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1EhNbnlCXfKt"},"source":["なお、いじめてみるとわかるが、それほど賢くはない"]},{"cell_type":"code","metadata":{"id":"y2wtBiZ9WroJ"},"source":["text = \"[CLS] What is baseball ? [SEP] It is a game of hitting the ball with the ball [SEP]\"\n","seg_ids = [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ,1, 1]  # 0:前の文章の単語、1:後の文章の単語\n","show_continuity(text, seg_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LKjotaOCIeeK"},"source":["`show_continuity`関数に、自然につながらない2つの文章を与えてみる"]},{"cell_type":"code","metadata":{"id":"v4qAKBlcGRYb"},"source":["text = \"[CLS] What is baseball ? [SEP] This food is made with flour and milk [SEP]\"\n","seg_ids = [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # 0:前の文章の単語、1:後の文章の単語\n","show_continuity(text, seg_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZLSh7kqtXwLg"},"source":["値が小さくなるため、関連していないとわかる"]},{"cell_type":"markdown","metadata":{"id":"uv454lBK1YCc"},"source":["# ファインチューニング\n","\n","事前学習済みのモデルに、追加で訓練する"]},{"cell_type":"markdown","metadata":{"id":"Dnpf03-e4DVP"},"source":["## ライブラリのインストール\n","\n","PyTorch-Transformaersの他、Transformersというライブラリも存在する\n","- Transformersは、PyTorch-Transformersの上位互換ライブラリとして構築されている(オンラインドキュメントで移行手法が提示されている)\n","- ここではTransformarsを利用する\n","\n","https://huggingface.co/transformers/index.html"]},{"cell_type":"code","metadata":{"id":"jzFkh_XH4DVQ"},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5nP8RaW-SLD-"},"source":["## モデルの読み込み\n","`BertForSequenceClassification`により、事前学習済みのモデルを読み込みます。"]},{"cell_type":"code","metadata":{"id":"36bQQblhwpqT"},"source":["from transformers import BertForSequenceClassification\n","\n","sc_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", return_dict=True)\n","print(sc_model.state_dict().keys())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jQV44nAdV82l"},"source":["## 最適化アルゴリズム\n","最適化アルゴリズムにAdamWを利用する\n","- オリジナルのAdamに対し、重みの減衰式が更新されている"]},{"cell_type":"code","metadata":{"id":"TVoPZhUPw4o7"},"source":["from transformers import AdamW\n","\n","optimizer = AdamW(sc_model.parameters(), lr=1e-5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Z2KV0V8ajTz"},"source":["## Tokenizerの設定\n","\n","- BertTokenizerにより文章を単語に分割、idに変換\n","- BertForSequenceClassificationのモデルを訓練する際には入力の他にAttention maskが必要となるが、BertTokenizerによりmask情報も得ることができる\n","  - 未来の情報を与えないようにする(チートしないようにする)\n","- return_tensorsを\"pt\"とすることで、PyTorch Tensor型を扱うようにする"]},{"cell_type":"code","metadata":{"id":"3bSSCHNExo-I"},"source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","sentences = [\"I love baseball.\", \"I hate baseball.\"]\n","tokenized = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n","print(tokenized)\n","\n","x = tokenized[\"input_ids\"]  # 単語のID(input_ids)を取り出してxすなわち入力とする\n","attention_mask = tokenized[\"attention_mask\"]  # 同様にattention maskも取り出すことができる"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bxYWCYvaQOBB"},"source":["結果を見てみよう\n","\n","- 各単語のIDが次のように得られる `'input_ids': tensor([[ 101, 1045, 2293, 3598, 1012,  102], [ 101, 1045, 5223, 3598, 1012,  102]])`\n","  - 2つ文章があるので別の次元で格納されている\n","\n","- 文章は2つ含まれているが、ここはまとめて一つの文章ID0として扱われている `'token_type_ids': tensor([[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]])`\n","\n","- Attention Maskは、`'attention_mask': tensor([[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]])}`であり、すべて1、つまりマスクしないことを意味している"]},{"cell_type":"markdown","metadata":{"id":"BMhbOWRXXT0n"},"source":["## ファインチューニングの実施\n","\n","転移学習(Transfer Learning)については既に学んだため、ここでは省略する\n","\n","たった2つの文章で訓練するため、全く効果は期待できないが、まずは手法を理解する\n","\n"]},{"cell_type":"code","metadata":{"id":"b1MqceDP1IeR"},"source":["import torch\n","from torch.nn import functional as F\n","import matplotlib.pyplot as plt\n","\n","sc_model.train()  # 訓練モードにする\n","t = torch.tensor([1,0])  # 文章の分類(loveの文章が1でポジティブ、hateの文章が0でネガティブ)\n","weight_record = []  # 重みを記録\n","loss_record = []  # ロスを記録\n","for i in range(100):\n","  y = sc_model(x, attention_mask=attention_mask)\n","  loss = F.cross_entropy(y.logits, t)\n","  loss.backward()\n","  optimizer.step()\n","  weight = sc_model.state_dict()[\"bert.encoder.layer.11.output.dense.weight\"][0][0].item()\n","  weight_record.append(weight)\n","  loss_record.append(loss.detach().numpy()) # とりあえずメモリ共有で、変更する場合は .copy()とする\n","  if i%10 == 0:\n","    print(i, \"-> Loss:\", loss.detach().numpy(), \" Weight:\", weight)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SZl6dJx-VVE9"},"source":["まずLossを見ると、学習が進んで値が小さくなることがわかる"]},{"cell_type":"code","metadata":{"id":"NkoltSohTNSB"},"source":["plt.plot(range(len(loss_record)), loss_record)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eTqCLKvqVZVx"},"source":["次にWeightを見ると、値がどんどん変化して更新されていることがわかる\n","- 追加の訓練により、重みが調整されていく様子が確認できる"]},{"cell_type":"code","metadata":{"id":"EKx0DDv6TPha"},"source":["plt.plot(range(len(weight_record)), weight_record)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vrgegdDZjf8E"},"source":["## ファインチューニングによる感情分析\n","ファインチューニングを活用し、文章の好悪感情を判別できるようにモデルを訓練してみよう"]},{"cell_type":"markdown","metadata":{"id":"m6moZnLFkFwr"},"source":["## ライブラリのインストール\n","\n","nlpライブラリをインストールする\n","- 自然言語処理ライブラリで、付随データセットを利用させてもらうために導入する  "]},{"cell_type":"code","metadata":{"id":"7qg6t5nnBjqs"},"source":["!pip install nlp"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZsgQNMJxpBnW"},"source":["## モデルとTokenizerの読み込み\n","事前学習済みのモデルと、これと紐づいたTokenizerを読み込む\n","- クラス分類を行うためBertForSequenceClassificationを利用する\n","- 高速な字句解析を行うBertTokenizerFastを利用する\n","- いつもと同様、bert-base-uncased 事前学習モデルを読み込む"]},{"cell_type":"code","metadata":{"id":"9R0HK29fHrf3"},"source":["from transformers import BertForSequenceClassification, BertTokenizerFast\n","\n","sc_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n","sc_model.cuda()\n","tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aWCmm2TjqToE"},"source":["## データセットの読み込み\n","\n","nlpライブラリに含まれるIMDbデータセットを利用する\n","- IMDbデータセットは、ポジティブかネガティブの好悪感情を表すラベルが付与された25000の映画レビューコメントデータセット\n","- 好意的なレビューは1、否定的なレビューは0が振られている\n","- 感情分析用では鉄板のデータセット\n","\n","https://www.imdb.com/interfaces/"]},{"cell_type":"code","metadata":{"id":"rfEnNpv9HuXI"},"source":["from nlp import load_dataset\n","\n","train_data, test_data = load_dataset(\"imdb\", split=[\"train\", \"test\"]) # 訓練用と検証用データに分けて読み込む"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7t_nwDeOX2Ok"},"source":["試しにデータを表示されてみる\n","- 英語です、がっかりしましたか？"]},{"cell_type":"code","metadata":{"id":"EAB_DeeTX1uu"},"source":["print(train_data[\"label\"][0], train_data[\"text\"][0])  # 好意的なコメントの例\n","print(train_data[\"label\"][20000], train_data[\"text\"][20000])  # 否定的なコメントの例"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CFIcwJVIaAs-"},"source":["DeepLで訳してみると次のような感じです\n","\n","> 1 ブロムウェル・ハイ」は、カートゥーン・コメディです。ブロムウェル・ハイ』は、『ティーチャーズ』のような学校生活を描いた番組と同時期に放送されていました。私の35年間の教師生活を振り返ると、「ブロムウェル・ハイ」の風刺は「ティーチャーズ」よりもはるかに現実に近いものだと思います。経済的に生き残るために奔走する姿、哀れな教師たちの虚勢を見抜く洞察力のある生徒たち、そしてすべての状況の情けなさは、私が知っている学校とその生徒たちを思い出させてくれます。生徒が何度も学校を燃やそうとしたエピソードを見たとき、すぐに ......... .......... のことを思い出しました。高いですね。古典的なセリフです。検閲官：あなた方の先生の一人をクビにするために来ました。生徒：Bromwell Highへようこそ。私と同年代の大人の多くは、「ブロムウェルハイ」を奇想天外なものだと思っているのではないでしょうか。そうでないのが残念です。\n","\n","> 0 この映画は努力していますが、1960年代のテレビシリーズの面白さが完全に欠けています。私は17歳ですが、ずいぶん前にYouTubeでこのシリーズを見たことがあり、楽しくて仕方がありませんでした。特殊効果は標準的ではなく、平板なカメラワークによって助けられていませんでした。また、「ホームアローン4」、「帽子をかぶった猫」、「きかんしゃトーマス」、「アダムス・ファミリー・リユニオン」などの作品があります。さて、ストーリーのアイデアは良かったのですが、残念ながら出来が悪く、早々に力尽きてしまったので、正直、家族で楽しめる作品ではないと思います。また、ウェイン・ナイトが気合を入れて演じたにもかかわらず、しゃべるスーツにも腹が立ちました。しかし、この映画で最も腹が立ったのは、クリストファー・ロイド、ジェフ・ダニエルズ、ダリル・ハンナという才能ある俳優を無駄にしてしまったことです。ジェフ・ダニエルズはこれまでも良い演技をしてきましたが、彼は何をすべきかわからないようでしたし、エリザベス・ハーリーのキャラクターも残念ながら役立たずでした。ダリル・ハンナは素敵な女優だが、一般的には無視されており、私は彼女が愛の対象になるというアイデアが好きだったが、残念ながら彼女の姿はほとんど見られない。（モンスターの攻撃は、子供たちを魅了するというよりも、怖がらせる可能性が高いのは言うまでもない）同様に、ウォレス・ショーンもある種の政府の工作員として登場する。        1/10 ベサニー・コックス"]},{"cell_type":"markdown","metadata":{"id":"Tz0UpoNQYIzs"},"source":["mapメソッドを利用して各データに前処理を施す\n","- ここではtokenizeを定義し、このtokenizeを全データに施す\n","- tokenizeは読み込んだIMDbのデータをTokenizerで処理し、語句IDに変換する関数である\n","- バッチサイズはデータ全体、つまり全データに対して一気に処理している(順番に取り出して何かするのではないため、これでよい)\n","- \"input_ids\", \"attention_mask\", \"label\"の順番にデータを並べて、PyTorchで利用できるようにPyTorchのDataLoaderと同様の形で出力させる"]},{"cell_type":"code","metadata":{"id":"Z2UD6DkjXzto"},"source":["def tokenize(batch):\n","  return tokenizer(batch[\"text\"], padding=True, truncation=True)\n","\n","train_data = train_data.map(tokenize, batched=True, batch_size=len(train_data))\n","train_data.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n","\n","test_data = test_data.map(tokenize, batched=True, batch_size=len(train_data))\n","test_data.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Y6Fcqmy2rG2"},"source":["## 評価用の関数\n","`sklearn.metrics`を用いてモデル評価のための関数を定義する\n","- バッチ対応されているので楽\n","- 使うのはaccuracy_score"]},{"cell_type":"code","metadata":{"id":"plAZjdkG0FdV"},"source":["from sklearn.metrics import accuracy_score\n","\n","def compute_metrics(result):\n","  labels = result.label_ids  # こちらが正解\n","  preds = result.predictions.argmax(-1)  #  予測値のうち値が最も大きい要素のインデックスを取り出す\n","  acc = accuracy_score(labels, preds)  #  両者を比較する\n","  return {\n","    \"accuracy\": acc,\n","  }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MjLqAVy7z0T3"},"source":["## Trainerの設定\n","Trainerクラス、およびTrainingArgumentsクラスを使用して、訓練を行うTrainerを設定する\n","- Trainerクラスを用いることで、それなりに面倒なファイチューニングを簡単に行うことができる\n","- TrainingArgumentsクラスで、ハイパーパラメータを適宜集約できる\n","  - 様々なファインチューニングで必要なパラメータ調整項目が設けられているので一度マニュアルを確認するとよい\n","  - 学習係数を500ステップまで値を上昇させ、その後weight_decayで下降に転じる手法をとる\n","    - 論文でもそのような方針が示されている\n","    - 複雑である場合、最初の一歩で大きく動かず、まずは確実に最適な方向に向いてから近づけるといった感覚で、これはポテンシャル場のイメージが浮かばないと、これでよいのかどうかはよくわからないところ(ということで論文で言われている通りにする)\n","  - train_batch_sizeのデフォルト値は16であるが、ColaboratoryのGPUをもってしても一部のGPUではメモリ不足となるため8などとする必要があるかもしれない\n","  - 16でおよそ30分弱程度、8で40分程度かかる\n","- ログをlogsフォルダに保存する\n","  - なお、TensorBoardに対応した形で保存してくれる(後で試そう)\n","\n","Trainerを使って訓練する\n","- modelでモデルを指定、ここでは先に宣言したsc_model\n","- argsでハイパーパラメータを指定、ここではtraining_args\n","- compute_metricsで評価用関数を指定\n","- train_datasetで訓練用データを指定\n","- eval_datasetで評価用データを指定"]},{"cell_type":"code","metadata":{"id":"ZhaexaAOI3kV"},"source":["from transformers import Trainer, TrainingArguments\n","\n","training_args = TrainingArguments(\n","  output_dir = \"./results\",\n","  num_train_epochs = 1,\n","  per_device_train_batch_size = 16,  # 大きい方が効率がよいがメモリ制約に引っかかりがちなので注意\n","  per_device_eval_batch_size = 32,\n","  per_gpu_train_batch_size = 8,\n","  warmup_steps = 500,  # 学習係数が0からこのステップ数で上昇\n","  weight_decay = 0.01,  # 重みの減衰率\n","  # evaluate_during_training = True,  # ここの記述はバージョンによっては必要ない\n","  logging_dir = \"./logs\",\n",")\n","\n","trainer = Trainer(\n","  model = sc_model,\n","  args = training_args,\n","  compute_metrics = compute_metrics,\n","  train_dataset = train_data,\n","  eval_dataset = test_data\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o0F5nXKpSCnS"},"source":["## モデルの訓練\n","\n","設定に基づきモデルを訓練する\n","- さすがに専用の統合環境なので、実行時間表示などが美しく、終了予定時刻表示も備わっており素晴らしい"]},{"cell_type":"code","metadata":{"id":"29fkN4UcI4jm"},"source":["trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c76zhkQVS2xZ"},"source":["## モデルの評価\n","Trainerの`evaluate()`メソッドによりモデルを評価する\n","- 今回は評価だけでも10分弱必要\n","- Accuracyは90%を簡単に超える値になるであろう\n","- 実はロスは頭打ちになっておらず、Accuarcyもサチュレーションを起こしてない！\n","  - まだまだ伸びるが、時間が…"]},{"cell_type":"code","metadata":{"id":"wIgke21zI6l_"},"source":["trainer.evaluate()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6EFwqzLRUhaB"},"source":["## TensorBoardによる結果の表示\n","TensorBoardを使ってlogsフォルダに格納された学習過程を表示する"]},{"cell_type":"code","metadata":{"id":"1vv39tuDJq5n"},"source":["%load_ext tensorboard\n","%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eKkp41QX3sk_"},"source":["# 日本語文章の分類\n","\n","日本語のデータセットで学習したBERTモデルをファインチューニングし、ニュース分類を行う\n","\n","Googleは2019年10月25日にBERTを検索エンジンに組み込むなど、広く応用されている\n","- 検索クエリに文章を入力した場合、より適切なページに誘導されるようになっている\n","- LaBSE: Language-agnostic BERT sentence embedding modelを利用\n","  - 109言語を事前学習させ学習データにない言語でも性能を発揮！\n","  - 複数の言語を同一空間の中に表現し、同一意味が近い位置になるように潜在空間を構成している\n","  - 知らない言語でも、その空間上でどこかにマップされれば、その付近の解釈であろうと判断する\n","- BERT採用事例をGoogleなどで検索すると、数多くの企業・サービスがBERTを利用していることがわかる\n","  - 特許検索、Q&A検索、金融、医療、サポート応答など文章検索に広く応用されている"]},{"cell_type":"markdown","metadata":{"id":"5cnfQkyM6Ta1"},"source":["## データセット\n","\n","先にも使ったlivedoorニュースコーパスを用いた記事分類を行う\n","- 精度比較にも丁度良いであろう\n","- シートが分かれているので、残念ながらもう一度ダウンロードが必要"]},{"cell_type":"code","metadata":{"id":"dIkJR55G4S0d"},"source":["import os\n","if not os.path.exists('text/topic-news/LICENSE.txt'):\n","  # ファイルが暗号化されているが、これはgoogle driveによるウィルス誤検出を回避するためである。\n","  #!wget \"https://drive.google.com/uc?export=download&id=15EvNnKB6Y6-jGpo1q6N5BZ8SqMI-xzze\" -O ldcc-20140209.zip\n","  !wget https://keio.box.com/shared/static/agjdm4m93o5lay6k0uy9wfadqi79hwpm -O ldcc-20140209.zip\n","if not os.path.exists('text'):\n","  !unzip -P dataai ldcc-20140209.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ydWZ2QxN66X8"},"source":["## 学習済みモデル\n","Pretrained Japanese BERT modelsを利用\n","- 東北大学、乾(いぬい)研究室が作成した日本語モデル\n","- https://github.com/cl-tohoku/bert-japanese\n"]},{"cell_type":"markdown","metadata":{"id":"q0GgR1DO3slD"},"source":["## ライブラリのインストール\n","\n","いつものようにライブラリTransformers、およびnlpをインストール、その他livedoorのデータを利用するためのdatasets、関連するfugashi、ipadicを導入する\n","- fugashiは、MeCabのwrapperで、形態素解析を簡単に動かしたい場合に役に立つ\n","- ipadicは、IPA辞書（IPADIC）のことで、MeCab公式が推奨している辞書である\n","  - この辞書は形態素解析器ChaSen用辞書として作成されMeCab用に調整されている\n","- このあたりの精度の良い鉄板ライブラリの利用も、従来は苦労して導入していたが、今や簡単に導入できるようになった"]},{"cell_type":"code","metadata":{"id":"7zACFpk33slE"},"source":["!pip install transformers\n","!pip install nlp\n","!pip install datasets\n","!pip install fugashi\n","!pip install ipadic"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zliYGLC5g0h2"},"source":["## データセットの読み込み\n","\n","Googleドライブに保存されている、ニュースのデータセットを読み込む"]},{"cell_type":"code","metadata":{"id":"jPV3qCYs9STS"},"source":["import glob  # ファイルの取得に使用\n","import os\n","\n","path = \"text/\"  # フォルダの場所を指定\n","\n","dir_files = os.listdir(path=path)\n","dirs = [f for f in dir_files if os.path.isdir(os.path.join(path, f))]\n","  # ディレクトリ一覧を取得\n","text_label_data = []  # 文章とラベルのセット\n","dir_count = 0  # ディレクトリ数のカウント\n","file_count= 0  # ファイル数のカウント\n","\n","for i in range(len(dirs)):\n","  dir = dirs[i]\n","  files = glob.glob(path + dir + \"/*.txt\")\n","    # すべてのディレクトリにある.txtファイルの一覧をワイルドカードで取得\n","  dir_count += 1\n","  for file in files:\n","    if os.path.basename(file) == \"LICENSE.txt\":  # LICENSEファイルは無視\n","      continue\n","    with open(file, \"r\") as f:\n","      text = f.readlines()[3:]  # 先頭3行は日付やタイトルなので削除\n","      text = \"\".join(text)  # 配列になるのですべてを結合して一つの文章にする\n","      text = text.translate(str.maketrans({\"\\n\":\"\", \"\\t\":\"\", \"\\r\":\"\", \"\\u3000\":\"\"}))\n","        # 改行やタブ、特殊文字を削除(便利) \n","      text_label_data.append([text, i]) # 本文とラベル情報を合わせてtest_label_dataに追加\n","\n","    file_count += 1\n","    print(\"\\rfiles: \" + str(file_count) + \"dirs: \" + str(dir_count), end=\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QFMGBewUDXqL"},"source":["ファイルは7367個、9個のフォルダつまり分類がある"]},{"cell_type":"markdown","metadata":{"id":"LADy70wOgyXg"},"source":["## データの保存\n","データを訓練データとテストデータに分割し、csvファイルとしてGoogle Driveに保存する\n","\n","これらのファイルを別途取得していれば、この後のセル以降を実行することで学習が可能となる"]},{"cell_type":"code","metadata":{"id":"fIyvN2MT4Unl"},"source":["import csv\n","from sklearn.model_selection import train_test_split\n","\n","news_train, news_test =  train_test_split(text_label_data, shuffle=True)\n","  # 訓練用とテスト用に分割\n","news_path = \"text/\"  # データを保存する場所(揃えてtextにしている)\n","with open(news_path+\"news_train.csv\", \"w\") as f: # 訓練データ\n","    writer = csv.writer(f)\n","    writer.writerows(news_train)\n","with open(news_path+\"news_test.csv\", \"w\") as f: # 検証データ\n","    writer = csv.writer(f)\n","    writer.writerows(news_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pboquD6L3slI"},"source":["## モデルとTokenizerの読み込み\n","日本語の事前学習済みモデルと、これと紐づいたTokenizerを読み込む\n","- BertJapaneseTokenizerとして日本語対応分かち書きライブラリを利用する\n","  - `cl-tohoku/bert-base-japanese-whole-word-masking`と日本語対応を読み込む\n","  - 東北大乾研モデル\n","- BERTモデルも日本語対応版を利用する\n","  - `cl-tohoku/bert-base-japanese-whole-word-masking`\n","  - ラベル数は9である\n","  "]},{"cell_type":"code","metadata":{"id":"aE3Ubf803slI"},"source":["from transformers import BertForSequenceClassification, BertJapaneseTokenizer\n","\n","sc_model = BertForSequenceClassification.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\", num_labels=9)\n","sc_model.cuda()\n","tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8MTRVx2x3slJ"},"source":["## データセットの読み込み\n","保存されたニュースのデータを読み込む\n","- 前回同様tokenize関数を定義し、これをmapしてデータを作成する\n","  - 文章の最大長は512であるが処理負荷を考え128に限定している(長くしてもよいだろう)\n","- load_datasetでは、csv形式と指定してファイルパスからデータを読み込む\n","  - カラム名はファイルを見ればわかるが、textとlabelとする\n","  - splitで訓練データと検証データに分けることができるが、すでに分割済みでありここでは訓練データを読み込むためtrainとだけ指定している\n","  - ここでも全体一気にmapする\n","  - set.formatでPyTorchを使うため'torch'と指定、カラムはinput_idsとlabelとする\n","- 同様に訓練データもロードする\n","\n"]},{"cell_type":"code","metadata":{"id":"F4ePBGEK3slJ"},"source":["from datasets import load_dataset\n","\n","def tokenize(batch):\n","  return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=128)\n","    \n","news_path = \"text/\"\n","train_data = load_dataset(\"csv\", data_files=news_path+\"news_train.csv\", column_names=[\"text\", \"label\"], split=\"train\")\n","train_data = train_data.map(tokenize, batched=True, batch_size=len(train_data))\n","train_data.set_format(\"torch\", columns=[\"input_ids\", \"label\"])\n","test_data = load_dataset(\"csv\", data_files=news_path+\"news_test.csv\", column_names=[\"text\", \"label\"], split=\"train\")\n","test_data = test_data.map(tokenize, batched=True, batch_size=len(test_data))\n","test_data.set_format(\"torch\", columns=[\"input_ids\", \"label\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3qnKn6ix3slJ"},"source":["## 評価用の関数\n","`sklearn.metrics`を使用し、モデルを評価するための関数を定義する\n","- 前回と同じ\n"]},{"cell_type":"code","metadata":{"id":"SzYFxTOr3slL"},"source":["from sklearn.metrics import accuracy_score\n","\n","def compute_metrics(result):\n","  labels = result.label_ids\n","  preds = result.predictions.argmax(-1)\n","  acc = accuracy_score(labels, preds)\n","  return {\n","    \"accuracy\": acc,\n","  }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oOIxmuQm3slL"},"source":["## Trainerの設定\n","Trainerクラス、およびTrainingArgumentsクラスを使用して、訓練を行うTrainerを設定する\n","- 前回と同じ"]},{"cell_type":"code","metadata":{"id":"nr6NbLdz3slL"},"source":["from transformers import Trainer, TrainingArguments\n","\n","training_args = TrainingArguments(\n","  output_dir = \"./results\",\n","  num_train_epochs = 2,\n","  per_device_train_batch_size = 16, # メモリが溢れる場合は8にしてみよう\n","  per_device_eval_batch_size = 32,\n","  warmup_steps = 500,  # 学習係数が0からこのステップ数で上昇\n","  weight_decay = 0.01,  # 重みの減衰率\n","  # evaluate_during_training = True,  # ここの記述はバージョンによっては必要ありません\n","  logging_dir = \"./logs\",\n",")\n","\n","trainer = Trainer(\n","  model = sc_model,\n","  args = training_args,\n","  compute_metrics = compute_metrics,\n","  train_dataset = train_data,\n","  eval_dataset = test_data,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_JiUJk753slL"},"source":["## モデルの訓練\n","\n","設定に基づきファインチューニングを行う\n","- バッチサイズが16であれば今度は5分程度で終了する\n"]},{"cell_type":"code","metadata":{"id":"QCFMAPyT3slL"},"source":["trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4euRWG8B3slM"},"source":["## モデルの評価\n","\n","Trainerの`evaluate()`メソッドによりモデルを評価する"]},{"cell_type":"code","metadata":{"id":"pf2nYlRy3slM"},"source":["trainer.evaluate()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bw1B7xGz3slM"},"source":["## TensorBoardによる結果の表示\n","TensorBoardを使ってlogsフォルダに格納された学習過程を表示する"]},{"cell_type":"code","metadata":{"id":"Svcu7hrY3slN"},"source":["%load_ext tensorboard\n","%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a-BscHjHxs0H"},"source":["## モデルの保存\n","訓練済みのモデルを保存する\n","- 一般的なPyTorchの保存は既に扱った\n","- 同様に保存して読み込んでみよう"]},{"cell_type":"code","metadata":{"id":"UvwVcXuIyH7V"},"source":["news_path = \"text/\"\n","sc_model.save_pretrained(news_path)\n","tokenizer.save_pretrained(news_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zZuJCZBK0RJx"},"source":["## モデルの読み込み\n","保存済みのモデルを読み込む"]},{"cell_type":"code","metadata":{"id":"ZWtcQRuP0X45"},"source":["loaded_model = BertForSequenceClassification.from_pretrained(news_path) \n","loaded_model.cuda()\n","loaded_tokenizer = BertJapaneseTokenizer.from_pretrained(news_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rq2zZ99R3Hs7"},"source":["## 日本語ニュースの分類\n","\n","読み込んだモデルを使ってニュースを分類する\n","- 基本的には既に扱った通り\n","\n","一つ記事を選択する"]},{"cell_type":"code","metadata":{"id":"dFOIjY511WVK"},"source":["import glob  # ファイルの取得に使用\n","import os\n","import torch\n","\n","category = \"movie-enter\"  # 映画に関する記事を取り出す\n","sample_path = \"text/\"  # フォルダの場所を指定\n","files = glob.glob(sample_path + category + \"/*.txt\")  # ファイルの一覧\n","file = files[12]  # 適当なニュース(ここでは12番目)\n","\n","dir_files = os.listdir(path=sample_path)\n","dirs = [f for f in dir_files if os.path.isdir(os.path.join(sample_path, f))]  # ディレクトリ一覧\n","\n","with open(file, \"r\") as f:\n","  sample_text = f.readlines()[3:]\n","  sample_text = \"\".join(sample_text)\n","  sample_text = sample_text.translate(str.maketrans({\"\\n\":\"\", \"\\t\":\"\", \"\\r\":\"\", \"\\u3000\":\"\"})) \n","\n","print(sample_text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oPZnUCQRhbhc"},"source":["では、このテキストの記事を当ててみよう"]},{"cell_type":"code","metadata":{"id":"z2uIfmGzhXk5"},"source":["max_length = 512\n","words = loaded_tokenizer.tokenize(sample_text)\n","word_ids = loaded_tokenizer.convert_tokens_to_ids(words)  # 単語をインデックスに変換\n","word_tensor = torch.tensor([word_ids[:max_length]])  # テンソルに変換\n","\n","x = word_tensor.cuda()  # GPU対応\n","y = loaded_model(x)  # 予測\n","pred = y[0].argmax(-1)  # 最大値のインデックス\n","print(\"result:\", dirs[pred])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oWcdxzrUhho0"},"source":["他の記事についても試してみるとよい\n","- また、自分で勝手に記事を考え、分類させてみるのも面白いであろう"]},{"cell_type":"markdown","metadata":{"id":"FgG8tLEZjDmw"},"source":["## BERTSUM\n","\n","文章の要約を行うことが最近よく行われている\n","- これを行う専用のBERTが文章要約用のBERTSUMである\n","- BERTSUMは、文章の開始を意味するCLSを、文章の区切りにも挿入する\n","- 文章がどのセグメントに属するかを指定する際、偶数番目と奇数番目の文章でそれぞれ例えば$E_A$や$E_B$といった具合に異なるセグメントに所属させる\n","- Encoder-Decoderの形態を有し、Encoderは事前学習モデルを用い、Decoderはゼロから学習させる\n","- 英語はもちろんのこと、日本語の実装もgithubで公開されている\n","  - 英語 https://git.io/JMDSL\n","  - 日本語 https://git.io/JMDSG\n"]},{"cell_type":"markdown","metadata":{"id":"6xowrwknU962"},"source":["# まとめ\n","\n","今回は、自動翻訳や記事分類、BERTなど自然言語処理の基本について扱ったが、大量の優れた学習データと、さらに多いノード数を持つ構造を利用できれば、翻訳性能や分類性能を向上できることがわかるであろう\n","- 結局、最後はデータなのである\n","- さらに大量のデータを利用したBERTモデルの強力さがわかったであろう\n","\n","応用として、例えば、最近よく見る自動レスポンスチャットのように、ある質問に対する回答を導き出すこともできる\n","- Amazon Alexaや、Goole アシスタントのようなことも、なんとなくイメージできるようになったのではないだろうか"]},{"cell_type":"markdown","metadata":{"id":"sEmx7ax0h_EM"},"source":["# 課題\n","\n","次のようなプログラムを作成し実行しなさい\n","\n","- 上記の記事分類の保存済みモデルをロードする\n","  - 保存済みモデルは、ダウンロードして自分で別途保存し、この別途保存したファイルをcontext内にドロップして配置してからプログラムを実行するとよい\n","  - 保存済みモデルをどのように与えるかは各自で考えること\n","  - 例えば、Google Driveをマウントしてもよい\n","- 3つの異なるトピックのランダムな記事を選択し、実際に分類が正解するか確認しなさい\n","- 2つの「自分で考えた記事」を用いて分類し、その分類について100字未満で簡単に考察しなさい"]}]}