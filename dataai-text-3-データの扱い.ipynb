{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dataai-text-3-データの扱い.ipynb","private_outputs":true,"provenance":[{"file_id":"1eqzU45uapJ67aWx2E-e3Z7XnO0VGwG1G","timestamp":1551208807345}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"9xrkJ1O1sOyE"},"source":["#@title Data-AI（必ず自分の名前・学籍番号を入力すること） { run: \"auto\", display-mode: \"form\" }\n","\n","import urllib.request as ur\n","import urllib.parse as up\n","Name = '\\u6C5F\\u6D32\\u51FA\\u4E95 \\u592A\\u90CE' #@param {type:\"string\"}\n","EName = 'Esudei Taro' #@param {type:\"string\"}\n","StudentID = '87654321' #@param {type:\"string\"}\n","Addrp = !cat /sys/class/net/eth0/address\n","Addr = Addrp[0]\n","url = 'https://class.west.sd.keio.ac.jp/classroll.php'\n","params = {'class':'dataai','name':Name,'ename':EName,'id':StudentID,'addr':Addr,\n","           'page':'dataai-text-3','token':'71873536'}\n","data = up.urlencode(params).encode('utf-8')\n","#headers = {'itmes','application/x-www-form-urlencoded'}\n","req = ur.Request(url, data=data)\n","res = ur.urlopen(req)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OKp1AYDI02dB"},"source":["---\n","> デジタルデータなんて所詮0か1さ。とはいえ、0と1の羅列だけでは意味をなさない。その意味やルールを与えたのは人間、自分が作った意味やルールに振り回されるなんて、滑稽じゃあないか。\n","---"]},{"cell_type":"markdown","metadata":{"id":"7AWhOioVQcvz"},"source":["# データの扱い\n","\n","機械学習以外でも、データを整理したり、まとめたり、内容を知るといった処理は重要である\\\n","ここでは、データを扱う・確認する・補正するという観点についてまとめる"]},{"cell_type":"markdown","metadata":{"id":"5FLGRB8os-Fh"},"source":["## まずは準備"]},{"cell_type":"code","metadata":{"id":"ZB6IWekhs9cw"},"source":["import matplotlib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import sklearn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MHGibT0-mvjA"},"source":["# データ取得\n"]},{"cell_type":"markdown","metadata":{"id":"3pdG03aumvjQ"},"source":["## データセットの読み込み"]},{"cell_type":"markdown","metadata":{"id":"IHH-ZuyRmvjS"},"source":["ここでは、scikit-learnに含まれているデータセットを利用する\n","\n","ここで利用するのは、次のデータである\n","\n","- Linnerud (生理学的特徴と運動能力の関係)  \n","ノースカロライナ州立大学の A. C. linnerud 博士が作成した、20 人の成人男性に対してフィットネスクラブで測定した 3 つの生理学的特徴と 3 つの運動能力の関係を表すデータである\n","\n","> データセットの詳細\n","\n","| | |\n","|:-:|:-:|\n","|レコード数|\t20|\n","|カラム数|\t説明変数:3, 目的変数: 3|\n","|主な用途|\t多変数回帰 (multivariate regression)|\n","\n","> 説明変数の構成\n","\n","| | |\n","|:-:|:-:|\n","|Weight\t体重\n","|Waist\tウエスト (胴囲)\n","|Pulse\t脈拍\n","\n","> 目的変数の構成\n","\n","| | |\n","|:-:|:-:|\n","|Chins|\t懸垂の回数|\n","|Situps|\t腹筋の回数|\n","|Jumps|\t跳躍|"]},{"cell_type":"markdown","metadata":{"id":"n0dO_-wORFBL"},"source":["データの利用には、`from パッケージ名 import ライブラリ名`として、ライブラリが複数まとめられたパッケージから特定のライブラリをインポートする\n","\n","なお、\n","- sklearn.datasets：scikit-learnに組み込まれているデータセットのパッケージ\n","- load_linnerud：linnerudデータセットを用いるためのライブラリ\n","である"]},{"cell_type":"code","metadata":{"id":"oOKKT3tkmvjU"},"source":["from sklearn.datasets import load_linnerud\n","linnerud = load_linnerud()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aKb2y7admvja"},"source":["ここで、説明変数としてWeight(体重)、Waist(胴囲)、Pulse(脈拍)の三つ、目的変数としてChins(懸垂の回数)、Situps(腹筋の回数)、Jumps(跳躍)を与える\n","\n","linnerud.dataは以下のようなarray型のデータが表示される\n","```\n","array([[   5.,  162.,   60.],  \n","       [   2.,  110.,   60.],  \n","       ...\n","       [  12.,  101.,  101.],  \n","```\n","\n","linnerud.targetを実行すると以下のようなarray型のデータが表示される\n","```\n","array([[ 191.,   36.,   50.],\n","       [ 189.,   37.,   52.],\n","       [ 193.,   38.,   58.],\n","       ...\n","```"]},{"cell_type":"code","metadata":{"id":"FOj0j_MYqkGy"},"source":["linnerud.target_names"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CBZCky6ER_5q"},"source":["linnerud.data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6_r8HxMASD54"},"source":["linnerud.target"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yS9YKPyamvjg"},"source":["データの表示方法には複数存在することに注意する\n","- array：Pythonで使われる配列である。なお、型は無表示である\n","- ndarray：配列同様、多次元のデータを格納できる構造でnumpyと対応している。なお、型はarrayと表示される\n","- DataFrame：2次元のデータを格納できる構造．pandasと対応している．なお、型はpandas.core.frame.DataFrameと表示される\n","\n","相互変換であるが、\n","\n","- pythonリスト型listをNumPy配列ndarrayに変換するにはnumpy.array()とする\n","- NumPy配列ndarrayをリスト型listに変換には、tolist()メソッドを使う\n","- ndarrayから、pandasの配列に変換するには、DataFrameメソッドを利用する\n","\n","\n","\n","pd.DataFrame(numpyデータなど, columns=カラム名)  \n","- 一番左の1列をインデックスと呼び、自動でナンバリングされる。ただし0オリジンであることに注意すること\n","- 一番上の1行をカラムと呼び、それぞれをカラム名を定義する必要がある\n","  \n","linnerud.feature_namesを実行すると以下のようなarray型のデータが表示される\n","\n","`['Chins', 'Situps', 'Jumps']``\n","- Chins：懸垂の回数\n","- Situps：腹筋の回数\n","- Jumps：跳躍"]},{"cell_type":"code","metadata":{"id":"lOPaLge9vUkO"},"source":["data = pd.DataFrame(linnerud.data, columns=linnerud.feature_names)\n","data.head(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iaD1g8_7mvjt"},"source":["linnerud.target_namesを実行すると以下のようなarray型のデータが表示される \n","```['Weight', 'Waist', 'Pulse']```\n","- Weight：体重\n","- Waist：胴囲\n","- Pulse：脈拍\n","である"]},{"cell_type":"code","metadata":{"id":"zP_WCDHzvXkr"},"source":["target = pd.DataFrame(linnerud.target, columns=linnerud.target_names)\n","target.head(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bstXvrzOvkQE"},"source":["## DataFrameの作成\n","\n","dataとtargetを横方向に結合しdatargetという名前のDataFrameを作成する"]},{"cell_type":"markdown","metadata":{"id":"9wqzBDSjmvj4"},"source":["\n","\n","- A.join(B)\n","- pd.concat([A, B], axis=1)\n","- pd.merge(A, B, right_index=True, left_index=True)\n","    - AもBもDataFrame"]},{"cell_type":"markdown","metadata":{"id":"2GsDb4IYywVD"},"source":["今回はjoinがもっとも素直な方法である。なお、how='outer'は省略できる"]},{"cell_type":"code","metadata":{"id":"kJbS1-AQmvj4"},"source":["datarget1 = data.join(target, how='outer')\n","datarget1.head(3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V-tU7oRRwiKQ"},"source":["concatは、indexが共通かつ、axis=1を省略すると縦方向の連結になる\n","\n","inner joinなど様々な結合方法があるので整理しておくと良い"]},{"cell_type":"code","metadata":{"id":"u53g03jXv3e-"},"source":["datarget2 = pd.concat([data, target], axis=1)\n","datarget2.head(3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jpUSOYSawpA2"},"source":["datarget2d = pd.concat([data[0:2], target[0:2]], axis=0)\n","datarget2d"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8K_2U0lCydX-"},"source":["本来、margeは共通のキーがあるときに利用する。したがって、今回は無理やりな例になる"]},{"cell_type":"code","metadata":{"id":"5jvClpSDwCgq"},"source":["datarget3 = pd.merge(data, target, right_index=True, left_index=True)\n","datarget3.head(3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mMJK6JiJvY9E"},"source":[" ## CSVデータの読み込み\n"," \n","CSVからデータを読み込む場合について補足する\n","\n","- ここでは、numpyが提供するloadtxtについて説明する\n","\n","例えば、1列目が目的変数、2列目以降が説明変数となる\n","\n","```\n","foo.csv\n","1,1.2,12,...\n","1,2.0,5,...\n","1,1.5,3,...\n","2,1.8,1,...\n","2,0.5,10,...\n","2,1.0,8,...\n","```\n","\n","というCSVファイルがあった場合、\n","\n","```\n","data = np.loadtxt('foo.csv', delimiter=',', dtype=float)\n","labels = data[:, 0:1] # 目的変数を取り出す\n","features = preprocessing.minmax_scale(data[:, 1:]) # 説明変数を取り出した上でスケーリング\n","x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.3) # トレーニングデータとテストデータに分割\n","```\n","\n","とするとよい\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hZyqAv00z-K1"},"source":["### numpyでのコロン : およびカンマ , による配列表記\n","\n","最初のコロンはスライシングのコロン(`x[1:]`におけるコロンと同じ)である。Pythonではスライシングはカンマで区切り複数記載できる\n","\n","次の例では、2から5まで2飛ばし、つまり、2と4個目の要素がスライシングで選ばれる\n","\n","- つまり、0から数えるので`[[7,  8,  9], [13, 14, 15]]`となる。この0列目なので、`[7,13]`となる\n","\n","- 要するに、第一行だけがほしい場合は`a[0]`、第一列だけがほしい場合は`a[:,0]`となる"]},{"cell_type":"code","metadata":{"id":"MlOMwWPk1m6-"},"source":["a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15]])\n","a[2:5:2, 0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XC4xBgx-NiA3"},"source":["データの読み込み、確認手法について以上である"]},{"cell_type":"markdown","metadata":{"id":"MnNuHzZbmvkX"},"source":["# データ加工"]},{"cell_type":"markdown","metadata":{"id":"CvkpmoOM_qxR"},"source":["## 欠損値補完"]},{"cell_type":"markdown","metadata":{"id":"vTSpzreO_urZ"},"source":["実際に用いるデータでは欠損値が含まれることが普通にある\n","- データを扱うには欠損値を処理しておく必要がある\n","- 上記データは欠損値がないため、欠損値のあるデータを用いて説明する\n","\n","今回は、自動車事故のデータセットを使う"]},{"cell_type":"markdown","metadata":{"id":"nC49AYpZFbDF"},"source":["ここでQuilt(キルト)を利用する。Quiltはデータセットの公開およびバージョン管理のためのツールであり、Python,Pandas,Jupyter等を使って機械学習の開発・研究をするユーザのために設計されている"]},{"cell_type":"code","metadata":{"id":"H_US7O8mFtVy"},"source":["has_quilt = !if [[ -f /usr/local/bin/quilt ]]; then echo 1; fi;\n","if not has_quilt:\n","  !pip install -q quilt\n","  !quilt install --force ResidentMario/missingno_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rcGIH1-CAbx-"},"source":["Quiltにある自動車事故データセットをメモリ内に取り込む。"]},{"cell_type":"code","metadata":{"id":"w9hQIrLkGEiJ"},"source":["from quilt.data.ResidentMario import missingno_data\n","collisions = missingno_data.nyc_collision_factors()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d9-72M3oGnu6"},"source":["中身を確認する。"]},{"cell_type":"code","metadata":{"id":"DzBNSvS0GoPs"},"source":["collisions.head(2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SkQVv-HPGetb"},"source":["大量の欠損値があるが、欠損値がnanとして記されているので、これを、データとしてのnan(np.nan)へ変更する"]},{"cell_type":"code","metadata":{"id":"YuPmUXFAGeRC"},"source":["collisions.replace(\"nan\", np.nan, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sdpZuFNAG-bZ"},"source":["表示させると、nanが、NaNに変わったことがわかる\n","\n","- NaNは、numpyにおけるNaNという特別な値を意味し、欠損値であることを示している\n","- nanは文字であり、「本当にnanという意味のある情報、例えばカレーはriceかnanかというアンケート」と混乱する場合がある\n","  - そこで、あえて特別な値を用いる必要がある\n","\n","- NaN（Not a Number、非数、ナン）は、コンピュータにおいて、主に浮動小数点演算の結果として、不正なオペランドを与えられたために生じた場合を表す値またはシンボルであり、計算機における数字の表現の一つとして標準化されている"]},{"cell_type":"code","metadata":{"id":"jX9iIYvpG5ge"},"source":["collisions.head(2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xVxSjc0sLDII"},"source":["collisions.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rUBEKbn6BHpC"},"source":["### 欠損値の数を把握する\n","\n","データの数が示されているが、総数は7303で、それに満たないデータには欠損があることがわかる\n","- かなりのデータが欠損していることがわかる"]},{"cell_type":"code","metadata":{"id":"oNvLZ1dZBFFv"},"source":["collisions.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PO6y-WwHBk36"},"source":["### 欠損の状況を把握する\n","\n","方針をたてよう\n","\n","- まとめてデータが欠落していれば(バースト欠損)、そこだけ除くと良いであろう\n","\n","- ほぼランダムに欠損がある場合は、当該場所の値を補完すればよいであろう\n","  - 補完には、重回帰解析による推定値で保管するなどが考えられる\n","\n","- いずれにしても、欠損の場所や欠落状況を把握することは重要\n","\n","pythonでは、様々な可視化ツールが用意されており、これらを利用できるメリットは大きい\n","\n","- 一般的なのはseaborn.heatmapを用いて視覚的に解析することであるが、ここではさらに強力なツールを紹介する\n","\n","こういう便利なツールが揃っており作る必要がない点もpythonやscikit-learnを利用する重要なメリット\n","- 存在を知っていること、もっといえば、更新が速いため、そういった更新や新ツールへの適応が重要である\n","\n","様々な可視化ができるmissingnoというmoduleがある\n","- Google Colaboratoryではデフォルトでインストールされている\n","- こちらの方が見やすく、また、正確で機能が高い\n","- 使えるものをわざわざ作るな！作る時間があれば使いこなせ！ないなら作れ！作る状況がでるぐらいに学べ！"]},{"cell_type":"code","metadata":{"id":"sfX5-ERgDXiV"},"source":["import missingno as msno\n","msno.matrix(collisions)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5l1UCFFqDt01"},"source":["右側のバーは、各行ごとに非欠損値の個数を線グラフとして表示しており、欠損値の重なりが多いとグラフは小さな値になる\\\n","**非欠損値であることに注意**すること"]},{"cell_type":"markdown","metadata":{"id":"WbSXHMJ2ErmF"},"source":["さらに、heamapメソッドを利用することで欠損値と発生箇所の相関もわかる。見方の概要は次のとおりである\n","\n","- ヒートマップには相関値が示される。-1から1の値をとり、丸めて0になる場合 (>-0.05 or < 0.05)、数値は表示されない\n","\n","- 欠損値のない値はヒートマップに現れない\n","\n","- -1の場合は左のカラムが欠損している一方で、下のカラムが全て欠損していないことを示す\n","\n","- 1の場合は、左のカラムが欠損しており、さらに、下のカラムも全て欠損していることを表す"]},{"cell_type":"code","metadata":{"id":"66wMcrcmEvYS"},"source":["msno.heatmap(collisions)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z5t6czwzMHaZ"},"source":["より地道な方法として、単純にisnull()で NULLの数を調べ、それをsum()で数え上げてみる"]},{"cell_type":"code","metadata":{"id":"BHSY-20GMRHg"},"source":["collisions.isnull().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GV0QngKxMYMi"},"source":["それぞれのカラム毎にどのくらいのnullが存在し、また値のバリエーションがどの程度あるか調べるには次のようにすると良い\n","- `count()`: 欠損値NaNではない要素の数をpandas.Seriesやスカラー値として取得する\n","- `isnull()`: isnaと同じで欠損値であればTrueになる\n","- `sum()`： Trueつまり1の数を数え上げるため、Trueの総数を計算することになる"]},{"cell_type":"code","metadata":{"id":"R7ZZb2QZMXWj"},"source":["def chknull(df):\n","  for i in df.columns:\n","    nall = df[i].count()\n","    nnul = df[i].isnull().sum()\n","    if(nall == 0):\n","      np = \"-\"\n","    else:\n","      np = str(round(float(nnul)/float(len(df))*100, 2))\n","    print(\"* \" + i + \"\\t#:\" + str(nall) +\n","      \"\\t# of NULL:\" + str(nnul)+\n","      \"\\t% of NULL:\" + np +\n","      \"\\t# of Orig:\" + str(df[i].value_counts().count()))\n","chknull(collisions)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gfPGQftbRbzU"},"source":["## 削除\n","\n","削除には、pandasのdropnaメソッドを利用する\n","\n","#### まとめて削除する\n","- `how='all'`とするとすべての値が欠損値である行が削除される\n","- `axis=1`とすると、すべての値が値が欠損値である列が削除される\n","- `how='any'`とすると、一つでも欠損値がある行が削除され、行ではaxis=1が利用できる\n","- `thresh=3`とすると欠損値ではない要素の数が3個以上含まれている行が残り、それ以外の行（欠損値ではない要素の数が2個以下の行）が削除される\n","\n","なお、行も列も消したい場合の推奨方法は、メソッドを2回呼ぶことである\n","\n","```\n","df.dropna(how='all').dropna(how='all', axis=1)\n","```\n","\n","#### 狙って削除する\n","\n","特定の行・列を基準に削除したい場合は、引数subsetに対象とする行ラベル・列ラベルをリストで指定する\\\n","例えば`subset=['name']`と指定する\n","\n","デフォルトではsubsetで指定した列のいずれかに欠損値がある行を削除する\n","\n","- `how='all'`とすると、指定した列すべてが欠損値である行のみを削除する\n","- `axis=1`とすると、subsetで指定した行に欠損値がある列を削除することができ、引数howも利用できる"]},{"cell_type":"markdown","metadata":{"id":"5g35_pFCgXhw"},"source":["ZIP CODEがない場合、BOROUGH（自治区）もないため、単純に削除する\\\n","削除した結果をcollisions_r1 に代入する"]},{"cell_type":"code","metadata":{"id":"qnZ3YyRDRtJI"},"source":["collisions_r1 = collisions.dropna(subset=['ZIP CODE'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7WbNPCJSjdU9"},"source":["全く値の入っていない列があるため、これらを削除し、削除した結果をcollisions_r2に入れる\n","- NUMBER OF CYCLISTS INJURED\tなどが削除されていることを確認する"]},{"cell_type":"code","metadata":{"id":"sI81WD51jj4v"},"source":["collisions_r2 = collisions_r1.dropna(how='all', axis=1)\n","collisions_r2.head(2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0YA3-WpCOALT"},"source":["collisions_r2.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yiOxocaxkXtv"},"source":["## 補完\n","\n","データ量が少ない場合などでは、単純に削除するとデータ量が少なくなりムダかつ妥当性や学習結果に支障が出る可能性がある\n","- そこで、なにかしらデータを補完する\n","- データを眺めたあと、どのような形で補完するかを決定すること\n","\n","以下に単純な数値データの補完方法について述べる\n","\n","- ラベル情報（文字情報など）の補完には、クラスタリングといった手法が別途必要となり、特に大量に補完する場合は注意が必要である\n","\n","そもそも、その準備したデータに問題がないのかを常に疑うこと\n","- 大量に補完したデータを学習に利用すれば、「学習で補完したデータを使って学習する」つまり、学習に都合の良いデータを使って学習しており、本来の意味が薄れている\n","- ここでは、kaggleを使う\n","\n","これで、\n","- Scikit-learn 付属のデータセット\n","- Quiltのデータセット\n","- Kaggleのデータセット\n","\n","以上が利用できるようになったはずである\n","\n","特にKaggleは、「The Home of Data Science & Machine Learning」（データサイエンスと機械学習の家）と題されている通り、世界中の機械学習・データサイエンスに携わっている約40万人の方が集まるコミニティーである\n","\n","- 企業や政府などの組織とデータ分析のプロであるデータサイエンティスト/機械学習エンジニアを繋げるプラットフォームであり、企業や政府がコンペ形式（競争形式）で課題を提示し、賞金と引き換えに最も精度の高い分析モデルを買い取るといったサービスが提供されている\n","- Kaggleコンペで上位に入ると、著名企業からハンティングされる可能性あり\n","\n","その中でもよく学習や例題に使われる、Titanic: Machine Learning from Disasterを利用する\n","- これは、著名な豪華客船タイタニック号が沈没したときの乗客員のデータである\n","- 多くの乗客が死亡してしたため、後から確認できない項目も多く欠損値が含まれるデータとなっている\n","\n","[https://www.kaggle.com/c/titanic](https://www.kaggle.com/c/titanic)本来はこちらから入手するが、入手にはアカウントの登録が必要である\n","\n","- アクセスしたら、Dataタブを選び、Data Sourcesから、test.csvとtrain.csvをクリックして、ダウンロードする\n","\n","同じデータは、こちらにも配置しているので、以下を用いるとよい\n","- なお、コードは自動でダウンロードするように設計しているのでダウンロードする必要はない\n","  - [train.csvはここをクリックするとダウンロードできる](http://class.west.sd.keio.ac.jp/dataai/data/train.csv)\n","  - [test.csvはここをクリックするとダウンロードできる](http://class.west.sd.keio.ac.jp/dataai/data/train.csv)"]},{"cell_type":"markdown","metadata":{"id":"FpLg-TwI98Gn"},"source":["簡単に済ますには、次のセルを実行して取得し、/contentの中に保存する\n","- データを読み込むと自動的に/contentの中に入る\n","- 読み込んだら、ファイルから、train.csvをクリックして中身を見て、Ageの欠損について確認する"]},{"cell_type":"code","metadata":{"id":"hebnvnTY-Gcv"},"source":["import os\n","if not os.path.exists('train.csv'):\n","  #!wget \"https://drive.google.com/uc?export=download&id=1OnwqCkFYr49GuEeB6NSHt7kzOOEy80oF\" -O train.csv\n","  !wget https://keio.box.com/shared/static/h4xfiaehi5vt9exmz246gnvm23qnpjlb -O train.csv\n","if not os.path.exists('test.csv'):\n","  #!wget \"https://drive.google.com/uc?export=download&id=1OosaY5iW9O00IR-TwJQMb3o1ydVUG33a\" -O test.csv\n","  !wget https://keio.box.com/shared/static/plxazmlkvooo35id3hcetdixm39r2gs6 -O test.csv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2clV9-IlWVb7"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","train_df = pd.read_csv(\"train.csv\")\n","test_df = pd.read_csv(\"test.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_DtqvBzRS9L3"},"source":["### 単純に平均値/最頻値を入れる\n","単純に平均値/最頻値をいれてしまう方法である\n","- まずデータをコピーしている\n","- pythonで=を使って値をコピーすると痛い目に合うのは**あるある**である\n","  - `a = b`はコピーではなく参照、つまり同じ変数の保存場所をどちらも指しているので、bを変更するとaも変更されてしまう\n","  - `a = b.copy()`とするとaは別メモリに保存されbの変更の影響を受けない\n","  - ところが、オブジェクトの中にオブジェクトがある場合(配列の配列など)は、残念ながらその中のオブジェクトを変更すると変更されてしまう\n","  - そこで、`import copy`として、`a = copy.deepcopy(b)`とすると、完全に中身も含めてコピーされる"]},{"cell_type":"code","metadata":{"id":"oZfisoMOTCrb"},"source":["import copy\n","train_df_original = copy.deepcopy(train_df)\n","plt.hist(train_df_original[\"Age\"].dropna(), alpha=0.2,color=\"r\") #もともとのグラフを赤で描画\n","plt.hist(train_df[\"Age\"].fillna(train_df[\"Age\"].mean()),alpha=0.2,color=\"b\") #平均値をいれたグラフを青で描画"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PWCA4iL6Cp5l"},"source":["### 平均からばらつきを考慮して補完する\n","平均値から標準偏差でばらつきを考慮して補完する方法を示す\n","- まずは、共通する処理として、平均・標準偏差・null数を取得しておく"]},{"cell_type":"code","metadata":{"id":"52tj-tu1C6Zy"},"source":["Age_average = train_df[\"Age\"].mean() #平均値\n","Age_std = train_df[\"Age\"].std()  #標準偏差\n","Age_nullcount = train_df[\"Age\"].isnull().sum() #null値の数＝補完する数"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oE1TNXLQDiYd"},"source":["次に、正規分布に従うとし、標準偏差の範囲内でランダムに数字を作る"]},{"cell_type":"code","metadata":{"id":"tLugErhjDnMQ"},"source":["rand = np.random.randint(Age_average - Age_std, Age_average + Age_std , size = Age_nullcount)\n","#Ageの欠損値\n","train_df_rand = copy.deepcopy(train_df_original)\n","train_df_rand[\"Age\"][np.isnan(train_df[\"Age\"])] = rand\n","#グラフ描画\n","plt.hist(train_df_original[\"Age\"].dropna(), alpha=0.2,color=\"r\")\n","plt.hist(train_df_rand[\"Age\"],alpha=0.2,color=\"b\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7F6MQUCcFzLf"},"source":["### 補完の考え方\n","\n","上記の補完方法を採用してよいかどうかは、それぞれの問題に即して考えなければならない\n","\n","欠損値の生成過程に関して、次の3パターンのいずれであるかを考慮する必要がある\n","\n","- MCAR(Missing completely at random)\n","\n"," 完全にランダムに欠損しているパターンであり、例えば、データシートにコーヒーをこぼしたや、メモリに宇宙線がランダムに打ち込まれ、データが化けたといった場合である\n"," - この場合の対処は容易であり、上記の方法で問題ない\n","\n","- MAR(Missing at random)\n","\n"," そのデータの他の特徴量に依存して欠損するパターンであり、例えば、日本人の信仰は\"無\"が多いなど、そもそも欠損することが別の理由で一般的な場合を指す\n","  - そもそも、欠損していることが普通であることから、補完せず、欠損そのものに別のラベルを与えるべきであろう\n","\n","- MNAR(Missing not at random)\n","\n"," 欠損となった値自体に依存して欠損するパターンであり、例えば、かなり古いがさだまさしの歌にあるように、O型だから(馬鹿にされることが多く)血液型を答えたくない場合もあるであろう\n"," - この場合の対処は難しく、欠損値がどの生成パターン由来かは、可視化して確かめる必要があり、適切に対処されなければならない"]},{"cell_type":"markdown","metadata":{"id":"akrb528vHNJr"},"source":["### 対策のまとめ\n","\n","- 削除手法\n"," - リストワイズ：欠損値を含むデータを削除\n"," - ペアワイズ：2つの特徴量の相関をプロットする際などに、NaNがあり不都合が生じる計算だけ無視する。\n","- 単一代入補完手法\n"," - 統計量の代入：平均値や最頻値、中央値などを代入し補完\n"," - 回帰代入法：欠損値の無いサンプルから回帰して補完\n"," - 確率的回帰代入法：回帰代入法の結果にノイズを加えて補完\n","- 完全情報最尤推定手法\n"," - FIML(full maximum likelihood method)：最尤推定(with EM)で補完\n","- 多重代入補完  MI(multiple imputation)\n"," - 欠損値を様々な単一代入補完したデータセットを複数作成し、各データセットで分析を行い、その結果を統合し欠損値を補完\n","- 機械学習的なアプローチ\n"," - weighted k-nearest neighbour algorithm (kNN)：kNNでいくつか近傍データを探してきて、重み付け和で欠損値を補完\n"," - Random Forestを用いた欠測データの補完とその応用による方法：RのMissForestや、FIML・MIといった手法\n"," "]},{"cell_type":"markdown","metadata":{"id":"5W7VCxG4HV3D"},"source":["## 実践的な補完方法"]},{"cell_type":"markdown","metadata":{"id":"e7VEgevGHhYq"},"source":["### interpolate()の基本的な使い方\n","以下のpandas.DataFrameを例に補完方法について述べる"]},{"cell_type":"code","metadata":{"id":"LZuQdnOuHpUc"},"source":["import pandas as pd\n","import numpy as np\n","df = pd.DataFrame({'col1': [0, np.nan, np.nan, 3, 4],\n","                   'col2': [np.nan, 1, 2, np.nan, np.nan],\n","                   'col3': [4, np.nan, np.nan, 7, 10]})\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DY1KPxycHuXi"},"source":["デフォルトでは各列に対して線形補間を行う\n","- 下端の欠損値には同じ値が繰り返される\n","- 上端の欠損値はそのままとなる"]},{"cell_type":"code","metadata":{"id":"Nz4PVYr2Hzu5"},"source":["df.interpolate()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mxEAI_7EIA5S"},"source":["引数axis=1とすると各行に対して補間される\n","- 右端の欠損値には同じ値が繰り返される\n","- 左端の欠損値はそのままとなる\n"]},{"cell_type":"code","metadata":{"id":"l2uaEYbhIBoa"},"source":["df.interpolate(axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2a3csb3mIWLW"},"source":["引数limitにより、欠損値が連続している場合、最大でいくつの欠損値を補間するかを指定することができる\n","- デフォルトはNoneで連続する欠損値すべてが補間される"]},{"cell_type":"code","metadata":{"id":"6drxOhDiIX2A"},"source":["df.interpolate(limit=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kxP56Z8vIiEK"},"source":["補間方向は引数limit_directionで'forward', 'backward', 'both'のいずれかを指定する\n","- デフォルトは'forward'である"]},{"cell_type":"code","metadata":{"id":"h3ulCqAwIilJ"},"source":["df.interpolate(limit=1, limit_direction='forward') # col2,0はNaNのまま"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5dUfzAAYIwC_"},"source":["df.interpolate(limit=1, limit_direction='backward') # col2,0が補完された"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X2ckMPJRI14L"},"source":["df.interpolate(limit=1, limit_direction='both')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8mrVP1WdJARY"},"source":["補間対象領域は引数limit_areaで指定する。'inside'だと内挿のみ、'outside'だと外挿のみ、None（デフォルト）だと両方が対象となる\n","- 外挿については上述のlimit_directionで前方（上側・左側）、後方（下側・右側）、両方を指定できる"]},{"cell_type":"code","metadata":{"id":"d7O1y7IqJDSF"},"source":["df.interpolate(limit_area='inside')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yPWvVx0XJGRs"},"source":["df.interpolate(limit_area='outside')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VgJrJIhyJJ5f"},"source":["df.interpolate(limit_area='outside', limit_direction='both')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pyITG-HkE1D2"},"source":["では、完全に補完つまり、NaNを無くすにはどうすればよいか？"]},{"cell_type":"code","metadata":{"id":"IDj-GOg7E0U7"},"source":["df.interpolate(limit_direction='both')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MTVG3iyPJ0Xm"},"source":["- **おわかりいただけただろうか**\n","\n","補間方法は第一引数methodに指定する\n","- デフォルトはmethod='linear'で線形補間である\n","  - まず、次のデータを準備する"]},{"cell_type":"code","metadata":{"id":"4YlmIbwxJRTp"},"source":["s = pd.Series([0, np.nan, np.nan, np.nan, 4, np.nan, np.nan],\n","              index=[0, 2, 5, 6, 8, 10, 14])\n","s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q2gXfymPKCC8"},"source":["s.interpolate()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uW2E7RxtKTbL"},"source":["method='linear'（デフォルト）ではインデックス列が数値でも特に考慮されないが、method='index'またはmethod='values'とするとインデックス列を考慮して補間される\n","- 但し、indexが数字であることが必須である"]},{"cell_type":"code","metadata":{"id":"YY5vax6wKHLT"},"source":["s.interpolate('index')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5FND_jrZKNuE"},"source":["s.interpolate('values')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xcZHzluAKlNC"},"source":["method='spline'とするとスプライン補間を行う\n","- 同時に引数orderに次数を指定する必要がある\n","\n","これまでのsの値ではスプライン補完ができないので、新たに次の値を用いて確認する\n","- スプライン補間は常にインデックス列を考慮して補間される"]},{"cell_type":"code","metadata":{"id":"F2aGG7O-Kwhh"},"source":["s = pd.Series([0, 10, np.nan, np.nan, 4, np.nan, np.nan],\n","              index=[0, 2, 5, 6, 8, 10, 14])\n","s.interpolate('spline', order=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dC0qytVrMYRC"},"source":["### fillnaの例\n","\n","まず、fillnaは、nanを埋めるための専用の関数である"]},{"cell_type":"markdown","metadata":{"id":"jLfLXrUoMsp-"},"source":["直前の値を使って埋めていく\n"]},{"cell_type":"code","metadata":{"id":"GYuxt78IMtHP"},"source":["df.fillna(method='ffill')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dYuINyBxM3XB"},"source":["直後の値を使って穴埋めをする"]},{"cell_type":"code","metadata":{"id":"avQdPETBM5LL"},"source":["df.fillna(method='bfill')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Zf1XhomNBjt"},"source":["平均値(df.mean())を用いて補完する方法は既に述べたが、他に中央値(df.median())や、最頻値(df.mode())を用いて補完する方法がある\n","- medianについては次の通りである"]},{"cell_type":"code","metadata":{"id":"JsecZ8YXNVoK"},"source":["df.fillna(df.median())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MTIGjYmlL25y"},"source":["### その他\n","\n","補間方法としては、そのほか、'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'barycentric', 'krogh', 'polynomial', 'piecewise_polynomial', 'from_derivatives', 'pchip’, ‘akima'が指定可能\n","- 特にtimeについて説明する\n","- 次のようなデータがあった場合、タイムスタンプに応じて補完される"]},{"cell_type":"code","metadata":{"id":"Xq7m-fiHMGOM"},"source":["df_nan = pd.DataFrame({'value': [1, np.nan, np.nan, np.nan, 31]},\n","                      index=pd.to_datetime(['2018-01-01', '2018-01-02', '2018-01-15', '2018-01-20', '2018-01-31']))\n","print(df_nan)\n","print(df_nan.interpolate())\n","print(df_nan.interpolate('time'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BeKDFtmIPbdm"},"source":["# 課題3(データ補完の効果)\n","\n","**[課題1]** 次のコードをコピーして実行し、結果も含めたノートブックを作成して提出しなさい\n","\n","**[課題2]** pandasのシンプルな補完を使って動作を確認してみなさい\n","\n","上記を一つのノートブックにして提出しなさい\n","\n","**注意点**\n","\n","ノートブックには次の内容を先頭に必ず記述すること\n","\n","- 先頭セルはテキストで、\n","\n"," 「# **データシステムの知能化とデザイン**」と記載\n","- さらに「# 第3回課題」と記載\n","- 次に、「## 学籍番号」と「## 氏名」を記載\n","\n","これは、全ての課題に共通する事項であり、忘れずに記載すること"]},{"cell_type":"markdown","metadata":{"id":"FGwheMMGhhp2"},"source":["まずはデータの準備をする\n","- またしても、作業ゲーである\n","\n","pandasの補完もかなり強力であるが、さらに高度な補完を行うため、scikit-learnのIterativeImputerを用いる\n","- fancyimputeという補完ライブラリも存在するので確認するとよい\n","\n","今回はメルボルンの住宅データを利用する\n","- どういうデータが入っているかだけ確認する"]},{"cell_type":"code","metadata":{"id":"AULcz72ZPm1d"},"source":["from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","from sklearn import preprocessing\n","pd.set_option(\"display.max_rows\", 101)\n","pd.set_option(\"display.max_columns\", 101)\n","#使用カラムを限定する\n","usecols=['Rooms', 'Type', 'Price', 'Method', 'SellerG',\n","        'Distance', 'Bedroom2', 'Bathroom', 'Car',\n","       'Landsize', 'BuildingArea', 'YearBuilt', 'Propertycount']\n","if not os.path.exists('Melbourne_housing_FULL.csv'):\n","    !wget \"https://drive.google.com/uc?export=download&id=1Ow53Wl7g40Pr1ExUoJ4P-IrSejpzUalt\" -O Melbourne_housing_FULL.csv\n","\n","df=pd.read_csv(\"Melbourne_housing_FULL.csv\",usecols=usecols)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mF33LBkbH6k8"},"source":["内容を確認する\n","- Pandas形式ならば安心してprintなしで全部表示してよい\n","- きちんと省略してくれる"]},{"cell_type":"code","metadata":{"id":"VLytle_NH4nj"},"source":["df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BuAAuaCUIGvH"},"source":["このデータについて、家賃以外の情報から家賃を当ててみよう\n","- なお、データには、そもそもPriceが設定されていないデータが存在していることがわかるので、これを削除しなければならない"]},{"cell_type":"code","metadata":{"id":"GbgI3ZzvIs3b"},"source":["df.dropna(subset=['Price'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BaJqc4elJIrL"},"source":["マニアックだが、次のようにしても同じ結果が得られる\n","- これは、`==`の比較演算子が、np.nanの比較に対してFalseを返すということを知っているからできる方法"]},{"cell_type":"code","metadata":{"id":"YDSbp0DIIZs4"},"source":["df[df[\"Price\"]==df[\"Price\"]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OhREAOONGba1"},"source":["df.dropna(subset=['Price'], inplace=True) #inplaceでdfそのものを修正する\n","print(\"全データ数\")\n","print(len(df[\"Rooms\"])) # Roomsに欠損がないことを知っているので\n","print(\"\\n欠損数の確認\")\n","print(df.isnull().sum())\n","#カテゴリ値を数値変換\n","le = preprocessing.LabelEncoder()\n","df[[ 'Type', 'Method', 'SellerG']]=df[[ 'Type', 'Method', 'SellerG']].apply(le.fit_transform)\n","df_train=df.drop(\"Price\",axis=1)\n","df_label=df[\"Price\"].values\n","print(\"\\n訓練データの確認\")\n","print(df_train)\n","print(\"\\n訓練ラベルの確認(つまりお部屋のお値段、これを当てる)\")\n","print(df_label)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c9FQwvSKhp4m"},"source":["マイクロソフトのLightGBMを利用して実際に学習させる\n","- 今回は、補完による性能への影響を見たいので、全て関数にしておく\n","- スコア(ロス関数の値)と、主要特徴量の一覧を返すが、ここでは主要特徴量は特に利用していない\n","  - 「どの特徴量が価格に大きな影響を与えるか」を影響力と共に知ることができるので、各自で試みると良い"]},{"cell_type":"code","metadata":{"id":"7syTWSfAQDfe"},"source":["import lightgbm as lgb\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import KFold\n","import copy\n","def run_model(df_train,df_label):\n","    params={\n","        'objective': 'regression',\n","        'random_state' : 1,\n","        \"metric\": \"rmse\",\n","        }\n","    kfold = 5\n","    score=0\n","    kf = KFold(n_splits=kfold,shuffle=True,random_state=94) #交差検証する関数を定義、データをkfold個にわけて1つをテスト用、残りを訓練に使う\n","    importance=0\n","    for i, (train_index, test_index) in enumerate(kf.split(df_train, df_label)): #定義したKFoldでdf_trainとdf_labelを分割\n","        print('[Fold %d/%d]' % (i + 1, kfold))\n","        X_train, X_valid = df_train.iloc[train_index], df_train.iloc[test_index] #train_indexなどは「選択した要素の配列」0から要素数あり、kfoldの確率で抜けている\n","        y_train, y_valid = df_label[train_index], df_label[test_index] #上と同様だが、要素が一つしかないのでこれでよい\n","        dtrain = lgb.Dataset(X_train, label=y_train) #まとめてデータセットを構成する\n","        dvalid = lgb.Dataset(X_valid, label=y_valid)\n","        bst = lgb.train(params, dtrain, num_boost_round=1000,valid_sets=[dtrain, dvalid],early_stopping_rounds=50,verbose_eval=100)\n","        # LigntBGMで学習、1000回試行して、ベストなものを返す\n","        importance += pd.DataFrame(bst.feature_importance(), index=df_train.columns, columns=['importance'])\n","        score+=bst.best_score[\"valid_1\"]['rmse']\n","    return importance,score/5"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rVvE979oiQ95"},"source":["では、まず最初に補完せずに、そのまま欠損を含むデータを利用して学習させてみる\n","- 意味合いとしては、欠損を欠損というデータとして扱うことになる\n","- 欠損に意味があれば、これもわるくないが、無作為に欠損する場合は場合は補完した方がよくなると考えられる"]},{"cell_type":"code","metadata":{"id":"0-zpkMgRiV05"},"source":["imp,score = run_model(df_train,df_label)\n","print(score)\n","scored = score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NzIiilEvivxI"},"source":["#欠損値を補完関数IterativeImputerのデフォルトのBaysian Ridgeで補完\n","imp = IterativeImputer(max_iter=50, random_state=5) # 方法の定義\n","df_train_fi = pd.DataFrame(imp.fit_transform(df_train)) # 定義した方法を用いて実際に補完\n","df_train_fi.columns = df_train.columns # カラム情報もコピーしておく\n","imp,score = run_model(df_train_fi,df_label)\n","print(score)\n","scorei = score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LBQmtFU8jyYE"},"source":["#欠損値を補完関数IterativeImputerのKNeighborsRegressorで補完\n","from sklearn.neighbors import KNeighborsRegressor\n","imp = IterativeImputer(estimator=KNeighborsRegressor(n_neighbors=15),max_iter=10,random_state=0)\n","df_train_fk = pd.DataFrame(imp.fit_transform(df_train))\n","df_train_fk.columns = df_train.columns\n","imp,score = run_model(df_train_fk,df_label)\n","print(score)\n","scorek = score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dRYPcD_RkUW_"},"source":["結果をみてみよう\n","- 結果は、RMSEであり、小さい方が良い結果といえる"]},{"cell_type":"code","metadata":{"id":"LDmq_JL7kXrU"},"source":["print(\"結果\")\n","print(\"何もしない\", scored)\n","print(\"Baysian Ridge\", scorei)\n","print(\"KNeighborsRegressor\", scorek)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hmhN2eWdk5RM"},"source":["補完に時間のかかる高度な方法を採用したからと言って、結果が良くなるとは限らない\n","- もちろん、これがLightGBMの特徴といえるかもしれない\n","- 欠損補完の目的の一つは、「欠損していることを特徴として利用するのではなく、統計的観点からバイアスなく統計処理を行う」ことにあるため、目的を見極めて補完の利用を考える必要がある"]},{"cell_type":"markdown","metadata":{"id":"IHBpP6OVG82o"},"source":["では、Pandasで補完してみよう"]},{"cell_type":"code","metadata":{"id":"UhI8E_SXqxuz"},"source":["#pandasで補完\n","dfp = df_train.interpolate(limit_direction='both')\n","df_train_fp=pd.DataFrame(dfp)\n","df_train_fp.columns=df_train.columns\n","imp,score=run_model(df_train_fp,df_label)\n","print(score)\n","scorep = score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RKfdMviQG1JD"},"source":["print(\"結果\")\n","print(\"何もしない\", scored)\n","print(\"Baysian Ridge\", scorei)\n","print(\"KNeighborsRegressor\", scorek)\n","print(\"Pandasで補完\", scorep)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GEXpRBjeHF1Z"},"source":["さすがにpandasのinterpolateはだめだが、なんとなにもしないデータが最高性能をたたき出した\n","- そういうこともあるよという例\n","- おおよそこの授業は、まっとうな結果ではなく、どちらかというとイレギュラーな結果を示していることに注意すること\n","  - 今後様々なイレギュラーに遭遇すると思うが、それに対応できるように"]},{"cell_type":"markdown","metadata":{"id":"udHS7wxek74E"},"source":["**(意欲的な人向け)**\n","\n","作業ゲーは嫌だという人向けに、以下、あくまでも任意ということで\n","\n","- 何が原因かをデータの特徴やアルゴリズムの観点から調査してみよう\n","- このことから、どのような「背景」つまり、「住戸価格」という現実的な話において、何がいえるであろうか\n","  - そのことは普遍的に(例えば他の街や国でも)いえるだろうか\n","  - 結局、現実に何が起きているのかを把握することが重要であろう"]}]}